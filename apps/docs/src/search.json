[{"f":"src/routes/blog/contents/2020_furikaeri/index.mdx","c":["2020年の振り返り。結婚と継続力","import { Image } from \"~/components/image/image\"; 2020 年も、もう残りわずかになりました。今年の振り返りをはじめてブログに残そうと思います。","技術","UseCotlin","2020 年 3 月より、自前で作ったツール "," で技術キャッチアップをはじめました。このツールは、Twitter に投稿された技術資料(ex.speakerdeck,slideshare)を収集するだけのツールです。日本だと、技術勉強会の資料は Twitter で投稿する習慣が（私の観測範囲では）あるため 、この習慣を利用して、様々な技術資料を広く発見できることができます。 下記のスプレッドシートに、技術資料リンクを自動登録しています。","日本だけじゃなく、海外の技術資料も発見できるので、色々な観点の技術を知ることができました。例えば、テスト手法、アーキテクチャの考え方、働き方の考え方などです。これを毎日欠かさずキャッチアップし続けていました。","Github Contribution","UseCotlin は、主に技術のインプットばかりで、アウトプットができていません。インプットのし過ぎで、頭でっかちにならないよう、毎日アウトプットを心がけていました。 ほぼ毎日、contribute するようにしました。Input と Output のバランスは難しいですが、取り組んで良かったです。","Twitter Follower","（思いつきで）Twitter のフォロワーを増やしてみようと思い、Twitter の Follower を自動的に増やす仕組みを構築しました。2020 年 9 月ぐらいからはじめて、フォロワー 1000 人ぐらいだったものがもうすぐ 3000 人ぐらいになります。 https://twitter.com/silverbirder/status/1318861346327252993","生活","結婚","11 月 22 日、26 歳で、はじめて結婚しました。 奥さんは、大学時代からお付き合いしていた人で、一度別れたりと紆余曲折ありましたが、私の方からプロポーズしまして、結婚することになりました。仕事を早く終わらせて、プライベートがとても充実しています。","在宅勤務と IoT","コロナの影響で、在宅勤務が当たり前になりました。今年買った IoT で良かったものを紹介します。","スマートエナジーハブ Nature Remo E lite","家の電力使用量をリアルタイムに可視化","NETATMO ウェザーステーション","家の室温、湿度、Co2 をリアルタイムに可視化","Arlo カメラ","玄関前やベランダをカメラで可視化","Withings Sleep&体重計","睡眠時間や体重を計測し、スマホからデータを可視化","来年の抱負","これまでは、技術をガンガンキャッチアップしていましたが、2021 年は、技術とプライベートのバランスを取ろうと思います。","終わりに","コロナコロナと、今年は騒ぎ立てていましたが、来年はもっと楽しい話題で盛り上がりたいなと思います。"],"t":"2020年の振り返り。結婚と継続力"},{"f":"src/routes/blog/contents/2021_furikaeri/index.mdx","c":["2021年の振り返り。","2021 年も、もう残りわずかになりました。今年も、振り返りをブログに残そうと思います。","技術","フルスタックエンジニアリング","今年は、業務でフルスタックな開発を多く経験できました。 昨年までは、Web アプリケーションの開発が多かったのですが、今年はクラウドインフラやデータ構築の業務が多かったです。 また、ゼロベースで構築したプロダクトの運用も経験したことで、去年に比べて幅広いエンジニアリング力を身につけられたなと思いました。 具体的に経験したソフトウェアは、","に書いていますので、ご参照ください。ざっくり言うと、クラウドインフラは GCP/Terraform, データ構築は BigQuery と Digdag です。","転職","現職が 2 社目になります。 1 社目が SIer で、2 社目が自社サービス会社で勤務していました。 1 社目から 2 社目は、(諸事情により)転職活動をせずに、転職していました。 そのため、3 社目の転職は、カジュアル面談や自己分析などといった転職活動をはじめて経験しました。 その転職活動については、"," で触れています。 転職活動することで、次のようなことを考えるようになりました。","私のキャリア形成 私の市場価値 私のできること・やりたいこと・すべきこと","生活","結婚記念写真","結婚式は、コロナの収束が見えない状況だったため、まだ挙げていません。 コロナの収束を待っていては、何も予定が決められないため、結婚記念写真だけでも撮ろうと決断しました。 撮影当日を、2021 年 3 月中旬ぐらいに設定したため、桜の綺麗な景色をバックに写真を撮ろうと計画しました。 例えば、次のようなことを考えました。","大阪城のチャペルを貸し切る プロのカメラマンに撮影頂く 和装・洋装どちらも着る","最後の\"和装・洋装どちらも着る\"は、衣装替えによってタイトなスケジュールになってしまうため、洋装のみとしました。 撮影当日は、曇りの天候でしたが、場を和ませてくれるカメラマンさんや、チャペルスタッフの方々のおかげで、とても晴れやかな雰囲気で撮影できました。 私と妻の両家の両親へ写真を共有し、とても喜んでくれました。","新婚旅行","新婚旅行に行ってきました。 当初、海外を考えていましたが、感染リスクを考えると国内にしようと話になりました。 そこで、ハレクラニ沖縄というハネムーンに最適なリゾート地に向かい、そこから 4 泊 5 日のスケジュールを考えました。 普段、あまり経験しないようなことをしようということで、SUP やクルーズなどを調べていました。 妻と予定を考えるのが楽しかったり、意見が衝突してケンカしたりと、良い思い出になりました。 またしても、新婚旅行の天候は芳しくなく、いくつか予定変更を余儀なくされましたが、 このような事態を何度も経験しているたためか、臨機応変に予定を組み直し、自由気ままに遊んで満喫しました。 竹富島の中心部から船乗り場まで、全力で走ったのは、その場のノリでやりすぎたなとも思っています。(笑)","来年の抱負","来年は、今年以上に忙しくなりそうです。 技術面では、来年 1 月 4 日より新たな職場で勤務することとなるので、環境の変化で疲弊しそうです。 生活面では、車や家といった大きな買い物を検討するのと、子作りも頑張らねばいけません。(妻の事情で、解禁) そのため、来年の抱負は ","挑戦と健康のバランスを保つ"," です。 まだ 20 代なので、何事にも失敗を恐れず挑戦していきたいと思っています。 例えば、Web フロントエンドの専門的な領域を探求したり、OSS コミッター活動を頑張ったりです。 他には、ペーパードライバーの私が、子供と遠出しやすいよう、車の運転に慣れるよう努力したいです。 ただ、挑戦し続けると疲労し、ストレスが蓄積しやすくなります。 今年、何度か妻とケンカをしてしまったのですが、原因の多くは、私の未熟さだと思っています。 喧嘩の例を挙げると、親切心で家事(料理,掃除,etc)をしていたのですが、感謝を妻に求めてしまい喧嘩になることが多かったです。(≠ 親切心) 当たり前ですが、行動や態度で気持ちを伝えるのではなく、ちゃんと会話して気持ちを伝えるように心がけようと思います。 特に『一緒にやる or お願いする』や『妻のタイミングで』をできるようになりたいです。 ※ 妻とは 9 年ぐらいの付き合いなので、言わなくても通じることが多いです。 ※ 私は、一人でなんでもやろうとする人で、几帳面な性格も相まって、完璧にこなそうとする癖があるようです。","終わりに","エンジニアとして、また、人として成長できるよう、来年も多くの経験を積んでいきたいと思います！"],"t":"2021年の振り返り。"},{"f":"src/routes/blog/contents/2022_furikaeri/index.mdx","c":["2022年の振り返り。転職と妊活","2022 年の終わりに近づきました。今年の振り返りをしようと思います。 これは、自分のために書いていくので、文章の構造化とか不適当にやります。","仕事","今年の 1 月から、新しい職場で働き始めました。人生 2 回目の転職です。 2 回目となると、転職関連の手続きは随分と慣れました。 新しい職場では、Slack や Discord でコミュニケーションを取るのですが、ボケるコメントが多く、とても賑やかで楽しい職場です。クスっと笑えるので、居心地良い環境だなと思います。 私はフルスタックなエンジニアリング力を経験してきたのですが、改めてフロントエンドエンジニア力を高めたいと思い(もともと IT 業界に入ったきっかけは Web フロントエンド)、転職してみました。 ただ、入社して半年ぐらいはドメインキャッチアップやバックエンド開発などをしていました。 特別な領域のドメインのため、それの勉強や、バックエンドのプログラミング言語も馴染みがないものだったので、色々と一杯一杯でした。 半年が経過したあたりから、フロントエンド開発に投資できるようになり、業務で初めて React や TypeScript を試すことができました。 1 つの画面を React に置き換えるプロジェクトを終え、すごいインターン生のフロントエンドレビューをひたすらしたり、フロントエンド刷新プロジェクトにチャレンジして、フロントキャリアが長い方と数ヶ月一緒に仕事をして、ドンドンと右肩上がりに吸収できていると満足しています。 また、私は本来、React などのフレームワークよりもブラウザの仕組みや、HTML や WebAPI の仕様を読むという方が好きなので、それは継続していこうと思います。 今年の 11 月から、名古屋拠点のメンバーと仕事をすることになり、私が住む大阪から新幹線通勤することになりました。そこでは、エッジなアーキテクチャを採用していて、それを楽しみに、今全力投球しています。","プライベート","わたしは、妻と二人で暮らしています。僕ら二人の気持ちとしては、そろそろ子供がほしくなってきました。祖父母や両親に、僕らの子を見させてあげたいと思っています。 妊活って、まったくわかりませんでした。 自然妊娠がなかなかできずに、とても悩みました。 職場の人から、不妊治療というのを教えてもらい、妻と相談し紆余曲折あって、不妊治療の検査に進むことができました。妻が保育士で多忙のため、なかなか検査にいく時間がないため、まだ検査は終わっていません。 妊活を進めていく中で、妻に精神的な負荷がかかってしまい、つらいときがありました。最近は、『子供は授かりもの』と運だと割り切り、できたら良いよねと思い始めるようになりました。","お金と時間","来年の 4 月ぐらいに、祖父母が所有する空き家を貰い住むことなりました。理由としては、空き家が実家の近くのため、両親の近くだと何かと安心できるから (子供の件で) です。 祖父母としても、他界する前に相続でバタバタして貰いたくないので、空き家を誰かに貰って欲しかったとのことでした。僕らとしても、家賃を払わなくて済む (固定資産税は年 2,3 万程度 だけで良い) ので、引き受ける決意をしました。 そうすると、支出がめちゃくちゃ抑えることができ、貯金も四捨五入したら 1000 万はあるため、妻には来年 4 月には働かず自由になって貰うつもりです。 お金はいくらでも欲しくなるのですが、正直私としてはお金よりも家族との時間を優先したいと常々思っています。 そのため、週 5 日の 8 時間勤務の正社員を継続するのか、もしくは契約社員やちょっとした副業でもやろうかと悩んでいます。 副業だと、以前やっていたプログラミングを教えるサービスをもう一度やろうかなと思ったり。 もしくは、人の写真を撮るのが好きで、写真って思い出にも残るし、いつか見返したときの温かい気持ちになれる素敵なアイテムだと思ってて、そういうのをちょっとした副業とかにもつなげてみるのも、面白そうだなと思います。","来年の抱負","仕事よりも、家族の時間を大切にしたいと思います。 と言いつつも、モノづくりは結局楽しいので、趣味の範囲でワチャワチャしつつ、仕事でスキルアップしていこうと思います。","過去の振り返り","過去の振り返りを読んでみると、なんだかエモい気持ちになりました。","結局、車の購入は見送りました。交通機関が充実してる地域に住んでいるため、まだデメリットが大きく感じました。"],"t":"2022年の振り返り。転職と妊活"},{"f":"src/routes/blog/contents/20_study_enginner/index.mdx","c":["20代後半エンジニアである私がこれから学ぶべきこと","私は、現在 26 歳の Web エンジニアです。これまでの技術に対する学び方と、これからの技術に対する学び方について、少し考えたいと思っています。","これまでの 20 代前半","新卒入社した当時の私は、業務上、PHP + MySQL on AWS の組み合わせで Web アプリケーション開発を学んでいました。 それとは別に、プライベートでは、Node.js + MongoDB on SAKURA レンタルサーバで Web アプリケーション開発もしていました。 当時、Web アプリケーション開発のフロントエンドとバックエンドをなんとなく動かせる程度で満足していたのですが、 先輩や同僚のエンジニアや、お勧めされた書籍から影響を受け、あらゆるものに興味を示しました。","SQL や正規表現、文字コードといったエンジニアにとって欠かせない技術 sed や awk, make といった shellscript の扱い方 手続き型、オブジェクト指向、関数型といった考え方のパラダイムシフトを発生するような考え方 MVC, MVVM, CleanArchtecture などのアプリケーション設計 ユニットテストや E2E テストなどを使った TDD 開発 CI/CD を活用した DevOps 開発 インフラやミドルウェアの自動構築 CloudNative なアプリケーション開発 SSR や SPA、SEO といった Web 設計 インタプリタ言語やコンパイラ言語 コンポーネントベースのフロントエンド開発 Web ゲーム開発 ビッグデータを扱う技術 様々なクラウドサービス技術 強化学習 Bot 開発やスクレイプ技術、GAS などの業務改善","Web アプリケーションに関する幅広い領域をあらゆる角度で学んでいったのかなと思います。","これからの 20 代後半","これからも、興味を持った分野を見つけて、手あたり次第学んでいくのも 1 つのキャリアだと思います。 ただ、自信過剰という訳じゃないですが、多分やればある程度できるようになれると自負しています。 しかし、私としては、そろそろ広く浅くから、狭く深く学ぶ領域(専門性)を見つけたいと思っています。 どういった領域を突き詰めたいか、考える時がきたのかもしれません。","何を突き詰めたいか","単純にソフトウェア技術を突き詰め、テックリードを目指すか、 プロジェクトマネージャーやエンジニアリングマネージャーといった管理職を目指すか。 それでいうと、前者を目指したい。具体的には、Web に関わる専門性を突き詰めたいなと。 また、アーキテクトのような職業に憧れています。 Vue や React など言語は違えど、Web アプリケーションの成果物はそう変わりません。 だとすると、あまり似たようなベクトルの言語を学んだところで、面白みが薄いのではと思っています。 それよりも、どのレイヤーにどういう役割を持たせるか、クラスをどのように設計するか、レスポンス速度をより早くするためにはどのようなシステム構成にすべきなのか、そういった視点を考える方が楽しいと最近感じています。 また、アーキテクト的な視点以外に、Web の Native(標準)部分の話やブラウザの話、W3C の動向など、これからの Web についてキャッチアップし続けることにも、力を注ぎたいと思っています。 それらを踏まえると、『Web の専門性が高いアーキテクト』のような立ち位置を目指したいと思います。","最後に","ポエムみたいな話になってしまいました。以上、最近の悩み事でした。"],"t":"20代後半エンジニアである私がこれから学ぶべきこと"},{"f":"src/routes/blog/contents/Good_Smart_Gadget/index.mdx","c":["スマート家電のよさをしってほしい","私が実際に使ってみて便利だと感じたオススメ商品を３つ紹介します！","スマート家電？なにそれ？","ハマる前までは、スマート家電という言葉自体よくわかっておらず、「なんだか賢い家電なんでしょ（適当）」と興味ありませんでした。 とある職場での会話をきっかけにして、その","便利さ","にドンドンとスマート家電にハマっていき、「","これはもっとはやくに知っておきたった！","」と後悔すら感じるほどでした。以降では、以下３つのスマート家電を紹介します！","決して回し者ではないです（笑）","オススメ１： Nature Remo","赤外線送信している媒体に対して、nature remo が代わりに送信してくれます。ですので、例えば照明にリモコンでポチポチ ON/OFF していたことを、代わりに nature remo がしてくれます。 これのよいところは、つぎのとおりです。","スマートフォンから ON/OFF できるので、わざわざリモコンを探さなくてすむ。 タイマー機能と一連の動作を設定できるので、「朝 7 時に電気とエアコンとテレビをつける」ということが自動的にしてくれる。 Google Home や Alexa などのスマートスピーカーと連動できるので、音声による制御もできる。（ok google, ただいま〜 → 電気エアコンテレビがつく） （つかってないけど）湿度、温度、照明の明るさ、スマートフォンとの距離をトリガーにできる","↓ スマートフォンから nature remo 経由で電気をつけてみます。 https://www.youtube.com/watch?v=_j-qXrxtsyU","オススメ２： Switch Bot","物理的に押すボタンに対して switch bot が代わりに押してくれます。 nature remo では制御できなかった物理ボタンに対応しています。 これのよいところは、つぎのとおりです。","スマートフォンから物理ボタンを押したり引いたりできる。 タイマーやスケジューリングも可能。 両面テープでしっかり固定されるので、どこでも使える。","これを使えば、帰宅中に、お風呂のお湯沸かしボタンを押せるので、 帰宅したらすぐにお風呂に入れます！ ※ ","というものを経由しないと自宅外から制御できないのが難点。 ↓ スマートフォンから switch bot 経由でボタンを押しています。 https://www.youtube.com/watch?v=wkrPf-FuXFU","オススメ３： Google Home Mini","これは無知な僕でも知っていた商品です。音声で google アシスタントにサポートしてくれるすぐれものですよね。 また、 さきほど紹介した Nature Remo や Switch Bot とも連携できるので、 「スマートフォンからの操作」から「音声から操作」できるようになります。これはすごく便利で、「スマートフォンを開く → アプリをタップ → 操作」という流れを google home mini に「電気をつけて」って言うだけで済みます。 また、一連の操作を登録できる「ルーティン」があるので、「電気をつける、エアコンをつける、テレビをつける」→「ただいま」と登録すると「ok google, ただいま」というだけで３つの家電がつきます。 この「nature remo + switchbot + google home mini」の組み合わせは、僕の生活スタイルにピッタリで、とても楽になりました。 ↓ google home mini から「ただいま」を言ってみる https://www.youtube.com/watch?v=_XGMBIXC3pU","おわりに","とある友達にこのことを伝えると「自分でやれば良いじゃん（笑）」と言われてしまいました。しかし、どっぷりハマった僕には、もう引き返せません。朝起きて、電気をつけて、エアコンをつけて、テレビをつけてる生活なんて…。 むしろ、「ほかにも便利なものないのか！？」と amazon で調べまくってます。（笑） ぜひとも、スマート家電のよさを知って頂ければ幸いです。","近況","近々、セサミというスマートロックが自宅に届きます。こちらは、家の扉にある鍵をスマート化します。 こちらが届きましたら、また記事を書いてみようかなと思います。"],"t":"スマート家電のよさをしってほしい"},{"f":"src/routes/blog/contents/Introduce_SESAME/index.mdx","c":["SESAME(セサミ) が届いたよ！","import { Image } from \"~/components/image/image\"; 「鍵どこいったっけ？」という悩みから、おさらば！","SESAME(セサミ)って なに？","https://jp.candyhouse.co/","あなたは家を出る時、何を持って出ますか？ 鍵、財布、そしてスマホ…？ もう鍵は必要ありません。 スマホがあなたの鍵になります。","スマートフォンから家の扉にある鍵を開けれるようになります。","Q. 危なくないの？","公式ページでは、次のようにアナウンスされています。 結論としては、しっかりと考慮されているそうなので、そこまで神経質にならなくて良いかもです。","長いバッテリー寿命","電池の持ちは約 500 日。残量が少なくなったらスマホに通知が来るから安心。","従来の鍵も使用可能","セサミは従来の鍵でも今まで通り使用可能です。スマホに慣れていない家族や、スマホの電池が切れてしまった場合でも安心です。","通知機能","誰かがドアを開閉した際に、あなたのスマホへ通知します。","セキュリティは某国軍事レベル","セサミは悪い人を寄せ付けません。セサミは AES-256-GCM と TLS 1.2 を採用しています。","Q. どんな機能があるの？","自動アンロック & ロック","SEMAME とスマートフォンが一定距離に近づく・離れたら、鍵を開く・閉じるようになっています。","ノック機能（iOS のみ対応)","アプリをバックグラウンドで起動している状態で、スマートフォンをノックすると、SEMAME が解錠してくれます。","鍵のシェア","特定の人に鍵をシェアすることができます。","使ってみるまでの過程","まず、私の家の扉はつぎの画像の感じです。ここの下の鍵に SESAME を取り付けたいなと考えていました。 ただ、試してみると、私の家の扉では SESAME が設置できませんでした… 何がダメかというと、SESAME は両面テープでくっつけるため、壁と設置する必要があります。ただ、私の家の扉には、段差があるため、SESAME をくっつけることができませんでした…。 そこで、カスタマーサポートに連絡したところ、超親切にフォローして頂き、アダプター作成をしてもらえました。（送料無料、ただしアマゾンレビューをする） 返事がとてつもなく早く、本当に素晴らしいと感動しました。 こちらが提供した情報は、扉の各必要な長さを提供したぐらいです。 それぞれの長さを提供すると、つぎのようなアダプターが完成したとのことで連絡をもらいました。（おそらく 3D プリンターで作成されたのかな) こちらを私の家の扉に設定してみると、ぴったしハマり、無事両面テープが貼れました！ぱちぱち！ 当初、白色のアダプターが来るのかと思ったのですが、黒色がきました。良いね！ https://youtu.be/6Bn8uYl0ans 家の扉前から SESAME を使ってみた Bluetooth が繋がっている状態だと、スムーズにロック解除ができました ！わーい！","残念ポイント…","SESAME とスマートフォンが Bluetooth に繋がって","初めて","、鍵の解除ができます。ですので、Bluetooth まで繋がるまでは、何もできません。 そこを待つのが長くて数十秒かかってしまうので、ちょっと待ってしまいます。また、オートロックは、１０歩程度離れると発動するのですが、オートアンロックは、ほとんど動作しませんでした。（近づいたら鍵が解除すること）。ノック機能も同様で、Bluetooth 接続されて初めてノックが有効になります。しかも、Bluetooth 接続した状態でも、ノックの失敗率が高く、あんまり使い物にはなりません…。改善の余地ありですね。","まとめ","カスタマーサポートのフォローには、アマゾンレビュー星５つぐらいの好感を持てました。機能に関しては、残念ポイントで伝えたとおり、目玉機能がちょっと残念な結果になってしまいました。しかし、当初の目的だった「鍵を探す手間」は、なくなりました。家の扉に近づくころに、アプリを起動すれば、Bluetooth 接続がすんなり通るため、タップ１つで鍵が開きます。要は使い所を工夫すれば、全然メリットの方が大きいのかなと個人的にはそう思います。 是非、みなさんも購入を検討してみてください。本体だと今、１つで 14,800 円です！扉を取替するよりかは断然安い！ ※ ちなみに、私は WiFi アクセスポイントなしの本体のみを購入しました。"],"t":"SESAME(セサミ) が届いたよ！"},{"f":"src/routes/blog/contents/Lets_debug_with_JavaScript_debugger/index.mdx","c":["JavaScriptのdebuggerを使ってデバッグしよう (Browser/Node.js/Jest)"," を使って、デバッグをしましょう。標準機能なので、React などのライブラリでも使えます。 tags: [\"JavaScript\", \"Debugger\"] cover_image: https://res.cloudinary.com/silverbirder/image/upload/v1657366278/silver-birder.github.io/blog/timothy-dykes-LhqLdDPcSV8-unsplash.jpg","import { Image } from \"~/components/image/image\"; JavaScript の標準機能 "," を使って、デバッグをしましょう。 標準機能なので、React などのライブラリでも使えます。","Browser","次の HTML ファイルを Chrome で開きます。","開いたページで、DevTools も開いておきます。 その状態で、Button をクリックしましょう。 そうすると、次の画像のようになります。","と書いた箇所で、処理が停止されます。 そのブレークポイントから、ステップイン、ステップアウト、ステップオーバーといった操作ができます。 Console タブで、変数や関数などの実行結果を確認できます。 このように、簡単にデバッグができるようになります。","Node.js","Node.js でも、同様に "," が使えます。 次の JavaScript コードを用意します。","このファイルを次のコマンドで実行します。","実行すると、次の画像のような出力になります。 その後、Chrome から "," にアクセスしてください。 アクセスすると、次の画像の画面になります。"," を Click したら、次の画像のようになります。 そうです、さきほどと同じように、"," の箇所で、処理が停止されます。 簡単ですね。","Jest","テストフレームワークの Jest も、同じように "," が使えます。 次のテストコードを用意します。","このファイルに対して、次のコマンドを実行します。","実行すると、次の画像のような出力になります。 また、同じく Chrome から"," にアクセスすると、同様にデバッグできます。 Browser,Node.js と同じ使い方になります。 わかりやすいですね。","終わりに","IDE やエディタでデバッグ設定することもできますが、こちらの方が断然楽ですね。","参考","https://developer.mozilla.org/ja/docs/Web/JavaScript/Reference/Statements/debugger https://nodejs.org/ja/docs/guides/debugging-getting-started/ https://jestjs.io/ja/docs/troubleshooting"],"t":"JavaScriptのdebuggerを使ってデバッグしよう (Browser/Node.js/Jest)"},{"f":"src/routes/blog/contents/Philippines_Travel_Notes/index.mdx","c":["フィリピンに行ってきたら、日本は良いなって思うようになった","import { Image } from \"~/components/image/image\"; 2019 年 10 月 11 日~2019 年 10 月 15 日の 5 日間、フィリピンに行ってきました。日本人男性(前職の先輩:Kikuchi)とフィリピン人女性が結婚するため、その結婚式旅行に同伴させて頂きました。Kikuchi さんとは、私が新人の頃に大変お世話になった方なので、お祝いの気持ちが込み上げてきました。:)","はじめての海外旅行","だったので、当日までわくわくな気持ちでいっぱいでした。ただ…あれ(↓)を知るまでは…","フィリピン式トイレ","フィリピン式トイレ！ 😰 拭き終わったトイレットペーパーをゴミ箱に捨てるだと！？ 😠 💩 信じられない…😵 Philippine Toilet もう駄目です、先輩。僕は行けません。僕は潔癖症なんです。あと、お腹が弱いんです…。けど、いくしかない…。","いざ、フィリピンへ！","1 日目、セブに到着したときには、すっかり夜でした。 ここで","ヤバい","社会に遭遇しました。道路です。車やバイクがガンガンに走ってきます。信号はほぼありません。","譲り合いの精神は、ほぼない","と感じるほどでした。関西人の強引さを最大限まで高めた人たちって感じです。 特にバイクが多く、二人乗りが多かったです。中には、4 人家族(父・母・子 2 人)で１つのバイクを運転していました。生活上仕方がないとは言え、日本に比べると、とても危険です。 ショッピングセンターにきました。とてもおしゃれ（語彙力）なイルミネーションです。 Kare という料理がフィリピン料理で有名です。晩ごはんに注文してみました。 衝撃的に口に合わなかったです…。 一緒に来ていた人が Kare Kare を味見して「","土","食べてるの？」と言われました…。これが本来の味なのか、お店独自のアレンジなのか分かりません。 1 日目のホテルは、トイレットペーパーを流しても良いトイレでした。 😍","2 日目の朝、散歩しました。","夜には見えなかったのですが、壁にガラスが刺さっていました。登り防止のようです。なんというか斬新です。 フィリピン人女性の実家であるボホール（タグビララン）へフェリーで移動。 小型（中型？）のフェリーだったため、とっても揺れました。あと、外の潮風が気持ち良いです。こういうオープンなところで満喫できるのは、個人的に好きなポイントです。 写真に写っている人は、私ではありません。前職の先輩(Nakamura)で、とても明るくコミュニケーション力が高い兄貴です 👍 セブからボホール移動すると、一気に田舎感が出てきました。 こういう所を期待していました 👏👏 バイクが沢山あって、屋台も多く、とても生活感あふれる世界です。 スーパーにきました。日本に比べて、半端なくレジの待ち時間が長いです。長いです。長いです！ 商品をスキャンする → 商品を紙袋につめる の 2 ステップが、とっても時間がかかります。いかに、","日本のレジ打ちが洗練","されているのか実感しました。 あと、従業員が","よくサボっています","。従業員の数が多い印象があり、そのせいか仕事をせずに、従業員同士で喋っていたり、携帯を触っていたり、さらには、お店の商品で投げあいっこすらしていました 😧。そう思うと、日本人はみんな真面目に働いているんだな〜。 偽ブランドの靴とカバンを買いました。どちらも 500 ペソ(約 1000 円)で購入できました。 フィリピンの物価はとても安いです。500ml の水が 20 ペソ、約 40 円です。そもそも、フィリピンの平均的な月給は、約 2 万ペソです。貧富の差が激しいため、裸でやせ細った人もたくさんいました。 安いので、体に悪そうな（こういうときぐらいいいじゃない！）お菓子をいっぱい買いました。うん、どれも普通！ 😄 人気の商品は、英語や日本語で商品説明の記載がありました。それ以外は、現地語のタガログ語みたいです。 2 日目(&3 日目,4 日目)のホテルは、トイレットペーパーをゴミ箱に捨てる方式のトイレでした。 😫 💩 を出した後、ゴミ箱の臭いがとても気にしていました。絶対その空間臭くなってるだろ！ … そんなことはありませんでした 😲。気にしすぎていたようです。外のトイレは汚かったですが、ホテル内のトイレは、十分に清潔にされていました。","基本的にホテルでトイレを済ませる","ことで、トイレ問題を乗り越えました！ 👍 3 日目の朝、ターシャを見に来ました。ターシャはとても小さく寂しがり屋のお猿さんです。聞くところによると、ターシャを驚かせてしまうと、死んでしまうそうです…。 Quiet !!! ロボック川を下るツアーに参加しました。この川周辺に暮らしている人たちが、手を降ってくれました。あの人達は、どこで暮らしているのでしょう。 🤔 赤色の服を着た少年少女が音楽とダンスを披露してくれました。とっても可愛らしかったです。言葉が分からなくても、楽しめました！ 次に、バギーという乗り物を体験しました。 国際免許が不要で、敷地内を爆走します。フィリピンの天候は変わりやすいのですが、奇跡的にも、全イベント中は雨に合いませんでした。移動中に毎回雨が降っていました。 ","天気の子は、我か！？","まだ序盤の時の写真なので、服が汚れていません。しかし、ガソリンの臭い、バギーの音、晴れ晴れとした天候、これらの要素が僕の気分をハイにさせます。 🚗⚡️️ 水溜りが多く、基本的には避けて移動するのが安全です。しかし、安全などつまらない。 あえて ","水溜りに全力で突っ込む！"," これがバギーの嗜みというものです。 全身、ドロドロ（言葉通り）になりました（都合上、写真はありません）。終盤に水落場があるので、そこで綺麗にします。 着替えは必須です！！！ アロナビーチというところきました。もう夕方になっており、一日遊び疲れたせいか、ぐったりです。 水切りして遊んで終わりました…。パラグライダー?をしている人たちをみかけました。楽しそうだな〜したいな〜と思いつつ、疲れていたので、ぼーっと眺めていました。（笑） 夜ご飯の移動に、はじめてトライシクル（通称: トゥクトゥク)に乗りました。 乗り物の作りはかんたんで、バイクとサイドカー、その上に屋根があるだけです。 ボホールでは、この乗り物で移動する人が多く、町中にたくさんいます。 運転は、想像通りの粗目で、ちょっと危険です。 帰りにチップをたくさん渡しました。とてもにこやかに返っていきました。 有名なデザート、Halo Halo を頂きました。上に乗っているのは、紫芋？で、パイナップル、ヨーグルト、シャーベット、スイカが入っていました。これは、当たり 😍 絶品でした！ 4 日目の朝、メインイベントである結婚式です。結婚式は、基本的に英語を使っています。現地語であるタガログ語も使われていたのかもしれませんが、日本語しか分からない僕には、どちらにしろ分かりませんでした。（笑） 海外にはじめてきたのですが、人と話したいのに","言葉が通じないというのは、とても残念で、もどかしかった","です。Google 翻訳を通して会話をしていたときもありましたが、あれは一時的なときな会話にしか向いていません。 実際に生活してみると、少し英語が聞き取れた気がしました。 結婚式に参加するのもはじめてでした。これがウィンディングロードというやつか！！ いつか通ってみたいな〜。 ※ 新郎新婦の写真を載せたかったのですが、やめときました。 結婚式が 2 時間ほど続いていました。何も言葉がわからないので、「神父の言葉 → 国歌 → 写真撮影」ぐらいのイメージしか伝わらなかったです。（笑） フィリピンの披露宴では、「新郎新婦がダンスを踊っている際に、彼らの服に","お金を安全ピンで付ける","」という伝統的文化がありました。お金を付けるときは、こちらもダンスをする必要があります。僕はダンスなんて人生一度もなかったので、","謎ダンス","を踊りつつ、お金を付けて、すぐさま自席に戻りました。 また、独身の方向けのイベントがありました。これもフィリピン独自の文化？なのでしょうか。全て言語が通じないので、なんだかよくわからず、流れに身を任せて事なきを得ました。","終わりに","結婚式旅行というより、フィリピン旅行という印象が強い旅でした。僕は、初結婚式の参加、初海外旅行という状態だったので、とても心配していましたが、結果としてはとても楽しい時間を過ごせました。当初懸念していたトイレ問題は、気にしすぎていただけでした。 外を世界を知ることで、自身の世界を評価できるという当たり前のことを再認識できました。他社を知ることで自社を評価できるように、海外を知ることで自国を評価できるようになりました。 いまから、とても個人的な意見を話します。日本人は、時間をきっちり守るし、仕事も真面目に働くし、譲り合って生活する文化があると思っています。あと、日本では、小中学校のような一般教養が平等に与えられているのは、良いことだなって思います。誰でも等しく何にでもなれるチャンスがあります。 フィリピンで驚いたことは、母国語（タガログ語）だけでなく、英語も普通に話せる人が多かったです。聞くところによると、複数言語を話せるのは当たり前だそうです。香港でも複数言語を扱える人が多いと聞きました。日本も母国語以外（英語）を学校で習いますが、獲得した能力的には低い印象があります。 貧富の差が激しいフィリピンですが、どんなに貧しそうな人でもスマートフォンは必ず持っていました。とても不思議に感じます。おそらく古い型のスマートフォンを使っていると思うのですが、電気だったり WiFi はどうしてるのかなと謎でした。 色々な発見が多かったフィリピン旅行ですが、また海外旅行を行ってみたくなりました。ただ、ツアーのような現地に詳しい人と一緒に行かないと、ちょっと不安だなと思います。ですので、次海外旅行するなら、ツアーから選んで見てみようと思います。とても楽しい旅行でした！"],"t":"フィリピンに行ってきたら、日本は良いなって思うようになった"},{"f":"src/routes/blog/contents/Roundup_IT_Antenna_Sites/index.mdx","c":["Roundup IT Antenna Sites","背景","今年の 9 月に PyConJP 2019 が開催されます。 https://pycon.jp/2019/ LT の募集があったので、LT 応募するためのネタ探しをはじめました 😄","IT Antenna Sites","私が普段見ている IT 系のサイトから調べました。 その時に使ったサイトを紹介します。 ※ 色々な人から参考にさせて貰ってます。 :)","Github","https://github.com/trending/python","PublicKey","https://www.publickey1.jp/mt6/mt-search.cgi?IncludeBlogs=2&tag=Python","Dev.to","https://dev.to/t/python","Reddit","https://www.reddit.com/r/Python","Hacker News","https://hn.algolia.com/?query=python","Qiita","https://qiita.com/tags/python","Hatena","https://b.hatena.ne.jp/search/tag?q=Python","Medium","https://medium.com/tag/python","Ubersuggest","https://app.neilpatel.com/jp/ubersuggest/overview?keyword=python","GoogleTrend","他にオススメありましたら教えて下さい。 😆","Python ネタ","次のような技術について知ることができました。","Deepfacelab","https://github.com/iperov/DeepFaceLab","DeepFaceLab is a tool that utilizes machine learning to replace faces in videos. Includes prebuilt ready to work standalone Windows 7,8,10 binary (look readme.md).","Grumpy","https://github.com/google/grumpy","Grumpy is a Python to Go source code transcompiler and runtime.","Pyodide","https://github.com/iodide-project/pyodide","The Python scientific stack, compiled to WebAssembly","PyOxidizer","https://github.com/indygreg/PyOxidizer","A modern Python application packaging and distribution tool","Pyre","https://github.com/facebook/pyre-check","Performant type-checking for python.","Pyxel","https://github.com/kitao/pyxel","A retro game engine for Python","ScrapydWeb","https://github.com/my8100/scrapydweb","Web app for Scrapyd cluster management, Scrapy log analysis & visualization, Auto packaging, Timer tasks, Email notice, and Mobile UI","さいごに","LT のネタとなるような Python に関する経験が乏しい私は、こういうアプローチでネタを探しました。まだどれを選択するかは決めていませんが、とりあえず応募はしようと思います！ 💪"],"t":"Roundup IT Antenna Sites"},{"f":"src/routes/blog/contents/a_brief_history_of_graphql/index.mdx","c":["GraphQLの歴史","import { Image } from \"~/components/image/image\"; GraphQL を業務で使い始めました。 いつものように、GraphQL の歴史が気になったので、調べてみました。","参考資料","https://www.youtube.com/watch?v=VjHWkBr3tjI GraphQL の共同開発者で、GraphQL Foundation エグゼクティブディレクターである Lee Byron さんから、GraphQL の歴史について、紹介されています。 次の資料も参考になります。","https://dev.to/tamerlang/a-brief-history-of-graphql-2jhd https://levelup.gitconnected.com/what-is-graphql-87fc7687b042","GraphQL が生まれる前","GraphQL が生まれる前の歴史を、簡単に要約しました。 | 年 | 要約 | | ---- | --------------------------------------------------------------------------------------------------- | | 2004 | ソーシャルメディア Web サイト「Thefacebook」が公開され、後に FaceBook になりました | | 2007 | iPhone の登場により、モバイルが急速に普及し始めましたが、FaceBook は HTML5 に賭けすぎて失敗しました | | 2012 | FaceBook はモバイル(iOS) のニュースフィードを REST API で開発し始めました |","REST API での開発における 3 つの課題","REST API で開発を進めていくと、次の 3 つの課題を抱えてしまいました。","Slow on network","1 つの API から必要なデータが全て返ってこないため、複数のリクエストを何度も往復する必要がありました","Fragile client/server relationship","API の変更を、クライアントコードに慎重に引き継がなければ、クラッシュしてしまいました","Tedious code & process","クライアントの開発は、API のレスポンスに非常に連動しているので、API のレスポンスの変更があれば、クライアントも変更しなければなりません","これらの課題を解決すべく、FaceBook は、スーパーグラフと呼ばれるプロトタイプを開発しました。 そのベストプラクティスを集めたものが、GraphQL となりました。","例:複数のリクエストを何度も往復する","複数のリクエストをする例が、次のページに書いています。","https://www.howtographql.com/basics/1-graphql-is-the-better-rest/","例として、ユーザー情報、ユーザーが投稿したコンテンツ、ユーザーのフォロワーという 3 つの情報を取得するケースです。 REST API の場合は、次の画像のように 3 往復することになります。 GraphQL の場合は、1 回の往復だけでデータが取得できます。","REST API から GraphQL へ","REST API から、GraphQL に切り替えた結果、次の 3 つのメリットを享受することができました。","Fast on network","必要なものだけを記述できるため、1 回のリクエストで十分です","Robust static types","どのようなデータが利用可能か、どのような型か、クライアントは知ることができます","Empowering client evolution","レスポンスのフォーマットはクライアントが制御できます。そのため、サーバーサイドはシンプルになり、メンテナンスも容易になります 古いフィールドを非推奨とし、機能は継続できます。この後方互換性によりバージョニング管理が不要になります","3 つの課題を 改めて考える","REST API の 3 つの課題を、2022 年の今、改めて考えてみます。","2012 年は、3G 回線が普及していた","複数リクエストや、API のペイロードが大きいと、ネットワークレイテンシに大きく影響していそう","2022 年は、5G 回線が普及している","ネットワークレイテンシは、そこまでクリティカルな問題にはならないのでは もちろん、低ネットワークを利用するユーザーが多いプロダクトなら、考慮が必要かも","複数リクエストは、BFF のようなファザードを建てることで、解決できないか","バージョニングと後方互換性については、今の REST API も変わりなく課題の 1 つ","スキーマ駆動な開発で、問題解決できるのではないか","簡単に書いていますが、3 つの課題は、もっと深い・困難な話だったのかもしれません。 ですが、今の時代で考えてみると、GraphQL を使うユースケースは、エッジケースなのかなと思ってしまいました。 本件の課題の根幹の 1 つは、ニュースフィードにおけるデータ構造の複雑さ(再帰的,ネスト)じゃないのかなと想像していました。","参考リンク","https://www.apollographql.com/blog/graphql/basics/why-use-graphql/ https://wundergraph.com/blog/why_not_use_graphql","GraphQL の魅力","データの取捨選択","GraphQL の必要なデータを記述できる機能は、魅力的と思います。 従来の REST API の開発設計では、次のようなパターンを業務で経験してきました。","レスポンスデータのバリエーションをグループ分けするクエリパラメータ Response group","Response group","small","最小セット","middle","small と large の中間","large","全てのフィールド","Response group での開発で、特に大きな課題と感じたことはありませんでした。 データの取捨選択は、API のスケールのしやすさがメリットのように思います。","データの階層構造","GraphQL は、リクエスト・レスポンスのデータに、階層構造を表せます。 これも、魅力的です。 従来の REST API では、リクエストのクエリパラメータは、フラットな形で送るしかありませんでした。リクエストボディを使って、JSON を送るという手段もあります。(まあ、これが GraphQL なんですが) REST API のリクエストに、階層構造を表せるのは、データの関係性を示せるため、柔軟性が高く良さそうです。","ただ、個人的な違和感","データ参照も POST","REST API は、参照なら HTTP GET、更新なら HTTP POST を使うのが当たり前です。 GraphQL は、参照も更新も HTTP POST を使います。これに違和感があります。 query は、HTTP GET、mutation は、HTTP POST で使い分けできるようにしたいです。","終わりに","GraphQL の歴史を簡単に紹介しました。 まだそんなに使ったことがないので、良さ・悪さをしっかり理解していきたいと思います。"],"t":"GraphQLの歴史"},{"f":"src/routes/blog/contents/a_mockable_unit_testing_method_that_can_be_completed_only_with_big_query/index.mdx","c":["BigQueryだけで完結するモック可能なユニットテスト手法","BigQuery、皆さん使っていますか？ 私は、業務で BigQuery を使ったデータ構築をしています。 品質担保のため、BigQuery の SQL に対してテストをしたいと考えています。 本記事では、BigQuery だけで完結し、かつ、Mock データを差し替え可能なユニットテスト手法について、紹介します。","動機","端的に言うと、BigQuery の SQL 改修時にデグレが発生していないか確認したいです。 業務で BigQuery の SQL を書いているのですが、それに対するユニットテストがありません。 Python や Javascript のような言語でアプリケーション開発する場合、XUnit 等のユニットテストフレームワークでユニットテストを書くのは、よくあると思います。 しかし、SQL に対するユニットテストというのは、(私の観測範囲上) あまり聞いたことがありません。 dbt(Data Build Tool)というツールを使えば、SQL へユニットテストをかけるようですが、私はそれの良さ・悪さを知りません。(興味本位で、試してみたい) それよりも、新しいライブラリ・ツールを覚えるのではなく、その言語の","標準的な技術","を用いて、SQL のユニットテストが書けないかと悩みました。 そこで、私なりに BigQuery の SQL に対するユニットテスト方法を考えてみました。","xUnit","xUnit は、ユニットテストのためのフレームワークです。","xUnit is the collective name for several unit testing frameworks that derive their structure and functionality from Smalltalk's SUnit.","※ https://en.wikipedia.org/wiki/XUnit xUnit は、各プログラミング言語に影響を与えました。 Java なら JUnit、Python なら unittest(厳密には JUnit から触発)のユニットテストができます。","xUnit x Python","BigQuery の前に、まず Python を使ったユニットテストを紹介します。 『100 円未満の果物を取得する』関数、","があったとします。","関数は、簡易的なモノで、実際には実データを参照する箇所になります。","関数に対してユニットテストを書くとすれば、次のコードになります。","にあるように、ユニットテストは、次の 3 段階で分けて書くと、分かりやすいです。","Arrange: 配置, 準備 Act: 実行 Assert: 検証","ユニットテストは、次のコマンドで実行できます。","テストは成功しました。今度は、失敗させてみましょう。 fruits.py のコードにある ","のロジックを誤ったものに変更します。 コードは、次のとおりです。","この状態で、さきほどのユニットテストを実施してみます。","期待通り、失敗しました。 ","ロジックを誤る","というのは、保守を続けていくと、","ほぼ間違いなく発生","します。 その際、","ロジックが誤っている"," と気付ける、つまりデグレを気づける事が大切です。 ユニットテストを書くメリットの 1 つは、この ","デグレを検知できるところ"," だと思っています。","xUnit x BigQuery","BigQuery の場合、どのようにテストすればよいのでしょうか。 まず、BigQuery の SQL を書くために元データを用意します。","shop.fruits テーブルから、先程の Python コードにあった","関数相当の SQL を用意します。","出力された","テーブルに対して、ユニットテストを書いてみます。","このテストにより ","テーブルに対して、次のことを保証できました。","price が 100 未満の fruit は 1 つ","ただし、これは実データ(shop.fruits テーブル)を参照して生成されたデータです。 そのため、次の問題があります。","実データを参照しているため、テストが不安定になる","実データに","がなくなると、テストが失敗する","フィードバックサイクルが長い","実データが多くなると、テスト実行時間が長くなる","そこで、テーブル関数というものを活用し、実データをモックデータに差し替えるようにします。","テーブル関数とは","BigQuery の公式ページより、テーブル関数について、次のことが書かれています。","テーブル関数（テーブル値関数、TVF とも呼ばれます）は、テーブルを返すユーザー定義関数です。テーブル関数は、テーブルを使用できる場所であればどこでも使用できます。テーブル関数はビューと似ていますが、テーブル関数ではパラメータを取得できます。","※ ","テーブル関数の定義は、次のようなサンプルコードがあります。","特徴は、"," です。関数を定義し、テーブル情報を返り値とします。今回のケースでは、"," のフィールドを持つテーブルです。 定義したテーブル関数は、次のように使います。","テーブル関数は、","に使えます。私は、ここに着目しました。 FROM 句で関数呼び出しする際、モックデータを返却できるようにできないかと考えました。 そこで、次の章にある手段を発見しました。","xUnit x BigQuery (Mock)","どのように差し替えるかというと、次の 2 つの順番にテーブル関数化していきます。","元データを Mock 差し替えできるように、テーブル関数化 ロジックを含んだ SQL の FROM 句を、① のテーブル関数に差し替えて、テーブル関数化","順を追って説明します。 ① 番目は、"," です。 元データは、今回","テーブルですので、Mock で差し替えられるようにテーブル関数化します。 次のコードが、Mock 差し替え可能なテーブル関数です。","この SQL には、2 つのステートメントがあります。","テーブル関数 shop.fruits_inject の定義 テーブル関数 shop.fruits の定義","1 つ目は、後で Mock 差し替え(上書き)するための関数です。 2 つ目は、"," という値を引数とした関数で、次の条件分岐があります。","is_test が True の場合","モックデータ(shop.fruits_inject)を返却","is_test が False の場合","実データ(shop.fruits)を返却","プロダクションコード時は、is_test を False とし、テストコード時は、is_test を True として使う想定です。 ② 番目は、"," です。 ロジックを含んだ SQL、今回は、","テーブルを関数化します。 次のコードが、① のテーブル関数で差し替えたテーブル関数です。","FROM 句に、先程 ① 番で定義した shop.fruits テーブル関数を呼び出します。 shop.fruits テーブル関数の引数は、shop.fruits_less_than 関数から渡ってくる","をそのままセットします。 また、shop.fruits_less_than 関数には、","という引数も持ち、less_than の price を柔軟に対応できるようにします。 これにて、テーブル関数化を終えました。次は、プロダクトコードとテストコードを紹介します。 では、まずプロダクションコードを書きます。","取り上げて重要なことは、ありません。プロダクションコードは、is_test を False とします。 次が、本記事のメインである、","BigQuery の SQL に対するユニットテストコード","です。","この SQL は、3 つのステートメントがあります。","shop.fruits_inject 関数の上書き(","モックの差し込み",") shop.fruits_less_than 関数で、is_test=True として生成し、一時テーブルで保存 ② の一時テーブルに対してアサーション","重要なのが、① 番です。shop.fruits_inject 関数を上書きします。 そうすると、shop.fruits 関数で参照する shop.fruits_inject 関数結果が、上書きされたものに切り替わります。 後は、② 番で shop.fruits_less_than 関数を呼んで一時テーブル保存し、③ 番でアサーションします。 では、テストを失敗させてみましょう。ロジックを含んでいる shop.fruits_less_than 関数を、次のように書き換えます。","WHERE 句の price が 100 未満ではなく、100 以下になっているので、","と","の 2 つが抽出されます。 この関数を再度定義し、もう一度テストを実行してみましょう。 そうすると、次のようなメッセージが出力されます。","期待通り、テストは失敗しました！🎉 BigQuery のユニットテストができるようになれば、次は CI に組み込みたいと思うはずです。 BigQuery は、GCP のサービスなので、GCP の CI サービスとして有名な Cloud Build を活用しようと思います。 Cloud Build の定義ファイルを、次に示します。","Cloud Build を Github 等の Git プラットフォームと連携すれば、Git のイベント毎(Push,Merge,etc)にユニットテストを動かすことができます。 ※ サンプルの cloudbuild.yaml にある、","や","、","は何度も実行するものではないのですが、 記事をわかりやすくするために書いています。","xUnit x BigQuery メリット・デメリット","BigQuery の Mock を差し替え可能なユニットテストについて、メリット・デメリットを列挙します。","メリット","BigQuery だけ学習すればよい","BigQuery の標準機能だけで、完結しているため","フィードバックサイクルが短い","実データを参照するのではなく、モックデータを参照しているため","テストが安定する","デメリット","BigQuery の SQL 実行順序を制御する必要あり","今回でいうと、次の順番でクエリ実行する必要がある","fruits_tvf.sql","fruits_less_than_tvf.sql","test_fruits_less_than_100.sql","回避案","BigQuery の Scripting を利用 Cloud Build の","を利用","並列実行すると、予期せぬ動作になる","で定義","shop.fruits_inject 関数が、何度も再定義されている"," は未サポート 回避案","BigQuery のトランザクションを利用","テーブル関数は未サポート","汎用性がない","他のデータベースエンジン(MySQL,PostgreSQL,etc)に、同じ手法(テーブル関数)は使えない","dbt の利用(多分)","終わりに","今回、この手法を使うことで、BigQuery の SQL に対してユニットテストを書けることがわかりました。 これにより、データに対する品質を一定担保しながら、安全に開発できるようになります。 他にも、次のようなテスト手法についても検討して良いかもしれません。","dbt によるデータモデルテスト Open Policy Agent(OPA)によるポリシーテスト","SQL を AST 分解 →JSON 化し、OPA でテスト","BigQuery のクライアントライブラリを使った、ユニットテスト","P.S.","dbt を試したいと思っています。"],"t":"BigQueryだけで完結するモック可能なユニットテスト手法"},{"f":"src/routes/blog/contents/algolia_community_party_in_kyoto/index.mdx","c":["Algolia Community Party in 京都 - 2019年5月10日 参加レポート","https://algolia.connpass.com/event/128524/ こちらに参加しましたので、ご報告までに記事を書こうと思います。","Algolia って？","百聞は一見にしかず","まずは下記サイトで、色々検索してみて下さい！","Algolia Community Sample","実際に Product として使われているサイト","どれも","爆速","に結果が返ってきませんか !? これ、 実は","SaaS","で動いているんですよ ?","概要","https://www.algolia.com/","Products to accelerate search and discovery experiences across any device and platform.","Algolia は、全文検索を提供してくれる SaaS です。 全文検索を使う場合、一般的には Elastic Search や Solr といったものをサーバに乗せて管理することが多いかと思います。 使い始めると、「カテゴリ選択、ファセット絞り込み、ハイライト」等の機能がほしくなり、独自開発することもあると思います。 Aloglia では、そういった全文検索に関わる機能を SaaS として提供してくれます。 使われているところでは、ブログサービスである","や、オンライン決算処理である","がメジャーでしょうか。 エンジニア向けとしては、","にも使われています。また、Firebase の公式でも使用事例として紹介されています。これは驚きですね。 https://firebase.google.com/docs/firestore/solutions/search?hl=ja Algolia の会社としては、フランスから 2012 年よりスタートしました。 ベンチャー企業であり、日本人のエンジニア募集もあるそうです。 SaaS コミュニティ用のイベントがあるそうで、こちらに Algolia さんも登壇されています。 https://www.saastr.com/ https://www.saastr.com/watch-the-saastr-masterclass-from-0-to-10m-in-arr-from-algolia-in-paris-video/","どんな機能があるの？","お話を Algolia の Solution Architect である","から説明があったものとして、下記のような機能があるそうです。","検索時の表記ゆれ タイプミス補助 カテゴリ、ファセットによる絞り込み 検索キーワードのハイライト パーソナライゼーション A/B テスト GEO 検索 画像検索 音声検索","他にも「安い」と検索すると設定次第で「500 円以下の商品」を表示させるような こともできるそうです。あとは、チャットボットにも使えるとのことです。 ※ ただ、まだ日本語には対応していないみたいで、現在開発中とのこと。","どうやって使うの？","https://github.com/algolia OSS としてライブラリを提供されています。 手っ取り早く使いたいときは、instantsearch.js でしょうか。 https://community.algolia.com こちらも参考になるかと思います。","Algolia は知っていたの？","知っていました。 Algolia を知ったきっかけは、大学時代の友人(id:castaneai)からでした。 Algolia は、全文検索システムを構築せずとも、お手軽に使えて、しかも高機能な SaaS ということで、 個人開発をする私にとって興味を持ち始めました。 その後、下記の記事で書いた通り OSS Gate の対象にもさせてもらいました。 https://tech-blog.monotaro.com/entry/2018/10/17/115442 また、作りたいものリストに溜まっていたアプリを作る時間があったので、 最近では、下記のような書籍管理を作りました。検索は Algolia を使っています。 https://github.com/silverbirder/book-store-vue","なぜ会場が、はてな株式会社なの？","はてなの社長である id:chris4403 さんが、","と前職での知り合いだったからだそうです。 https://mackerel.io/ja/ はてなも、サーバー監視サービスである mackerel（鯖）を SaaS として提供しています。 このお二方が、前職を離れてからも、同様の事業に携わっているということに、不思議な縁だな〜と思いました。 会場では、ピザを提供して頂きました。美味しかったです！ごちそうさまでした！ https://twitter.com/silverbirder/status/1126841269097865216?s=20","最後に","SaaS は、その専門の技術を持ってサービス提供をされています。 独自に開発するよりも、そういった専門の SaaS を駆使することで、 開発コストや運用コストの削減につながります。 そもそも、そういった専門技術を持っていない環境は、独自開発することとなり、 学習コストもかかりますし、非機能要件を考えないといけないので大変です。 「かけるべき部分に時間を割いて、それ以外は SaaS に移す」というのは今の時代の効率良い開発スタイルだなと思います。"],"t":"Algolia Community Party in 京都 - 2019年5月10日 参加レポート"},{"f":"src/routes/blog/contents/apache-beam-kotlin/index.mdx","c":["Apache Beam + Kotlin 開発 実践入門","どうも、こんにちは。Re:ゼロ 2 期 始まりましたね 👏、 "," です。 最近、仕事の関係上、Apache Beam + Kotlin を使うことになりました。それらの技術が一切知らなかったので、この記事に学んだことを書いていきます ✍️。 サンプルリポジトリは、下記に載せています。 https://github.com/silverbirder/apache-beam-kotlin-example/tree/master/src/main/kotlin","Apache Beam とは","https://www.st-hakky-blog.com/entry/2020/04/29/172220 ** Batch や Streaming を 1 つのパイプライン処理 ** として実現できるデータパイプライン、それが Apache Beam です。(Batch + Stream → Beam) 言語は、Java, Python, Go(experimental)が選べます。 また、パイプライン上で実行する環境のことをランナーと呼び、Cloud Dataflow や Apache Flink、Apache Spark などがあります。 ※ Streaming 処理は、サーバーの能力がボトルネックになりがちです。そこで、Cloud Dataflow という GCP のマネージドサービスを使用すると、その問題が解消されます。 機械学習など豊富な ** 分析ライブラリ ** を使いたい場合は、Python、 ** 型安全な ** 開発をしたい場合は、Java を選べば良いかなと思います。 今回は、Java を選びました。モダンな書き方ができる Kotlin でコーディングします。","セットアップ","ソフトウェアバージョンは、次のとおりです。","IDE として intelliJ を使用しており、Kotlin SDK(1.3.72)が内蔵しています。","パイプライン処理の概要","PCollection は、ひとかたまりのデータセットだと思って下さい。 よくあるサンプルコード "," を例に進めます。 ※ 元々は、","があったのですが、ローカルマシン単体で動かせないため、多少アレンジしました。WordCount は、ある文章から単語を抽出しカウントを取るだけです。 メインのコードは、こちらです。動かすときは、IDE からデバッグ実行します。（この辺りは省略します。詳しくは Makefile を見て下さい 🙇‍♂️）","PTransform","Apache Beam のコアとなる PTransform についてサンプルコードを載せます。","ParDo","ParDo は、PCollection を好きなように加工することができます。 最も、柔軟に処理を書くことができます。","GroupByKey","Key-Value(KV)の PCollection を Key でグルーピングします。","Kotlin では、Iterable が動作できないため、Java の Iterable を使う必要があります。","Flatten","複数の PCollection を 1 つの PCollection に結合します。","Combine","PCollection の要素を結合します。 GroupByKey の Key 毎に要素を結合する方法と、PCollection 毎に要素を結合する方法があります。 今回は、GroupByKey のサンプルコードです。","Partition","PCollection を任意の数でパーティション分割します。","Streaming と Windowing","パイプラインを、そのまま使えば Batch 実行となります。 Batch は、有限のデータに対し、Streaming は無限のデータに対して使います。 無限のデータを処理するのは、Windowing というものを使い、無限を有限のデータにカットして、処理します。 Streaming 処理するためには、下記のようにコードにします。","テストコード","Apache Beam もテストコードが書けます。 サンプルコードは、","です。 実行するパイプラインを TestPipeline にすることで、テストができます。","終わりに","Apache Beam は、他にも Side input や Additional outputs などがあります。 使いこなせるためにも、これからも頑張っていきます！ さて、Re:ゼロ 2 期を見ましょう 👍"],"t":"Apache Beam + Kotlin 開発 実践入門"},{"f":"src/routes/blog/contents/ara-framework/index.mdx","c":["Ara-Framework で Micro Frontends with SSR","import { Image } from \"~/components/image/image\"; みなさん、こんにちは。silverbirder です。 私の最近の興味として、Micro Frontends があります。 https://silverbirder.github.io/blog/contents/microfrontends 今、Ara-Framework というフレームワークを使った Micro Frontends のアプローチ方法を学んでいます。","Ara-Framework とは","Build Micro-frontends easily using Airbnb Hypernova","※ ","Ara-Framework は、Airbnb が開発した Hypernova というフレームワークを使って、Micro Frontends を構築します。","Airbnb Hypernova とは","A service for server-side rendering your JavaScript views","簡単に説明すると、Hypernova はデータを渡せばレンダリング結果(HTML)を返却してくれるライブラリです。 これにより、データ構築とレンダリングを明確に分離することができるメリットがあります。","Ara-Framework アーキテクチャ","Ara-Framework のアーキテクチャ図は、次のようなものです。 ※ ","構成要素は、次のとおりです。(↑ の公式ページにも説明があります)","Nova Proxy","ブラウザのアクセスを Layout へプロキシします。 Layout から返却された HTML をパースし、Hypernova のプレースホルダーがあれば、Nova Cluster へ問い合わせします。 Nova Cluster から返却された HTML を、Hypernova のプレースホルダーに埋め込み、ブラウザへ HTML を返却します。","Nova Directive (Layout)","全体の HTML を構築します。Hypernova のプレースホルダーを埋め込みます。 Node.js, Laravel, Jinja2 が対応しています。","Nova Cluster","Nova Binding を管理するクラスタです。 Nova Proxy と Nova Bindings の間に位置します。","Nova Bindings (Hypernova)","データを渡されて、HTML をレンダリングした結果を返します。 (Hypernova をここで使います) React, Vue.js, Angular, Svelte, Preact が対応しています。","このように、Layout と Rendering (Nova Bindings) を明確に分けることで、独立性、スケーラビリティ性が良いのかなと感じます。 各レイアの間にキャッシュレイヤを設けることでパフォーマンス向上も期待できます。 詳しくは、公式ページをご確認下さい。","Ara-Framework サンプルコード","Ara-Framework を実際に使ってみました。サンプルコードは下記にあげています。 https://github.com/silverbirder/micro-frontends-sample-code-2 package.json はこんな感じです。 package.json","作っていく手順は、次の流れです。","Nova Proxy を作成 Nova Directive (Layout) を作成 Nova Cluster を作成 Nova Bindings (Hypernova) を作成","Ara-Framework を使うためには、次の準備をしておく必要があります。","Nova Proxy は、Nova Directive へ Proxy しますので、その host を書きます。 nova-proxy.json","また、Nova Proxy は、Nova Cluster へ問い合わせするため、"," という変数に URL を指定する必要があります。 Nova Proxy を動かすときは、次のコマンドを実行します。","Nova Directvie は、"," を使います。 これは、Node.js の handlebars テンプレートエンジン(hbs)で使えます。 Express の雛形を生成します。","詳細は割愛しますが、次の HTML ファイル(hbs)を作成します。 ※ 詳しくはこちら ","layout/index.hbs"," が Hypernova のプレースホルダーである "," です。 name は、Nova Bindings の名前 (後ほど説明します)、data-*は、Nova Bindings に渡すデータです。 また、script で client.js を load しているのは、CSR を実現するためです。 動かすのは、Express を動かすときと同じで、次になります。","Nova Cluster は、Nova Bindings を管理します。 views.json","Search や Product は、後ほど作成する Nova Bindings の名前です。server は、Nova Bindings が動いている URL です。 Nova Cluster を動かすときは、次のコマンドを実行します。","Nova Bindings","Nova Bindings を作るために、次のコマンドを実行します。","そこから、自動生成されたディレクトリから、少し修正したものが次のとおりです。 search/Search.jsx","今までの説明ではなかったですが、Nova Bridge である "," を使っています。 これは、Nova Directive に似ているのですが、使えるファイルが React や Vue.js などの JS フレームワークに対応しています。 そのため、Nuxt.js や Next.js,Gatsby.js にも使えるようになります。 ※ わかりにくいですが、このサンプルの Nova Bridge は、CSR で動作します。SSR で動作させるためには、Nova Proxy を挟む必要が (たぶん) あります。 product/Product.vue","Nova Bindings のこれらを動作させるためには、次のコマンドを実行します。","動作確認","今まで紹介したものを同時に実行する必要があります。 そこで、"," を使います。","動作として、次のような画像になります。","最後に","繰り返しますが、Ara-Framework を使うとデータ構築(Nova Directive)とレンダリング(Nova Bindings)を明確に分離できます。 また、レンダリング部分は、それぞれ独立できます。今回紹介していない API 部分は、誰がどのように管理するのか考える必要があります。 ただ、Nova Bindings で使用する CSR 用 javascript は、重複するコードが含まれてしまい、ブラウザロード時間が長くなってしまいます。 そこで、webpack 5 から使えるようになった Federation 機能を使って解決するとった手段があります。 Ara-Framework の紹介でした！"],"t":"Ara-Framework で Micro Frontends with SSR"},{"f":"src/routes/blog/contents/arch-unit/index.mdx","c":["TypescriptでArchUnitしてみた","import { Image } from \"~/components/image/image\"; ArchUnit をというものを最近知りました。依存関係のテストができるそうです。さっそく試してみたいと思いますので、その備忘録として残しておきます。","ArchUnit","https://www.archunit.org/","ArchUnit is a free, simple and extensible library for checking the architecture of your Java code using any plain Java unit test framework. That is, ArchUnit can check dependencies between packages and classes, layers and slices, check for cyclic dependencies and more. It does so by analyzing given Java bytecode, importing all classes into a Java code structure.","Java のアーキテクチャをテストできるライブラリで、パッケージやクラス、レイヤー、スライス（？）の依存関係をテストできるそうです。 そこで、親の顔よりも見たこの図をテストしたいと思います。","Typescript でも ArchUnit したい","ArchUnit は Java 製です。私は Typescript の ArchUnit がしたいです。 そこで、良さげなライブラリを発見しました。 https://github.com/MaibornWolff/ts-arch 特に拘りなく、アーキテクチャのテストができれば何でも良いかなと思います。 極端な話、ソースコードを AST パースし、依存関係を抽出できれば自作できるんじゃないかと思います。","試してみた","試したソースコードは、下記に置いています。ご参考下さい。 https://github.com/silverbirder/try-archunit 全体のソースコードツリーは次の構成です。","各プロダクトコードは、下の階層のファイルを import しているだけとします。","下記ファイルにある UML のコンポーネント図で依存関係を表します。","UML を可視化すると、下記の図のとおりです。 テストコードは、下記のとおりです。","このテストケースは PASS します。 では、違反コードを書いてみます。","3 レイヤーが上位の 4 レイヤーを使用しています。この状態でテストを実行すると、 見事 Failed となりました。つまり、依存関係の誤りを自動的に検出することができます。","最後に","規模が大きなプロジェクトほど、依存関係が複雑になりがちです。(Java でいう) パッケージやクラスの依存関係を適切に設計できていたとしても、誰かが壊しかねません。せっかく設計したのに壊されるのは、とても残念なので、テストコードで守ってあげましょう！"],"t":"TypescriptでArchUnitしてみた"},{"f":"src/routes/blog/contents/automated_synchronisation_using_github_actions_and_pull_requests/index.mdx","c":["GitHub ActionsとPull Requestを活用した、同期の自動化","import { Image } from \"~/components/image/image\"; あけまして、おめでとうございます。神社のおみくじで、人生はじめて大吉を引きました、silverbirder です。 普段の業務で、Figma のデザイントークンや API のスキーマファイル、i18n のメッセージファイルなどを、フロントエンドへ同期するコミュニケーションが不毛に感じています。そこで、GitHub Actions と Pull Request を活用して、同期コミュニケーションを削減する仕組みを紹介します。 目新しい情報はないかもしれませんが、同じお困りごとを持つ人へ助けになれば、幸いです。","GitHub Actions で使用するもの","今回紹介する仕組みの核となるのが GitHub Actions の repository-dispatch トリガーです。 https://docs.github.com/ja/rest/repos/repos?apiVersion=2022-11-28#create-a-repository-dispatch-event このトリガーは、GitHub API を経由して、GitHub Actions のワークフローを起動することができます。そのため、次のように 異なるリポジトリでの GitHub Actions ワークフローを連携できます。 repository-dispatch と create-pull-request は、次の GitHub Actions です。 https://github.com/peter-evans/repository-dispatch https://github.com/peter-evans/create-pull-request","respository-dispatch","repository-dispatch-event を dispatch する Action","create-pull-request","Pull Request を作成する Action","これらの GitHub Actions を使わずに "," などを使って代替できますが、便利なモノを使って楽をします。","GitHub リポジトリ以外からのトリガー","GitHub のリポジトリ(username/other)からトリガーだけでなく、他のサービスからでもトリガーできます。例えば、Google Sheets からだと、Google Apps Script から GitHub API を呼べばよいです。 他にも、Kibela の outgoing webhook を、Server が受けて、Server が GitHub API を呼び出す方法があります。 Server は、IFTTT や Zapier のようなサービスでも良いですし、自前のサーバーでも良いでしょう。","自動 commit","schema ファイルから、型を生成したい(yarn codegen)こともあると思います。そういうときは、次のフローを追加します。 git-auto-commit-action は、変更したファイルを git commit するだけの Action です。 https://github.com/stefanzweifel/git-auto-commit-action create-pull-request だけでも、自動 commit することができます。私は、次のケースで使用しました。","Figma の","で、Figma 上から Pull Request を作成する。","GitHub Actions で、style dictionary の build したものを commit したい","Preview","Figma のデザイントークンや、i18n のメッセージファイルを更新したとき、Preview できる仕組みがあると、画面の確認ができて、良いです。 例えば、vercel や chromatic の preview です。 https://vercel.com/docs/concepts/deployments/preview-deployments https://www.chromatic.com/docs/review","サンプルコード","i18n のメッセージファイルをフロントエンドへ同期する GitHub Actions を、紹介します。 | repository | やること | | ----------------- | ------------------------------- | | username/frontend | i18n のメッセージファイルを利用 | | username/message | i18n のメッセージファイルを管理 |","受け入れテストをマークダウンで管理","安心してマージできるように、受け入れテストを整備しておきましょう。 具体的には、","で仕様書を","で管理します。 例えば、次のような仕様書です。","この Markdown も、GitHub Actions で Pull Request するフローに載せましょう。新しいシナリオが追加された場合、(cucumber のライブラリ上) テストコードが存在しないとエラーとなります。 機能で担保したいシナリオを Markdown で管理していくことで、次のメリットがあります。","仕様が明確になる CI で受け入れテスト(cucumber)を動かし成功すると、仕様を満たす状態 となる","ハマったこと","GitHub Actions Bot の commit で、他のワークフローをトリガーできない","https://github.com/orgs/community/discussions/27028 token に、PAT を渡すように変更すれば解決します。 他の解決策としては、workflow_run のトリガーを使えます。 https://docs.github.com/ja/actions/using-workflows/events-that-trigger-workflows#workflow_run ただし、デフォルトブランチでのみ動作します。","repository-dispatch の POST は、JSON で制限がある","https://github.com/peter-evans/repository-dispatch#client-payload 同期したいファイルを json に変換して、dispatch する event ペイロードに含めようと、当初考えていました。ただ、次の懸念があったため、却下しました。","json にしてしまうとコメントが消える JSON のバイトサイズに上限がある","そこで、同期したいリポジトリの github.ref を event ペイロードに含めて、event を受けた側がソースコードをチェックアウトして使う方針に切り替えました。","終わりに","GitHub Actions と Pull Request を活用することで、自動的にアプリケーションのソースコードを更新する仕組みを簡単に組み立てられます。 このような Ops があれば、Slack でのメッセージラリーをする回数が減らせられます。ぜひ、ご活用ください。"],"t":"GitHub ActionsとPull Requestを活用した、同期の自動化"},{"f":"src/routes/blog/contents/automatic_generation_of_log-based_E2E_test_scenario/index.mdx","c":["ログベースのE2Eテストシナリオ自動生成案","モチベーション","E2E テストのシナリオってメンテナンスしていますか？ 機能が増えたときに、ちゃんとシナリオはアップデートできていますか？ 進化が激しいシステムだと、このメンテナンスがつらくなってきます。 『あれ、このシナリオなぜか動かなくなったぞ』 『タイミングによって、このシナリオ失敗するんだよね』 『最近増えたあの機能、シナリオでカバーされていないな』 耳が痛いです。","アイデア","アクセスログや、UI イベントログから、よく通るルート TOP○%のパターンを抽出し、 シナリオパターンリストを出力したい。それを E2E テストで動かすようにしたい。 そうすれば、シナリオを書かなくても、ログベースで主要なルートが E2E テストできます。 また、ログから新しいシナリオケースを導き出せれば、機能改善されてもシナリオも改善されます。 じゃあ、これだけで E2E はシナリオを書かなくて良いのでしょうか。どうなんでしょう。 できる限り、シナリオを書きたくないです。が、例えばクレジットカード支払いの UI イベントとかって、 セキュリティ上取得しちゃ駄目だと思います。ので、部分的にはシナリオを書かないといけないのかなと 思っています。"],"t":"ログベースのE2Eテストシナリオ自動生成案"},{"f":"src/routes/blog/contents/aws_summit_osaka_2019/index.mdx","c":["AWS Summit Osaka 2019 2019年6月27日参加レポート","import { Image } from \"~/components/image/image\"; 大阪のグランフロント大阪で開かれました「AWS Summit Osaka 2019」に参加してきましたので、 私の中で良かった３つのセッションを紹介したいなと思います。 https://aws.amazon.com/jp/summits/osaka-2019/ hastag はこちら ","私のメモはこちら https://scrapbox.io/silverbirder-memo/AWS_Summit_Osaka_2019","Amazon Sumerian による VR/AR/MR アプリケーションの開発","Amazon Sumerian の位置づけ","xR と呼ばれる３つの R について説明がありました。","xR","VR (virtual reality)","仮想の世界に没入","AR (augmented reality)","物理に仮想をオーバレイ","MR(mixed reality)","物理と仮想が相互作用","VR や AR については、広く知れ渡っていると思いますが、MR ははじめて聞きました。 VR は、Oculus Quest のようなヘッドセットで仮想世界に没入できます。 https://www.youtube.com/watch?v=BqM27iLnDJs AR は、ポケモン Go のようなアプリで現実世界に仮想のキャラクタを投影できます。 MR は、VR と AR の Mix みたいな感じですね。ヘッドセットをかぶりながら、現実世界に仮想世界が mix された景色が見えます。 代表的なものとして、Microsoft HoloLens があります。 Amazon Sumerian は、この VR/AR にフォーカスしたサービスになります。","xR アプリの課題","課題は下記の感じです。","ハードウェアが浸透していない 何が必要？ どうやって作る？ 使ってもらえるかわからない","私自身、xR のアプリを作ったことが１回だけありますが、同じような課題に悩んだことがあります。どうしても専用ハードウェアが必要になり、使ってもらうハードルが高くなってしまいます。","Sumerian の特徴","特徴は 4 つあります。","Web ブラウザベースの開発環境","開発する環境は Web ブラウザベースになるので、特別なものを用意する必要がないです。良いですね。","マルチプラットフォーム","モバイル、デスクトップ、VR ヘッドセット、AR プラットフォームに対応しています。これが 一番魅力的 なんじゃないかなと思います。開発者にとってもユーザーにとってもありがたいですよね。","Sumerian Host","セリフにあわせて口を動かしたりジェスチャーを行うキャラクターが 8 人いるそうです。こちらのキャラで開発する感じでしょうか？","AWS のサービスとの連携","AWS SDK を使って各種サービスを使えます。そのため、より柔軟なアプリケーションを開発することができます。","感想","xR は Web 好きな私でも興味がある技術です。Sumerian をつかうことで、xR の開発をよりスピーディに進めれるようなサービスと感じました。 実際触るかどうかは分かりませんが（無料枠使い切ってしまったので...)、こういった xR を開発するための手段を１つ知れたことは良かったと思います。 （他のクラウドサービスには xR 向けサービスないのですかね...?） https://aws.amazon.com/jp/sumerian/pricing/ https://aws.amazon.com/jp/sumerian/ ※ 下記のレポートもご参考下さい https://dev.classmethod.jp/cloud/aws/awssummit-2019-tokyo-h2-01/","クラウドネイティブなモダンアプリケーション開発入門","モダンアプリケーションのデザインパターン","今回紹介されたパターンは、マイクロサービスのデザインパターンのことを指しているのでしょうか。 https://microservices.io/patterns/microservices.html デザインパターンといえば、GoF のデザインパターンが有名ですね。 https://en.wikipedia.org/wiki/Software_design_pattern 最近では、分散システムにフォーカスした","があります。 今回登壇で話されいた内容を私が説明するより、下記のほうが十分に説明がありますので、そちらをご参考下さい。 https://qiita.com/yasuabe2613/items/3bff44e662c922083264","今回のセッションでは、少し駆け足になっていたせいか全て聞き取れなかった印象でした。 ただ、マイクロサービスデザインパターンの存在を知れてよかったです。 CQRS というパターンを業務上調査した覚えがあるのですが、マイクロサービスデザインパターンの １種だったんですね。知りませんでした。 デザインパターンというのは、先人の知恵が蓄積された素晴らしいカタログなので、 １度目を通しておこうと思いました。 ※ 下記のレポートもご参考下さい https://dev.classmethod.jp/cloud/aws-summit-2019-day3-a03-06/","クラウドネイティブがもたらすスケーラブルな開発、インフラストラクチャー、そして組織","Nulab の現状","Nulab のサービス","Nulab では",",","の３つプロダクトをもっています。 backlog では、ユーザー数が順調に伸びてきており、今年で 100 万人を突破したそうです。","backlog について","backlog には、4 つのサービスに分かれており、それぞれ Issues, Wiki, Gantt, Git があります。 前 3 つのサービスが Monolith で作られており、後 1 つのサービスが 3 つの言語(Perl, Python, Java)で作らていたそうです(Go で再実装されました)。 インフラ部分については(backlog の話に限らない...?)、クラスタが日本に 6 個、海外に 2 個存在し、インスタンスが 200 個もあるそうです。 それらは、Terraform+Ansible で管理するようにしていたそうですが、 物理ホストのメンテナンスに大きくコストがかかる という問題がありました。 また、コードベースが巨大化になると開発者、特に新規の人は理解するのに時間がかかってしまう問題もありました。","Kubernetes・EKS の導入","Backlog の問題点から、開発やインフラをスケールするため Kubernetes を検討するようになりました。 そこで(比較的規模が小さい?)Cacco に Kubernetes で動くように運用してみたそうです。Nulab ではコンテナのノウハウが蓄積されているので、効率よく進めれたそうですね。 しかし、kubernetes で運用していくと、ControlPlane の面倒を見るのが手間になってきます。そこで、マネージドサービスである EKS を使いはじめたそうです。 どういった点にメリット/デメリットがあるのか知るために、既存と新規を Nginx を通して平行提供したそうです。 運用を進めることで kubernetes や EKS のノウハウが蓄積され、Backlog に EKS を検討する材料を手に入れることができます。","Nulab さんの取り組みで勉強になったのは「小さなところから検討したい技術を導入し、ノウハウを蓄積する」ところです。 社内で実績がない技術をプロダクトとして導入するには、それなりに調査する必要があります。 また、その技術に明るい人がいれば導入までの工数は短くなると思いますが、大抵の場合、そういった人は少ないはずです。 そこで、Nulab さんのような取り組みをすると、低いリスクで大きなリターンが得られます。 小さいところからスタートするので、失敗してもリスクは少なくて済みますし、 運用ノウハウが蓄積できれば、拡大できます。 私も、プロダクトへ何度か提案したことがありますが、今回のポイントも検討してみたいなと思います。 ※ 下記のレポートもご参考下さい https://aws.amazon.com/jp/blogs/startup/summit-osaka-2019-racap/","全体感想","AWS Summit Osaka は今回が初めてだそうです。前回は震災の影響で中止になったみたいです。 AWS は、私がはじめて触ったクラウドサービスなので、今回参加してみました。 Sumerian ってものを知りませんでしたし、マイクロサービスデザインパターンも知りませんでした。 こういう大規模なセミナーでは、様々なジャンルのセッションが集まっているので、全く知らない領域のセッションを受けてみたり、より Deep なセッションを受けたりと面白いです。 関西に住んでいる私にとっては、こういった大規模セミナーは中々珍しいので、とてもありがたかったです。"],"t":"AWS Summit Osaka 2019 2019年6月27日参加レポート"},{"f":"src/routes/blog/contents/bmxug_kubernetes_watson_discovery/index.mdx","c":["【大阪】BMXUG勉強会 -Kubernates体験＆Watson Discovery入門- 2019年3月27日参加レポート","今回、k8s の体験を目的として参加したのですが、意外な収穫があったので、 共有したく、記事を書くことにしました。hashtag はこちら ","https://bmxug.connpass.com/event/117966/","Watson Discovery","Watson Discovery とは？？","簡単に言うと、IBM 製の検索 API になります。 全文検索システムでおなじみに ElasticSearch とは違った「IBM」ならではの機能が搭載されています。 また、無料で使えるとのことで、興味津々になってしまいました。","どんな特徴があるの？","なにかしらの文章データを WatsonDiscovery に渡してあげることで、 文章にあるコンテキストを種々様々な側面から抽出してくれます。 特に、大量のドキュメントを検索したいときに使う場合に活躍します。","メタ情報による検索","登録した文章データから良い感じのメタ情報を抽出してくれます。 たとえば、下記の属性があるみたいです。","エンティティ リレーション キーワード カテゴリー コンセプト セマンティックロール センチメント エモーション","例えば、怒っている文章を渡すと WatsonDiscovery では、 「怒りのエモーション」メタ情報を付与されます。ほえ〜！すごい！ 確かに独自な機能ですよね。 この機能は、日本語ではサポートされていませんでしたが、 2018 年 8 月ごろにできるようになったそうです！","隠れたコンセプトをみつける","文章にあるワードだけでなく、文章にないワードのコンセプトも見出してくれるそうです。 どうなってるんだ！？","クローラも提供されている(WebCrewl)","日本語のニュースを定期的にクロールして、ディスカバリーの辞書を更新してくれる機能もあるそうです。 手段の１つに、URL を指定するだけで勝手にクロールしてくれる方法もあります。簡単だ。。。 これを使えば、データを用意しなくて済みますし、お問い合わせ Q&A みたいなのが簡単に作れちゃいます。","サンプル紹介","例えば、「喜ばれるホワイトデーのプレゼント」で検索してみます。 また、メタ情報検索として、センチメンタル「ポジティブ」を指定。 （データは、WebCrewl で収集済み） 結果を、種々様々な形式のランキングを表示してくれました。 例えば、「いくらお金をつかうか？」というランキングでは、TOP が","無料","でした。これは面白かった。","visual insight (ベータ版)","ワードの距離を視覚的に見せてくれるそう。wordCloud もありました。","最後に","IBM ならではの、独特の全文検索システム API でした。 無料で使えるそうなので、時間があるときにでも使ってみたいなと思います！"],"t":"【大阪】BMXUG勉強会 -Kubernates体験＆Watson Discovery入門- 2019年3月27日参加レポート"},{"f":"src/routes/blog/contents/chrome_extension_development_feedback/index.mdx","c":["Chrome拡張機能(Manifest V3)の開発で知ったこと","import { Image } from \"~/components/image/image\"; 皆さん、Chrome 拡張機能をご存知ですか？ Chrome 拡張機能は、Chrome ブラウザをカスタマイズするための機能です。 私は、Chrome 拡張機能を過去(数年前)に 2 つ作っていて、その当時は、Chrome 拡張機能の仕様である Manifest V2 に従っていました。 そして、今再び、Chrome 拡張機能で作りたいものができたので、久々に作ろうと決意しました。 作ろうと思ったものの、どうやら今の Chrome 拡張機能の仕様は Manifest V3 を推奨しているようです。 そこで、今回、開発した際に知ったことをまとめようと思います。 ちなみに、実際に作ったものは次のものです。","https://github.com/silverbirder/chrome-extensions-tiktok-scraping-downloader","上にある図が、この Chrome 拡張機能の設計図になります","※ Chrome 拡張機能の概要について詳しく知りたい方は、","をご覧ください。","Chrome Extensions Components","Chrome 拡張機能は、主に次の 4 つのコンポーネントが存在します。","サービスワーカー上で動作し、ブラウザ上のイベント駆動(ページ遷移やブックマーク差所など)で反応します。","の ","フィールドで設定します。","Web ページのコンテキスト上で動作し、DOM へアクセスできます。","URL バーの右側にあるボタンを押した(Action)際に表示される UI です。 ブラウザ体験を損なわさない最低限の機能だけの提供を推奨されています。","Chrome 拡張機能アイコンを右クリックして、オプションを選択すると表示される UI です。 Chrome 拡張機能をカスタマイズしたい設定ページに使います。","私なりに、これらのコンポーネントの使い分けを考えると、次になります。","DOM へアクセスする必要がある","Content Scripts を使う","ページに依存しない処理がある","Background Scripts を使う","環境変数の設定が必要","Option Page","UI Elements は、基本的に必要ないのかなと思いました。","Debug","デバッグって、どうやるんでしょうか。","こちらにやり方が書いてありました。 私なりに解釈した結果、次の 2 つで使い分けるのかなと思います。","① そもそも、Chrome 拡張機能がロードできない場合","manifest.json ファイルに記述で誤りがあるなどで、Chrome 拡張機能がロードできない場面があります。 そういうときは、次の手順を実行します。"," へアクセス 次の図にあるような ERROR ボタンをクリック","恐らく、何かしらエラーメッセージが出力されていると思います。 それを解決しましょう。","② ① 以外の場合","Chrome 拡張機能はロードできるが、期待通りに動作しない場面があると思います。 そういうときは、DevTools を開きましょう。","Background Scripts の場合"," へアクセスし、","の右にあるリンクをクリック。(上図)","DevTools が開きます。","Content Scripts, UI Elements, Options Page の場合","UI 上で右クリックして "," をクリック","DevTools には、console タブがあるはずです。そこのログメッセージを確認しましょう。","Message Passing","各コンポーネント間で、通信するのは、どうしたら良いのでしょうか。 例えば、Content Scripts から Background Scripts へデータを渡したいときなどです。 次の資料が、参考になります。","資料を読むと、次のようなパターンの通信ができるようです。","各コンポーネント間の通信","Background Scripts ⇔ Content Scripts など","Chrome 拡張機能間の通信","A Chrome 拡張機能 ⇔ B Chrome 拡張機能 Chrome 拡張機能の ID を使って通信します","Web ページからの通信(Sending messages from web pages)","Web ページ ⇔ Chrome 拡張機能のコンポーネント","通信の具体的なコードは、","メソッドを使います。 Background Scripts から Content Scripts へ通信する場合、どの Chrome タブに送信するか","で事前に id を見つけておく必要があります。 また、後で紹介しますが、","でアクセス可能な Javascript を Web ページへ Inject(",")した場合、その Javascript と Content Scripts の通信は、","と","を使いましょう。 ","が使えないので。","Web Accessible Resources","Content Scripts から Web ページの DOM へアクセスできますが、window オブジェクトにある変数へアクセスすることができません。","window オブジェクトへアクセスするには、Web Accessible Resources を使う方法があります。","具体的にコードで説明しましょう。 manifest.json で必要なフィールドの例は、次のとおりです。","Content Scripts と Web Accessible Resources の Javascript は次のとおりです。","このように、web_accessible_resources.js を Web ページの body タグへ append します。 その web_accessible_resources.js では、window オブジェクトにアクセスすることができます。","chrome.webRequest API","Chrome ブラウザでネットワークトラフィックを監視する Chrome 拡張機能の API があります。 それが、","です。","これがあれば、Web ページでどういうリクエストが発生しているか分かるようになります。 manifest.json のフィールドで、","の設定が必要です。 サンプルで、Background Scripts のコードを紹介します。 まず、manifest.json の必要なフィールドを書きます。","次に、Web ページからリクエストが完了(onCompleted)したイベントを監視するコードを書きます。","この details にはリクエストの URL が含まれています。さらに詳しく知りたい人は、","をご確認ください。","最後に","Chrome 拡張機能、久々に開発してみると、進化しすぎていてキャッチアップに苦労しました。 私と同じような方の助けになれば、幸いです。"],"t":"Chrome拡張機能(Manifest V3)の開発で知ったこと"},{"f":"src/routes/blog/contents/circle_ci_backstop_visual_regression_testing/index.mdx","c":["CircleCI + BackstopJS (Puppeteer) でビジュアルリグレッションテストを継続的に監視する","CircleCI と BackstopJS を組み合わせて、『継続的に Web ページの視覚的な変化を監視するツール』を作成しました。 https://github.com/silverbirder/silver-enigma","Motivation","Web アプリを運用する上で、システム改善は継続的に行われます。 そのシステム改善をリリースする前に、入念なテスト（ユニットテスト、インテグレーションテスト、E2E テスト）をパスする必要があります。しかし、Web アプリの規模が大きくなるにつれて、","意図せずデグレ","が発生してしまう可能性が大きくなります。 そのデグレを気づくのが『社内部指摘』なのか『エンドユーザからのお問い合わせ』からなのか分かりません。 そこで、","Web アプリを定期的に監視すること","で、予想外なデグレッションを早期に発見できる仕組みが欲しくなりました。 必要とする要件は、次のとおりです。","スケジューリング実行可能","無料で使えること","わかりやすい視覚的変化のレポート カスタマイズしやすい監視方法","ここから、CircleCI + BackstopJS(Puppeteer)という組み合わせが生まれました。","Usage","次の手順で構築できます。","backstop.json に監視したい URL を設定 CircleCI に必要な環境変数を設定(","参照) CircleCI で Job を実行し、Artifacts にあるレポートを閲覧","Puppeteer を使って Web アプリへリクエストしています。 そのため、『Javascript を無効にする』や『Cookie を設定する』といったニーズに対応できます。","Conclusion","このツールにより、外部から Web アプリの変化を早期に発見できるようになりました。 ただし、リクエストするスケジューリング間隔には十分にお気をつけください。 リクエストをしすぎると、対象 Web アプリの負荷が高まってしまいます。"],"t":"CircleCI + BackstopJS (Puppeteer) でビジュアルリグレッションテストを継続的に監視する"},{"f":"src/routes/blog/contents/client_microfrontends/index.mdx","c":["クライアントサイド(ES Module)でMicro Frontends","import { Image } from \"~/components/image/image\"; 2021 年、あけましておめでとうございます。本年も宜しくおねがいします。最近、体重が増えてしまったため、有酸素運動を頑張っています。 本記事は、昨年の冬あたりから検証していた クライアントサイド統合での Micro Frontends について話そうと思います。検証したソースコードは、次のリポジトリにあります。 https://github.com/silverbirder/micro-frontends-sample-code-6","概要","全体設計イメージ図は、次のとおりです。 サーバーサイドは静的コンテンツを返すだけとし、クライアントサイドでアプリケーションを構築します。 構築手段は、ブラウザ標準である ES Module Import を使用し、アプリケーションに必要な Javascript(index.js, bootstrap 用)を load します。 UI に必要な各 Team の Component は、API 経由で取得し、レンダリングする構成になります。","Javascript Module","Javascript(index.js)には、次の Module を含めようと考えていました。","DOM Parser","任意の要素の情報を取得する","Imorter","任意の Module を取得する","Router","Web アプリケーション全体の Routing を管理する","Worker","バックグランドで実行する処理を管理する","EventHub","Module 間の通信を制御する","これらをざっと考えていた訳ですが、結局実装したのは Importer と Router ぐらいです (笑)。力尽きてしまいました。 また、前提として可能な限り各 Team の依存関係を独立するよう心がけます。Micro Frontends では、独立できてこそのメリットが享受できるため、できる限り各 Team の共通化は避けるようにします。","Javascript Importer","全体設計イメージ図にも書いていますが、Javascript の Importer は、Component Discovery API を通して、各 Team の Component を Import します。この構成は、Microservices の Service Discovery Patterns に似せています。この構成を取ることで、各チーム同士は独立(非依存)することができます。","Javascript Router","Router は、アプリケーション全体の Routing を管理します。例えば、"," は Top ページ、 "," は検索ページといった具合です。 Router には、後ほど説明する WebComponents との相性が良い vaadin/router を使用しました。 https://vaadin.com/router vaadin/router では、WebComponents を指定して Routing するため、指定された WebComponents は、Importer より取得します。","Component","Component は LitElement という WebComponents ベースのライブラリを使用しています。各 Team の Component(LitElement のライブラリ込)を Import していると、重複した load となりパフォーマンスがよろしくありません。共通ライブラリを事前に load (import map とかで)することをお勧めします。 WebComponents ということなので、Shadow DOM でレンダリングすることになります。CSS のスコープが独立できるため、他へ影響することはありません。ただ、全体的なブランドカラーを統一したい等 Design System がある場合、Component の共通化のやり方を(慎重に)考える必要があります。","Build Package, Design System, Performance Metrics","各 Team を独立したいといっても、共通化しないといけないことがあると考えています。私が想定しているものは、次のとおりです。","Design System","Component 全体のデザインを統一する","Performance Metrics","計測指標のルールを全体で統一する","Rendering Time Response Time etc","Build Package","ライブラリの扱い方を統一する","External ECM Version etc","と書いているだけで、実際に試した訳ではありません(笑)。","所感","Micro Frontends のサーバーサイド統合でもそうでしたが、Component を集約・提供するサービスは、クライアントサイド統合でも必要になりました。今回でいうと、Component Discovery API です。これは、Component 間の依存度を下げるためのレイヤーであり、Micro Frontends では、ほぼ必須の要素なのではないかと思います。","最後に","Micro Frontends は、統合パターンも大切ですが、もっと大切なのは、ドメインをどう分解するかだと思います。この分割が適切ではないとどうしても共通化しなければならないケースが誕生し、Micro Frontends のメリットが活かせないと思います。そろそろ、プロダクションレベルで検証したいと思いますが、中々重い腰を上げらない今日このごろです。"],"t":"クライアントサイド(ES Module)でMicro Frontends"},{"f":"src/routes/blog/contents/cloud_ide/index.mdx","c":["コロナ禍におけるエンジニアのためのCloud IDE","import { Image } from \"~/components/image/image\"; 2020 年 3 月頃からコロナが流行りだし、もう 12 月になります。働き方が大きく変わり、リモートワークが当たり前の時代となりました。 エンジニアの働き方も同様に変わりました。そこで、今回は Cloud IDE というものを紹介しようと思います。","リモートワークと DaaS","リモートワークが増えると、DaaS のようなサービスを利用する企業が増えたのではないでしょうか。 DaaS の簡単な説明を引用しますと、次のとおりです。","DaaS とは、“Desktop as a Service”の頭文字を取った略語で「ダース」と読みます。 普通ならば個人の PC にデスクトップは存在し、データは個人の PC 内に保存されていますが、DaaS においては個人のデスクトップがクラウド上に構築され、ネットワークを通じてそのデスクトップを呼び出して利用することになります。 ここでは、DaaS とはどういう仕組みなのかを説明し、その必要性、メリットについて詳しく述べていきます。","DaaS はクラウドサービスの一種で、特定のソフトウェアを端末にインストールすることなく、ネットワークを通じて利用できるという特徴があります。 クラウド上にあるデスクトップ環境を呼び出して利用できるため、個人の PC はディスプレイとキーボードなど必要最低限の機能があれば良いので、テレワークをするために高いスペックの PC を用意する必要はありません。","※ ","例えば、クラウド上で開発環境(お気に入りのエディタ, プログラミング言語, 使い慣れたツール, etc)を構築して、そこにアクセスして仕事をするようになります。アクセス元は、私物の PC や会社から支給されている PC などが多いと思います。","Cloud IDE","Cloud IDE は、クラウドにある統合開発環境(IDE)のことで、主にブラウザから操作できるようなものが多いです。 ざっと有名なものをリストアップしてみました。 | 提供元 | IDE 名 | | --------------------- | ------------------------ | | Microsoft Azure | Visual Studio Codespaces | | Github | Codespaces | | Amazon Web Services | Cloud9 | | Google Cloud Platform | Cloud Shell Editor | | Coder | Coder | | OSS | Gitpod | ブラウザで見ると、どんな UI でしょうか。いくつか例を載せておきます。 Cloud Shell Editor や Gitpod は、OSS の ","Theia"," というものを使っています。 また、全体的に UI がとても似ていますよね。これは、次の記事でわかりやすく説明されていますので、ご興味があればお読みください。 https://qiita.com/monamour555/items/f93287c273a388261968 これらの Cloud IDE は、ここ最近 Publickey でよく目にします。記事と投稿日時をまとめてみました。","2020 年 4 月 3 日 投稿","2020 年 5 月 7 日 投稿","2020 年 9 月 7 日 投稿","2020 年 9 月 11 日 投稿","2020 年 11 月 10 日 投稿","稚拙な推測ですが、リモートワークが普及し、働く環境も変化したためかなと思っています。 物理的な PC で開発するのではなく、クラウド上にある PC で開発する、それが当たり前になるのかなと。","Theia とは何か、Github の about より引用します。","Eclipse Theia is a cloud & desktop IDE framework implemented in TypeScript","この OSS の興味深いところの 1 つに、設計書が公開されているところです。 https://docs.google.com/document/d/1aodR1LJEF_zu7xBis2MjpHRyv7JKJzW7EWI9XRYCt48 Theia は、ローカルで動かすことができます。Web アプリだけじゃなく、ネイティブアプリ(Electron)もあります。 https://github.com/eclipse-theia/theia/blob/master/doc/Developing.md#quick-start また、Docker コンテナも公開されています。 https://github.com/theia-ide/theia-apps ベンダーニュートラルなので、VM インスタンスに Theia を入れて独自に運用するなど、ベンダーに依存しません。","個人的な話","個人的に、Gitpod を使いたいのですが無料だと月 50 時間までしか使えません。 https://www.gitpod.io/pricing/ \"Professional Open Source\" というものを応募したところ、"," へ招待頂き、公開リポジトリの無制限利用ができるようになりました。","Gitpod を使い続けて思うこと","Gitpod は、.gitpod.yml というファイルで環境構築されます。 https://www.gitpod.io/docs/configuration/ ベースとなる Docker イメージを指定して、必要なライブラリを事前にインストールできたりします。 公式ブログに、Gitpod の完全ガイドがあります。 https://www.gitpod.io/blog/gitpodify/ また、様々な OSS を Gitpod で簡単に動作確認できます。","実際に Gitpod を使ってみると、確かに便利です。 アクセス元の PC は、非力なノート PC でも良く、Github の Repository 毎に Gitpod のコンテナがあるため、相互に影響しません。 ただ、ネットワーク遅延でちょっと待ったり、Gitpod のショートカットキーより、ブラウザのショートカットキーが上書きされて困ることが多少あります。","終わりに","ブラウザ上で開発するのって、昔からあったように思いますが、あんまり注目されていなかったのでしょうか（私が無知なだけかもしれません）。 AWS や GCP、Github など各社が積極的に手を出しているところを見ると、これからますます期待できる分野なのだと思います。"],"t":"コロナ禍におけるエンジニアのためのCloud IDE"},{"f":"src/routes/blog/contents/cloud_native_days_tokyo_2019/index.mdx","c":["Cloud Native Days Tokyo 2019 -2019年7月22-23日参加レポート","import { Image } from \"~/components/image/image\"; 今回、東京で開催されました Cloud Native Days Tokyo 2019 に 2 日間とも参加してきましたので、報告しようと思います。 セッション毎の報告というより、全体を通した感想を話そうかなと思います。 https://cloudnativedays.jp/cndt2019/ リンクをまとめています。 https://qiita.com/zaki-lknr/items/1c26bb713aef9645f5e6","CNCF の利用率","一日目の Keynote で印象的だった内容です。 発表者は、OSDT 実行委員長である長谷川さんです。 来場者アンケート 1354 人から聞いた「クラウドネイティブ技術を活用フェーズについて」の紹介がありました。 既に本番環境に適用している人は、なんと 46% という驚きの結果でした。また、開発環境に至っては、 63% ということでした。 このイベントに参加している時点である程度フィルターはかかっていると思いますが、それでも大きな割合だと感じました。 次の図では、CNCF プロジェクトの 180 日間における Commit 数をグラフ化したものです。 生みの親である Google が 1 位で independent(個人)が 2 番目、日本企業 Fujitsu が 6 位です。熱意が伝わってきますね。 ※ 2019/07/24 時点 ただ、CNCF のメンバーとして日本企業は 17 社 しかないそうで、まだまだこれからといったところでしょうか。 https://landscape.cncf.io/members さらには、Kubernetes から認定された日本企業ではまだないみたいです。残念です。 https://kubernetes.io/partners/#kcsp 今後は、次のようなカンファレンスが海外でもあるみたいです。ぜひ参加してみたいと思います。 https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2019/ https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/","CloudNative とは？","クラウドネイティブ技術は、パブリッククラウド、プライベートクラウド、ハイブリッドクラウドなどの近代的でダイナミックな環境において、スケーラブルなアプリケーションを構築および実行するための能力を組織にもたらします。 このアプローチの代表例に、コンテナ、サービスメッシュ、マイクロサービス、イミューダブルインフラストラクチャ、および宣言型 API があります。","※ ","「スケーラブルなアプリケーションを構築および実行」が重要です。これを実現する手段の１つに Kubernetes があります。 「CloudNative = Kubernetes」ではなく、「CloudNative ∋ Kubernetes」という感じです。 ただ、最近では Kubernetes を違う観点で考える人が増えてきたそうです。 それが、二日目の Keynote で発表された北山さんのスライドにあります。 https://speakerdeck.com/shkitayama/change-the-game-change-the-world Kubernetes は「platform のための platform」と言われるようになりました。 これは、slide.No.9(Kubernetes is a platform)で見て分かる通りで、次のようなことがわかります。","運用管理者","Self-Healing によってスケールが簡単になる","アプリケーション開発者","簡単にデプロイすることができる","これらは、たしかに「プラットフォームから得られる価値」になりますが、 逆に次のような考慮が必要なってきます。","Self-Healing はどこまで信頼性を担保できるか","ユーザー影響を最小限にするためには、どうればよいか","これらのような「プラットフォームを利用するコスト」が「プラットフォームから得られる価値」よりも大きくなってしまいがちになります。 そこで、Operator (CRD)という概念が最近ホットになっています。","なぜ CRD がホットなのか？","CRD という言葉は様々なセッションで取り上げらていました。 CRD と Operator については、下記をご参考下さい。 https://silverbirder.github.io/blog/contents/kubernetes_meetup_tokyo_19_osaka_satellite Kubernetes を運用すると、既存のリソースだけでは物足りない所がでてくるそうです。 そういう部分が「プラットフォームを利用するコスト」を大きくしてしまいます。 そこで、オリジナルのカスタマイズしたリソースを独自に開発し、運用を自動化することを目的とした CRD、Operator が生まれました。 ただ、独自に 1 から作るよりも、下記のサイトから使った方が効率的なときもあります。 https://operatorhub.io/ けど、結局は困ったとき、ソースコードを読むことになるので、それぐらいの能力がないと、 運用を回せない気がします。 zlab の ladicle さんの次のスライドがとてもわかりやすく、まとまっていました。 これは貴重な資料ですね。 https://speakerdeck.com/ladicle/kuberneteswokuo-zhang-siteri-falseoperesiyonwozi-dong-hua-suru ちなみに、独自に 1 から作ったケースがサイバーエージェントの山本さんの発表で、次のスライドです。 https://speakerdeck.com/mayuyamamoto/kuberneteskuo-zhang-woli-yong-sitazi-zuo-autoscalerdeshi-xian-surusutoresuhurinayun-yong-falseshi-jie 同じくサイバーエージェントの青山さんがライブコーディングされていたリポジトリが次のものになります。 https://github.com/cloudnativejp/webserver-operator","Kubernetes は必要ですか？","Kubernetes を使うべきかの話が 2 日間でちらほらありました。 次のような議論もあります。 https://www.atmarkit.co.jp/ait/articles/1907/23/news120.html CloudNative なアプリケーション構築を目指す場合、どうしても Kubernetes を使う方向になりがちですよね。 今回参加したセッションの多くの企業では、Kubernetes を採用するための検討が下記のような感じでした。","プレインな Kubernetes か、マネージドな Kubernetes か","大体はマネージドな Kubernetes を使う。 かゆい所に手を伸ばすときになって、プレインな Kubernetes を使う。","Kubernetes のエンジニアは何人か。それは専属か","どこも Kubernetes の知識を保有するエンジニアは少ない。 数人程度で専任で進めることが多い。","ノウハウを蓄積するために、スモールスタート","様々なセッションがあった中で、とても王道なステップを踏まれている企業がありました。それは、SoftbankPaymentService の鈴木さんの次のスライドです。 https://www.slideshare.net/JunyaSuzuki1/springpcf-cndt2019-osdt2019-keynote 企業に適した CloudNative 化だなと勉強になりました。 特に「運用を回すコストを考慮すると、Kubernetes ではなく PaaS を使う」 というポイントが好きです。","Circuit Breaker","耳にタコができるぐらい、この単語を聞きました。 下記のサイトが参考になります。 https://qiita.com/yasuabe2613/items/3bff44e662c922083264#circuit-breaker","同期リクエストの先で一部のマイクロサービスに障害があると、クライアントやその先の「クライアントのクライアント」までブロッキングが波及することになりかねない。 この問題を、クライアントと実サービスの間に Circuit Breaker と呼ばれるプロキシを介在させて、実サービスの呼び出し失敗が一定基準を超えると、クライアントからのリクエストを即座にリジェクトさせて、ブロッキング連鎖を解消するパターン。","Kubernetes でアプリケーションを構築すると、分散システムの恩恵を受けるために、 アプリケーションをマイクロサービス化する流れになります。そのマイクロサービス化でよく踏む地雷が、 「後ろの API が死んだら、連鎖的に他サーバも死ぬ」という現象です。 これを回避するために、上記の Circuit Breaker パターンを使う企業が多数いらっしゃいました。 本当にいろんなセッションで聞きました...。","twelve factor app","次の Wantedly さんのスライドが、私の中では話題になりました。 https://speakerdeck.com/potsbo/k8s-kubernetes-8-factors 要は、「アプリケーションとしての設計の考え方(twelve factor app)を、インフラ部分でも適用してみた」という感じです。 どれも具体的なところまで説明されており、実際に Kubernetes を構築する際に役に立つものだと思います。","技術にフォーカスした発表","今回のイベントでは、何か 1 つの技術にフォーカスした発表が多くありました。 それぞれ私なりにまとめてみました。ご参考下さい。","Chaos Engineering","https://speakerdeck.com/mahito/cndt-osdt-2019-2g1","Docker","https://www.slideshare.net/AkihiroSuda/cndt-docker","Envoy","https://speakerdeck.com/taiki45/cloudnative-days-tokyo-2019-understanding-envoy","Logging","https://speakerdeck.com/yosshi_/kubernetes-loggingru-men","LinuxKernel","https://speakerdeck.com/tenforward/cndt2019","Prometheus","https://speakerdeck.com/tokibi/prometheus-setup-with-long-term-storage","Sandbox","https://docs.google.com/presentation/d/1O9Q9E1hH6mBA5w8oDENnCYObZvij1-Dr_obvsY3X29k/edit","Scheduler","https://speakerdeck.com/ytaka23/cloudnative-days-tokyo-2019","Spinnaker","https://speakerdeck.com/sansanbuildersbox/introduction-to-deployment-patterns-with-spinnaker:embed]9","Istio","https://speakerdeck.com/dangossk/a-deep-dive-into-service-mesh-and-istio-cndt-2019","その他","サイバーエージェントさんより、エンジニアにとってとても嬉しいアイテムを頂きました。 https://twitter.com/ca_adtechstudio/status/1152080444445167616 さっそく、キーボードにとりつけてみました。最高です！ こちらのサービスから作られたそうで、私も自前で何か作ってみようかなと思いました。","最後に","CloudNative にどっぷり浸かった 2 日間でした。 どの企業でも CloudNative を導入したことによる「つらみ」や「価値」を共有して頂いたおかげで、これから導入する人たち（私を含む）にとっては、有意義な時間でした。 全てのセッションを吸収できたわけではないですが、ここで記載したスライドだけでも理解を深めたいなと思います。 https://cloudnativedays.jp/cndk2019/ 今度は大阪で開催されるそうです。これも絶対参加したいなと思います！","蛇足（参加するまでの経緯）","筆者は Web が大好きなエンジニアで、Kubernetes については理解が浅い人間です。主にフロントエンドに注力しています。 ただ、昨年の DeveloperBoost2018 で、サイバーエージェントの青山さんのセッションをうけて Kubernetes に興味を持ち始めました。 https://codezine.jp/article/detail/11291 青山さんは Kubernetes にとても詳しい方で、世代が近いせいか、私もこれぐらい夢中になれるものを見つけたいと感じるようになりました。 私は Web に関わるものなら何でも好きで、Kubernetes も含まれます。そこで、青山さん著作の","を全て実践することにしてみました。もちろん お家 Kubernetes でです。 実際に触ってみると、スケールする簡単さに驚きました。ほぼコマンド一発で Pod が複製されて、「え！？」とびっくりです。 そこから、段々とハマっていき今回のイベントに参加することになりました。"],"t":"Cloud Native Days Tokyo 2019 -2019年7月22-23日参加レポート"},{"f":"src/routes/blog/contents/cloud_run_3_step_glang/index.mdx","c":["Cloud Runをたった3ステップでデプロイしてみた (golang)","import { Image } from \"~/components/image/image\";","Cloud Run とは？","Cloud Run is a managed compute platform that enables you to run stateless containers that are invocable via HTTP requests. Cloud Run is serverless","※ ","詳しくは割愛するが、Cloud Functions や App Engine と同じようなサーバーレスで動作するもの。 コンテナを deploy するため、GKE から制御することもできる。","デプロイしてみた","を参考に進めていく。 ちなみに、動作環境は下記コンテナ内に行う。","step1. gcloud の各種設定","※ 2019/04/11 時点では、Cloud Run は beta.","step2. アプリケーションコードの作成","step3. 登録&デプロイ","感想","普段私は、個人開発をしているときによくつかっている ","という Serverless Deployments を使っている。こちらは、v1 のときは docker コンテナを使えていたのだが、v2 になると使えなくなってしまった。ただ、無料で簡単にデプロイできるものを選んでいると、こちらのサービスが最善だと感じていた。 しかし、今回の GoogleCloudNext19 の発表で、CloudRun というものを Beta 版でリリースされたことを知り、早速使ってみた。 何事もなく、今回の手順を進めて一切失敗することなく、3 分以内にデプロイまで進めることができた。 これは、なんて楽で便利なんだと感心してしまった。 また、","を見ると、CloudFunctions のようなリクエストによる従量課金制で、月 2 百万リクエストまで無料だ。個人開発においては、AppEngine のようなインスタンス起動時間による料金設定よりも、こちらの方が断然オトク。 これはもう now.sh をやめて、こっちに乗り換えるっきゃない!!"],"t":"Cloud Runをたった3ステップでデプロイしてみた (golang)"},{"f":"src/routes/blog/contents/cloudflare_workers_mfe/index.mdx","c":["Cloudflare Workers (Edge Worker) で Micro Frontends","import { Image } from \"~/components/image/image\"; 今回、また Micro Frontends の構築を試みようと思います。構築パターンの内、サーバーサイド統合パターン、特にエッジサイド統合を試しました。 その内容を紹介します。サンプルコードは、下記に残しています。 https://github.com/silverbirder/micro-frontends-sample-code-5/","Edge Side Include (ESI)って？","https://www.w3.org/TR/esi-lang/ ESI は、SSI と似たようなもので、サーバーサイド側でコンテンツを挿入する仕組みの 1 つです。ESI の場合、挿入するコンテンツ(ページフラグメント)が Edge 側にあると理解しています。 そのため、Edge キャッシュをコンテンツ毎に効かせれるメリットがあります。 現状、ESI 言語仕様は W3C へ提出していますが、承認が降りていない状況です。Akamai などの CDN 企業や、Varnish などのキャッシュプロキシサーバは、ESI を一部実装しています。 個人で試すのに、Akamai は金銭的に厳しいですし、varnish の VCL を記述したくない(好き嫌い)です。 そこで、Edge Worker と呼ばれる仕組みを試そうと思います。 次の引用は Akamai ブログからです。","EdgeWorkers は、世界中に分散配置された Akamai の Edge サーバー上で、カスタムしたプログラムコードを実行できるようになる新しいサービスです","※ ","要は、Edge Workers とは CDN が提供するプラットフォーム上で、プログラムコード、例えば Javascript などが実行できるサービスです。","Edge Workers","個人で使える Edge Workers だと、","や"," があります。後者の Cloudflare Workers には、"," という HTML を書き換える機能があり、Micro Frontends に使えそうだったため、今回は Cloudflare Workers を使用します。","構成","次のような構成を考えてみました。 ※ ","と"," に影響されています。 ※ ","の sketch style で書きました。 それぞれのブロックが Cloud Workers となります。 簡単に、図の左から右の順に説明していきます。 Router で、Web アプリケーションのルーティングを管理します。 ルーティングでは、HTTP リクエストの内容に基づいて、どのページか振り分けます。 振り分けられたページでは、後述するフラグメントを含めて HTML を構築します。 その HTML を HTMLRewiter で処理し、Proxy に存在するフラグメントがあれば、フラグメントの HTML へ置換されます。 フラグメントでは、HTML,CSS,JS を取得する PATH を JSON 形式で返却するようにします。 JSON を返す URL は、/manifest.json と統一しています。 このような構成を取ることで、担当領域を分割することができます。 例えば、フラグメント A とページ X をチーム 1 が管理し、フラグメント B、C、ページ Y をチーム 2 が管理するなどです。 また、Rust の WebAssembly を下記のようなテンプレートで組み込むことができます。 https://github.com/cloudflare/rustwasm-worker-template 特定の重い処理を Rust の WebAssembly で処理するようなフラグメントをページに混ぜることができます。","構築して困ったこと","同一ドメイン内での Edge Workers 通信が不可","Cloudflare Workers は、任意のドメインで動かすことになります。 例えば、ドメイン A 内に複数の Cloudflare Workers X と Y があったとすると、 X から Y への通信ができないです。 https://community.cloudflare.com/t/issue-with-worker-to-worker-https-request/94472/37 そのため、複数の Cloudflare Workers を使用する場合は 複数のドメインが必要になります。 先程の例なら、ドメイン A に属する Cloudflare Workers X からドメイン B に属する Cloudflare Workers Y へ通信することができます。 私は、freenom の tk ドメイン(無料)を複数購入しました。 https://freenom.com/","直接 IP アドレスへリクエストできない","ローカル開発時に困ったことがあります。 Cloudflare Workers をローカル開発する場合、"," というコマンドで検証します。 検証中に、他の Cloudflare Workers の URL(localhost:XXXX)へアクセスしようとしても、直接 IP となるため失敗します。 https://support.cloudflare.com/hc/ja/articles/360029779472-Cloudflare-1XX-%E3%82%A8%E3%83%A9%E3%83%BC%E3%81%AE%E3%83%88%E3%83%A9%E3%83%96%E3%83%AB%E3%82%B7%E3%83%A5%E3%83%BC%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0#error1003 そのため、下記のようなサービスを使って、私は解決させました。 https://ngrok.com/ https://github.com/localtunnel/localtunnel","Cloudflare Workers による制約が大きい","Cloudflare のプラットフォーム上では、下記のランタイム API が使用できます。 https://developers.cloudflare.com/workers/runtime-apis Cloudflare Workers の仕組みを把握していないのですが、この提供されている API 以外は、 確か使えなかったような気がします。","最後に","Edge って、私の印象では、単なる静的コンテンツを置くだけのものと考えていました。 それが、動的なコンテンツ、つまり Edge Workers のような存在を知り、Edge の世界が広がったように感じます。 Web アプリケーションを、よりユーザーに近い Edge へ配置するようにすれば、レスポンス速度改善が期待できます。 Micro Frontends というより、Edge Workers の話が多かったですね。(笑)"],"t":"Cloudflare Workers (Edge Worker) で Micro Frontends"},{"f":"src/routes/blog/contents/covid-19-omicron-knowledge/index.mdx","c":["オミクロン株に感染したので、分かったことを書く","2022 年 1 月 24 日、オミクロン株に感染しました。知らないことが多かったので、分かったことを書こうと思います。 ※ あくまで、私が経験した内容であり、一般的な内容かどうかは、知りません。","結論","オミクロン株は、本当に感染しやすい","(感染した)私","ワクチン 2 回摂取 フルリモートワーク 基礎疾患なし 手洗い・マスクの徹底 会食なし","感想","感染するかどうかは、運がかなり大きい気がする。 コロナウィルス(デルタ株など)は、感染しないための対策が大切と思ってたけど、オミクロンに限っては、感染後の準備も大切だなと思う。","喉(中咽頭)が、","めっっっちゃ痛い","オミクロン株の特徴の 1 つ 感想","唾液を飲み込むと、痛みが走る。","特に夜中は、","激痛になる","。 飲み込み動作を避けるために、唾液を溜めたり、ティッシュで捨てたりする。(ティッシュは袋に入れて、ちゃんと締める)","喉を潤わせることが大切","マスク、のど飴 大事。","症状が、インフルエンザに近い症状","症状","喉の痛み、高熱 その他なし (嗅覚・味覚障害なし)","重症化なし","高熱は、内科で処方して貰ったお薬(葛根湯,カロナール)で充分治る。 カロナール、ありがとう。","体調変化の時系列","| 日付 | 体温 | 症状 | 行動 | | ------------------- | ---- | ---------------- | ---------------------- | | 2022 年 01 月 24 日 | 37.5 | なし | 内科へ受診 | | 2022 年 01 月 25 日 | 39.1 | 喉(中咽頭)の炎症 | 内科で PCR 検査 | | 2022 年 01 月 26 日 | 38.0 | 喉(中咽頭)の炎症 | なし | | 2022 年 01 月 27 日 | 37.2 | 喉(中咽頭)の炎症 | PCR 検査より陽性と判明 | | 2022 年 01 月 28 日 | 36.4 | 喉(中咽頭)の炎症 | なし | 熱は、平熱に戻りましたが、喉がまだ治りません。","終わりに","まだ保健所から電話がかかってきません。日に日に感染者数が増加してて、保健所は相当忙しいんでしょうね...、お疲れさまです。"],"t":"オミクロン株に感染したので、分かったことを書く"},{"f":"src/routes/blog/contents/crawlee-was-useful-for-crawling/index.mdx","c":["クローリングをシュッとやるのに、Crawleeが便利だった","スクレイピングしたいときって、あると思います。 Crawlee という OSS が便利だったので、共有します。","背景","スクレイピングしようと思うと、得意な言語でクローリングプログラムを書いて、html をスクレイピングすると思います。 私は、Node.js が得意なので、fetch + jsdom で書くことが多いです。ブラウザレンダリングが必要な場合、ヘッドレスブラウザを使うこともあります。 毎回これを組み立てるのが、ちょっと面倒だなと思います。そういうときに、Crawle という OSS が便利でした。","Crawle","https://crawlee.dev/ より引用します。","Crawlee is a web scraping and browser automation library. It helps you build reliable crawlers. Fast. Crawlee won't fix broken selectors for you (yet), but it helps you build and maintain your crawlers faster.","Crawlee は壊れたセレクタを直せませんが、クローラーを素早く作ることができます。","Crawle の良いところ","Crawle の良いなとおもった特徴を挙げます。","crawlee のテンプレートがある","crawlee は、"," でコード生成できます。","TypeScript のサポートがあります。 また、Crawler はデフォルトで plain HTTP crawler である Cherrio を採用しています。 必要に応じて、Playwright や Puppeteer を使うことができますし、Crawler の切り替えもインターフェースが揃っているため、簡単にできます。","https://crawlee.dev/docs/quick-start#choose-your-crawler","RequestQueue という仕組み","クローラで、復数の URL にアクセスすることは、よくあると思います。 リクエストは、RequestQueue というキューで管理され、自動的にクローラがアクセスしていきます。 キューはユニークな URL で管理されるため、重複したアクセスはありません。","https://crawlee.dev/api/core/class/RequestQueue","この仕組みは、次のような簡単なコードで実現できます。","さらに、enqueueLinks という機能があります。これは、アクセスしているページの anchor の URL を RequestQueue に追加します。 次のコードが、enqueueLinks の例です。","enqueueLinks には、様々なオプションがあります。","https://crawlee.dev/api/core/function/enqueueLinks","例えば、リンクを globs でフィルタリングしたり、anchor のセレクタを指定できたりします。","データは JSON で保存される","スクレイピングで手に入れたデータは、json で保存できます。","https://crawlee.dev/docs/introduction/saving-data","例えば、リクエストした URL を集めたいときは、次のようなコードです。","保存先は、"," になります。 めちゃくちゃく簡単にデータが保存できます。","終わりに","Crawlee の SaaS として、Apify があります。これで気軽に試してみるのもありかもしれません。","https://apify.com/"],"t":"クローリングをシュッとやるのに、Crawleeが便利だった"},{"f":"src/routes/blog/contents/ernie_vilg_demo/index.mdx","c":["ERNIE-ViLG を Google Colaboratory で動かしてみた","import { Image } from \"~/components/image/image\"; ERNIE-ViLG というのが、\"二次元キャラ\" に強いという記事を目にしました。 実際に使ってみようと、次のページで試したんですが、レスポンスがイマイチでした。","https://huggingface.co/spaces/PaddlePaddle/ERNIE-ViLG","そこで、","を参考にして、ERNIE-ViLG を Google Colaboratory を書こうと思いました。","Google Colaboratory で動かす","実際に作ったものは、次のモノです。","中身については、正直良くわかっていないですが("," 通りに試しただけ)、簡単に紹介しようと思います。","準備","次のコマンドを叩いて、ERNIE-ViLG の準備をします。(GPU 環境でないと動作しません)","ERNIE-ViLG を使う","使うのは、2 つのパターンがあります。","CLI で実行する(hub コマンド) Python で実行する(hub ライブラリ)","CLI の場合は、次のとおりです。","Python の場合は、次のとおりです。","オプションは、次の説明の通りです。","text_prompts","生成したい画像の内容を記述した入力文","style","スタイルで画像を生成することが可能","油画 (油絵) 水彩 (水彩画) 粉笔画 (パステル) 卡通 (漫画, カートゥーン) 儿童画 (子供向け) 蜡笔画 (クレヨン) 探索无限 (無限大を探る)","topk","生成する画像数（最大 6 枚）","output_dir","保存先のディレクトリ (デフォルト:ernievilg_output)","※ ","text_prompts や style は、中国語で書く必要があります。","Google Colaboratory で 画像を簡単に見たい","ERNIE-ViLG を動かすと、出力ファイルが Google Colaboratory のフォルダに入ります。 画像を見るためには、画像をダウンロードして、開くという手間があります。 そこで、フォルダを Google Drive と同期するという機能があります。 これを使えば、保存先を Google Drive にしておけば、Google Drive の UI 上から画像を見ることができます。 めちゃくちゃ便利なので、ぜひ使ってみてください。"],"t":"ERNIE-ViLG を Google Colaboratory で動かしてみた"},{"f":"src/routes/blog/contents/first_release_techbook7/index.mdx","c":["技術書典7 で「はじめてのWeb Components入門」を初出版します！","import { Image } from \"~/components/image/image\"; この度、初めて書籍を出版することになりました！ 「はじめての Web Components 入門」本を技術書典 7 で販売します。","技術書典 7 って？","コミックマーケットのエンジニア向けみたいなものです。 詳しくは、下記のリンクを参照下さい。 https://techbookfest.org/event/tbf07","あなたは誰？","詳しくは、私のポートフォリオを参照下さい。 https://silverbirder.github.io/ Web アプリケーションが大好きなエンジニアです。 今は、EC サイトのフロントエンドエンジニアをしています。","どんな本？","Web Components について 何も知らない方向けの書籍になります。 書籍の内容の大きな流れは、下記のとおりです。どれもサンプルコードを用意しています。","Web Components の基礎 ★ 実際に Web Components を作成 関連ライブラリの紹介","「実際に Web Components を作成 」について紹介","お題として Todo Components を作ることになります。作る流れは順序立てて紹介しています。","最小限のコンポーネントを作成 レンダリングの工夫 コンポーネント間のデータ連携","上記のステップを踏むことで Todo Components が完成します。もちろん、サンプルコードがあるので動作確認できます。 また、カバレッジ 100%テストコードの実装も紹介していますので、TDD も実現できます。Web Components のテストコードは、あまりネット上に紹介されていない印象があったので、折角なので混ぜてみました。 最後に、（実際はしていませんが）Web Components を公開する手順も紹介しています。","コラムネタ","コラムネタとして下記のようなもの（全てではない）を書いています。","cloneNode vs innerHTML html-imports WAI-ARIA slot :host","目次","第一章 フロントエンド開発","フロントエンド開発の苦労 コンポーネントベースの JS フレームワーク JS フレームワーク非依存 まとめ","第二章 Web Components","概要 Web Components 4 つの基本機能 ライフサイクルメソッド まとめ","第三章 Web Components を作ってみよう","作って学ぶ TodoComponents を作ってみる [実践] TodoItem と TodoInput を作る [実践] render を工夫してみよう [実践] MyTodo を作る [実践] 各コンポーネントを連携する その 1 [実践] 各コンポーネントを連携する その 2 [実践] 各コンポーネントを連携する その 3 [実践] テストコードを書く [補足] slot Web Components の公開 まとめ","第四章 Web Components の関連ライブラリ","Polymer LitElement Material Web Components その他の周辺ライブラリ まとめ","なぜ書いたの？","主に下記のとおりです。","Web Components を学習したかったから ポートフォリオとして残したいから エンジニアと交流したいから","特に、1 番が大きかったです。 Web が大好きな私にとっては、Web Components という技術に興味がありました。 けれど、実際に試したことがなかったので、これをきっかけにして使ってみたい！という欲求が大きくありました。","いつ、どこで販売されるの？","2019 年 09 月 22 日で、「池袋サンシャインシティ 展示ホール C/D」で販売します。 弊サークル「silverbirder」の配置は「う 03C」です。","販売形態は？","本と電子本の 2 つにするつもりです。 本は日光企画様より製本して頂いたもののみの販売となります。 電子本は BOOTH で販売する予定です。 本を購入された方は、電子本を 無料 で差し上げます。","終わりに","今回、初物が多すぎました（笑）。 そのせいで、プライベートの時間がほぼなくなりました（笑）。","初技術書典参加 初製本 初 Web Components 勉強 初クリスタ（イラスト用)","技術書典では、エンジニアそれぞれの 今 書きたいものをアウトプットしています。 そうすると、新鮮な情報をキャッチアップしやすくなるため、非常に情報収集するのが効率的だと思います。 技術書典を参加したことがありませんが、執筆してみて思います。 「 どの本も絶対面白い！ 」はずです。 だって、好きなこと書けるんですから（笑）。私も含まれます。 技術書典 7 当日は、是非とも「う 03C」にお越し下さい〜〜！ （他のサークルさんの本買いに行きたい...!!!）","関連リンク","見本誌","技術書典 7","https://techbookfest.org/event/tbf07/circle/5117648689954816","Twitter 宣伝","https://twitter.com/silverbirder/status/1166171153875886080"],"t":"技術書典7 で「はじめてのWeb Components入門」を初出版します！"},{"f":"src/routes/blog/contents/fragments_to_be_composed_in_Micro_Frontends_are_defined_in_Web_Components_and_shared_in_Module_Federation/index.mdx","c":["Micro Frontendsで組成するフラグメントをWeb Componentsで定義してModule Federationで共有する","import { Image } from \"~/components/image/image\"; Micro Frontends(以降、MFE)で組成するフラグメントを Web Components で定義して Module Federation で共有する方法を、ざっくり紹介します。 サンプルコードは、次のリポジトリにあります。","https://github.com/silverbirder/playground/tree/main/node/web-components-is-api-for-micro-frontends","※ MFE については、以下のブログ記事をお読みください。 https://silverbirder.github.io/blog/contents/mfe/","用語","フラグメント","各フロントエンドチームが提供する UI 部品(HTML,CSS,JS,etc) コンポーネントと言い換えても良いです","組成","フラグメントを使って、ページ全体を構築する","MFE で有名な Michael Geers さんの記事より、次のサンプル図があります。 この例は、EC サイトのサンプルです。 チェックアウトチームは React を使っていて、フラグメントは次の 2 つです。","購入ボタン(",") バスケット(",")","組成は、プロダクトチームが担っています。 組成は調整の難しさがあるので、専任のチームがあっても良いかもと思います。","フラグメントを Web Components で定義","フラグメントは、各フロントエンドチームが自由に定義できます。React で書いたり、Vue で書いたりできます。 フラグメントを組成するチームからすると、フラグメントのインターフェースが揃っている方が使いやすいと思います。 そこで、フラグメントを Web Components で定義しましょう。(定義の中身は React や Vue など自由です) このやり方は、以下の MFE を実現する 3 つの設計パターンどれでも適用できると思います。","ビルドタイム組成パターン サーバーサイド組成パターン クライアントサイド組成パターン","次に、サンプルコードを紹介します。","検索ボタンのフラグメント","検索ボタンのフラグメント(Web Components)を書きます。 それは、ボタンとクリックハンドラを定義した簡単なものです。 フレームワークは、React を選択しました。","この Web Components は、"," と書いて使います。 他のフラグメントと連携する場合、カスタムイベントを使います。 この Web Components は、クリックボタンを押したら、"," というカスタムイベントを発火します。","JSON を表示するフラグメント","次に、このイベントのデータ(",")を表示するフラグメント(Web Components)を書きます。 与えられた json 文字列を表示するだけのシンプルなものです。 Web Components へデータを与える手段は 3 つあります。(さらにあるかもです)","HTML 属性 (ex. ","プリミティブな値(数値、文字など)で使う","イベントリスナー (","非プリミティブな値(配列など)で使う","Slot (","HTML 要素を差し込みたいときに使う","今回は、HTML 属性を選択しました。"," のように使います。","は、与えられた json を","で表示しているだけです。","これまで紹介したフラグメントを組成します。 組成するためには、フラグメントを提供する仕組みが必要です。 そこで、Webpack の Module Federation を使います。 ※ Module Federation を採用すると、各フロントエンドチームのビルドシステムを Webpack で縛ってしまうデメリットがあります。 ※ 他の提供する仕組みとして、"," が使えないかなと思ったんですが、未検証です。","Module Federation","Module Federation は、Webpack@5 から導入された機能です。","https://webpack.js.org/concepts/module-federation/","Each build acts as a container and also consumes other builds as containers. This way each build is able to access any other exposed module by loading it from its container.","Module Federation は、各ビルドをコンテナとして機能させ、他のコンテナを使うことができます。 今回で言うと、Web Components の SearchButton と JsonDiv をコンテナ化し、組成のビルドでコンテナを参照します。 具体的なコードを紹介します。","コンテナ化","検索ボタンをコンテナ化してみます。(JSON を表示するフラグメントも同様のコードです)","何をコンテナとして提供するか export します。 次に、webpack の plugins コードを定義します。","先程の export したファイルを exposes で設定します。 ライブラリの重複ロードを防ぐために shared を設定します。 これで、SearchButton をコンテナ化し提供できるようになりました。","では、コンテナをロードする組成側のビルド(webpack)を見てみます。","remotes で、コンテナをロードする URL を設定します。 次に、entry コードです。","を読むと分かりますが、entry コードは、","で動的ロードする必要があります。 次に、bootstrap コードです。","ここにある "," や"," がコンテナを動的ロードしているところです。 ロードするものは、Web Components なので","で定義します。 また、","の","イベントハンドラをリッスンし、イベントデータを","属性に設定する処理を書きます。 これで、組成が完了です。 実際に、動きを見てみたい場合は、リポジトリの README.md を見て試してみてください。","この手法におけるメリット","Web Components をフラグメントとして使うメリット・デメリットは、次のとおりです。","メリット","適合性","Web Components は、Web 標準技術なので、ライブラリとの適合は容易 HTML タグを使うようにカスタム HTML タグを使えば良い","独立性","Shadow DOM というサンドボックス環境で開発可能","デメリット","Javascript が動く必要あり","最後に","Micro Frontends で組成するフラグメントを Web Components で定義して Module Federation で共有する方法を紹介しました。 実運用の経験はないですが、アイデアとして使えるかもしれないと思いました。"],"t":"Micro Frontendsで組成するフラグメントをWeb Componentsで定義してModule Federationで共有する"},{"f":"src/routes/blog/contents/frontend_de_kanpai_7/index.mdx","c":["【増枠】Frontend de KANPAI! #7 - Going on 令和 - 2019年7月19日参加レポート","import { Image } from \"~/components/image/image\"; 今回は DeNA さん主催の Frontend のイベントに参加してきましたので、 報告しようと思います。hashtag はこちら ","firebase の勢いがすごい。あと now も多少人気で、now 信者の私とっては嬉しい 。 ※ ","※ ","https://frokan.connpass.com/event/135584/","イベント概要","「Frontend de KANPAI!」（以下、FROKAN）は、フロントエンドエンジニアやフロントエンドに興味がある人が集い、ドリンク片手にゆるく交流・技術交換ができるコミュニティを目指しています。","特徴的だったのが立食形式という点です。 会場には多少の椅子が用意されているものの、ほとんどの人が立ってドリンクを持ちながら、登壇者のお話を聞いていました。 座っていると私はあんまり他の参加者とコミュニケーションしない人です。 椅子があると、その椅子が自身のテリトリーになってしまって閉じこもってしまう気持ちがあります。 しかし、立っているとその縛りがないので、楽にコミュニケーションを取れた感じがありました。 照明が暗かったというところもあると思います。ちなみに、お酒は一切飲んでないです。（笑） ただ、翌日は少しつらかったです...。 https://twitter.com/silverbirder/status/1152348180643627008 普段は、メモをがっつり書いて twitter に投稿している私ですが、 今回は一切そのようなことをしていません。したがって、ほぼ記憶ベースでイベント内容を報告します。ご了承ください。 誤りありましたら、ご指摘下さい。","印象に残ったお話","React と WebComponents で Vanilla な開発","日本経済新聞社 宮本 将 さん","WebComponents に興味がある私は、この発表は気になっていました。 https://twitter.com/silverbirder/status/1149648900627693572 内容をざっくり説明すると「CustomElements をプロダクトとして使っていたけど、つらみがあったので typescript で縛るようにしたよ」 というものでした。CustomElements は Web 標準の技術ですが、react や vue のような prop による型縛りがありません。 すべて文字列として表現するため、バリデーションチェックがつらいというお話が印象的でした。 そこで、react(JSX) + typescript を駆使して CustomElements を Wrap することで上記問題を解決されたそうです。 実際に導入したことによる「つらみ」というものは、プロダクト導入検討している私にとってはありがたいお話でした。 そもそも、なぜ CustomElements を導入したのかというお話もありました。 日経さんでは WebApplication というより Static な WebSite に近いプロダクトだそうです。 そのため、react や vue といったフレームワークは必要以上な機能が多いため却下し、 Vanilla な JS で CustomElements を進めて行こうという経緯があるそうです。 プロダクトの特徴に応じてコンポーネント選択すべきね。 ※ CustomElements っていろんなフレームワークで対応しているんですね。 https://custom-elements-everywhere.com/","実録フグ料理","株式会社ディー・エヌ・エー Takepepe さん","当初、「お、ネタ枠か？」と思っていたのですが、 予想以上に面白いお話でした。 Project Fugu というものがあります。 https://www.heise.de/developer/artikel/Google-Projekt-Fugu-Die-Macht-des-Kugelfisches-4255636.html","Unter dem Codenamen Fugu plant Google die Einführung zahlreicher Webschnittstellen in seinem Webbrowser Chrome, welche die Lücke zwischen Progressive Web Apps und ihren nativen Gegenstücken schließen wollen.","Google 翻訳を通すと","コードネーム Fugu の下、Google は Chrome ウェブブラウザで数多くのウェブインターフェースを立ち上げることを計画している。これは Progressive ウェブアプリと彼らのネイティブの対応物との間のギャップを埋めるであろう。","つまり Chrome ウェブブラウザからネイティブな部分を操作できることを目的としています。 今回 Takepepe さんが紹介していたのは Shape Detection API です。 バーコードやテキスト、そして顔を形状検出するデモの発表がありました。 どれもサクッと簡単に動作していましたが、まだ実験的な機能なため安定しない部分もあるそうです。 次の画像は顔検出した箇所にモザイクを入れるデモです。これは笑いました。 ※ ","ちなみに、Fugu という名前の由来は、 「ネイティブな部分を操作することは色々な事ができるようになり夢が広がるが、使い所を誤ると危険なもの」という話から フグの「調理の仕方によって美味しい料理になるが、毒があるので調理の仕方を誤ると危険なもの」という自戒の念を込めて Fugu になりました。 ネイティブな部分を操作できるということは、センシティブな情報を取得できてしまうという面があります。 そこが使い方によっては「毒」になるものですね。","新しい API","株式会社ディー・エヌ・エー feb19 さん","https://speakerdeck.com/feb19/xin-sii-api いきなりポエムを語り始めた feb19 さん。 「DeNA の人たちは、みんなこうなのか？」と面白く見ていました。 この発表では、HTML,CSS,JS の API が紹介されていました。 LazyLoad という有名なものから、少しマニアックな inverted-color まで幅広く、よくこんなもの知ってるんだなと驚きました。","最後に","フロントエンドのイベントは、今回初めてかもしれません。 参加者の多くは、どちらかというと「陽の者」が多く、ノリが楽しい人たちばかりでした。 参加者の方と技術的な話をしたのですが、やはり react, vue, augular のワードが多かった印象です。なんだか聞き飽きた印象もありますが、それだけ需要があるのだなと改めて思いました。"],"t":"【増枠】Frontend de KANPAI! #7 - Going on 令和 - 2019年7月19日参加レポート"},{"f":"src/routes/blog/contents/gas_fetchall_redirect/index.mdx","c":["Google Apps Script で FetchAllとRedirctURL の組み合わせは悪い","import { Image } from \"~/components/image/image\"; Google Apps Script (以下、GAS)で、困ったことがあったので備忘録として残しておこうと思います。","やろうとしたこと","特定ハッシュタグにおける、ツイートに書いてあるリンクを集めようとしていました。 そのリンクは、特定のドメインのみでフィルタリングしたいとも思っていました。 これらを RESTful API として提供したかったので、手軽に作れる GAS で作ろうと考えていました。","取り組んでみたこと","Twitter に書くリンクは、全て短縮 URL になります。 そのため、短縮 URL にアクセスし、リダイレクト先の URL を取りに行く必要がありました。 GAS では、リクエストメソッドである fetch があります。その fetch の","というオプションを false にし、responseHeader の location を取ることで、解決(リダイレクト先の URL 取得が)できます。 https://developers.google.com/apps-script/reference/url-fetch/url-fetch-app#advanced-parameters また、1 リクエストだけをする fetch では、直列処理になってしまうため、大変遅いです。 複数リクエストが同時にできる featchAll を使うことで、並列処理ができ、パフォーマンスが良いです。 要するに次のようなコードで解決しようと考えていました。","追記 (20200228)","https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets Twitter の API レスポンスに "," がありました。説明はありませんでしたが、Tweet に貼られたリンク(短縮 URL と、オリジナル URL)の情報が入るそうです。","困ったこと","この手段だと、Location を 1 つ 1 つ辿っていくことになります。 そのため、リダイレクトを自動的に追う( "," )よりも、処理コストが大きいです。まあ、そこは目を瞑ります。 次です。 https://www.monotalk.xyz/blog/google-app-script-%E3%81%AE-urlfetchapp-%E3%81%AE-%E4%BE%8B%E5%A4%96%E3%83%8F%E3%83%B3%E3%83%89%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/ fetch や fetchAll は、"," としたとしても、ExceptionError が発生してしまいます。 そうすると、例えば 1000 件の URL を fetchAll した場合、 どれが成功で、どれが失敗で、どれが未実施か  がわからないというところです。 Promise.allSettled が使えれば、解決できるのかなと思いますが、現状 Promise は使えません。 私が思う解決策としては、","fetchAll ではなく、fetch を使う fetchAll でリクエストする件数をいくつかの塊に分ける。(一気にではなく、分ける）","最後に","そもそもなのですが、今回やろうとしたことって GAS の良さがないですよね。 GAS は、GSuites 連携を簡単にできるという良さがあります。 しかし、今回はちょっとしたクローラーを作りたいだけでした。もちろん、GAS でも作れると思いますが、いくつかを妥協しないといけなくなります。 もし、そこが妥協できないのであれば、別の手段を検討する必要があります。","教訓","表面的","fetchAll するときは、リダイレクト先 URL を取得しない","根本的","目的に適したツールを選択する","ちなみに、このツールは、並列処理をシンプルにコーティングできる golang で書き直そうと考えています。"],"t":"Google Apps Script で FetchAllとRedirctURL の組み合わせは悪い"},{"f":"src/routes/blog/contents/gcpug_kansai_cloud_next_extended/index.mdx","c":["【大阪】GCPUG Kansai 〜 Cloud Next Extended ～ - 2019年5月14日 参加レポート","https://gcpug-osaka.connpass.com/event/128130/ こちらの参加しましたので、ご報告します。hashtag はこちらです。","目的","2019/04/09 ～ 04/11 にサンフランシスコで開催された Google Cloud Next '19 San Francisco で発表された Google Cloud の 新サービスに関する解説や振り返りの内容がメインのイベントとなります！","セッション紹介","GCPUG Kansai 紹介","関西には、こんなにも多く GCPUG コミュニティがあるみたいです。すごい、いっぱい！ Osaka は、継続して参加しようと思います。GCP 大好きですし。","Cloud Next Recap 1","発表者","Ian Lewis(Google)","内容","Google で Kubernetes の担当されているそうです。 また、Pycon や、connpass にも携わっているそうです。","Anthos","読み方は、アンソスと呼ぶそうです。難しい...。 特徴として、下記が挙げられるそうで... ・アプリケーションをモダナイズ ・ポリシーオートメーション ・一貫したエクスペリエンス よーわからないので、gg ってみた。 https://www.publickey1.jp/blog/19/googleanthoskubernetesgoogle_cloud_next_19.html","コンテナ化したアプリケーションをオンプレミスとクラウドのどちらでも実行可能にする、ハイブリッドクラウドおよびマルチクラウドのためのプラットフォーム。","オンプレミスを含むどのクラウド上にアプリケーションがデプロイされていても、Anthos の管理画面から統合管理可能。","なるほど、Anthos はマルチクラウドを実現するためのプラットフォームなのですね。 ふむふむ、わかりやすい。 また、Istio をベースとして Anthos が作られたとも発表されていました。 Istio については、","をご確認下さい。 Istio の機能の特徴として下記があるそうです。 https://twitter.com/nankouyuukichi/status/1128245474215858176?s=20 k8s では、対象とするクラスタを管理します。規模が拡大するにつれ、 サービスが複雑になってくるケースがあります。その際 Istio が、そのあたりを 良い感じに管理してくれると、理解しています。（ざっくり感） ※ マルチクラスタは既に実現できていた(?) Anthos は、その対象範囲をクラウドだけでなく、オンプレ(GKE on Prem)も含めるようにしたと思います。","CloudRun","これは、下記で一度試した経験があります。 https://silverbirder.github.io/blog/contents/cloud_run_3_step_glang コンテナとして deploy できるようになります。 正直、AppEngine, CloudFunction, CoundRun とデプロイサービスが増えてきて、 どれが何に良いのか分からなくなりそうです...。下記に、まとまっていました。 https://docs.google.com/presentation/d/1DCJlrXQKWN63pAz9vtdVNFhMPHceyiKHK0IrFjcwOcU/edit#slide=id.g5693476139_0_155","CloudRun on GKE","こちらは、k8s に CloudRun を deploy できるみたいです。 詳しくは分かりません。","Knative","https://cloud.google.com/knative/?hl=ja","Knative は、オンプレミス、クラウド、サードパーティのデータセンターなど、場所を選ばず実行できるソース中心でコンテナベースの最新アプリケーションを構築する際には不可欠な一連のミドルウェア コンポーネントです。","んー、なんとなくわからなくないですが、他のサイトを見てみます。 https://www.apps-gcp.com/knative-overview/","Knative を使用するためには、Kubernetes がインストールされたクラスタを用意する必要がありますが、Knative は Kubernetes と同様にコンテナをオーケストレーションするためのものである、という点は変わりません。 Knative は、クラウドにおける PaaS や FaaS のようなアーキテクチャを、Knative がインストールされていれば(つまり、Kubernetes クラスタであれば)どこでも実現できるものです。","なるほど。要は、クラウドサービスに依存しないコンテナオーケストレーションなんですね。 GCP を使おうが AWS を使おうが、エンジニアにとって、それは特段大切ではなく、 アプリケーションのプロダクトコードが重要だと思います。そこで、クラウドサービスを 意識せずに、k8s を使うことができちゃうということですね。","gVisor","https://www.publickey1.jp/blog/18/gvisorgoogle.html 従来は、下記のような問題をコンテナは抱えていました。","コンテナ間で OS のカーネルを共有しているためにコンテナ間の分離レベルは高くなく、同一 OS 上で稼働している別のコンテナの負荷の影響を受けやすかったり、コンテナから OS のシステムコールを直接呼び出せることなどによるセキュリティ上の課題を引き起こしやすくもあります。","そこで、gVisor の出番","従来のコンテナの軽量さを保ちつつ、コンテナの分離について新たな実装を提供することよって、準仮想化に近い、より安全な分離を提供するコンテナランタイム","なるほど〜！（ただ、記事を読んだだけｗ）","Cloud Next Recap 2","佐藤 一憲(Google)","機械学習について AutoML を紹介されていました。 私は、そういったものが苦手だったので、よく覚えてないです...","Cloud Next Recap 3","Kirill Tropin(Google)","スピーキングは英語だったので、よく覚えてないです...","Cloud Run ネタ","ちまめ@rito","発表資料","https://speakerdeck.com/chimame/cloud-run-one-step-ahead","2 コマンドで cloudrun できるぐらい、簡単！ ただ、プロダクトとして扱うには、いくつか問題があるそう。","docker full build するみたいで遅い → kaniko で、cache が効くそう。 https://github.com/GoogleContainerTools/kaniko","memoryStore がまだ未対応(VPC)","GCP 大阪リージョンとレイテンシ","salamander さん","大阪リージョンのレイテンシについて紹介されました。 https://docs.google.com/presentation/d/1dbGgjr3Z9o-bOxmT5SQ5bRHMEI0Jzh0BQUQkXlEGyYE/edit?usp=sharing","最後に","Google では、もはや当たり前のように Kubernetes のサービスを進めている印象でした。 クラウド、オンプレを関係なく動かせるプラットフォームである Anthos や、 どのクラウドサービスでも関係なく動かせるコンテナオーケストレーションである Knative など、 どこでも kubernetes を動かせるように進めらています。 これは、kubernetes を使えるようにならないと！ 下記で、勉強中です！ https://silverbirder.github.io/blog/contents/start_the_learning_kubernetes_03"],"t":"【大阪】GCPUG Kansai 〜 Cloud Next Extended ～ - 2019年5月14日 参加レポート"},{"f":"src/routes/blog/contents/gdg_devfest_tokyo_2019_web/index.mdx","c":["GDG DevFest Tokyo 2019に参加したら、Webの未来にワクワクした"," でわざわざ新幹線を使ってまで参加しましたが、それに見合う発見が多くありました。今回、私が学んだ内容について、報告しようかなと思います。 tags: [\"Report\", \"GDG\", \"Dev Fest\", \"Tokyo\", \"Web\"] cover_image: https://res.cloudinary.com/silverbirder/image/upload/v1614429012/silver-birder.github.io/blog/GDG_DevFest_Tokyo_2019_Gate.jpg socialMediaImage: https://res.cloudinary.com/silverbirder/image/upload/v1614429012/silver-birder.github.io/blog/GDG_DevFest_Tokyo_2019_Gate.jpg","import { Image } from \"~/components/image/image\"; GDG DevFest Tokyo 2019 というイベントに参加してきました。 最近はプライベートの都合上、中々時間が取れていませんでした。 しかし今回、会社の都合上、良い感じに時間を確保できたため、こちらのイベントに参加してきました。 "," でわざわざ新幹線を使ってまで参加しましたが、それに見合う発見が多くありました。 今回、私が学んだ内容について、報告しようかなと思います。 https://gdg-tokyo.connpass.com/event/137666/","GDG DevFest Tokyo 2019","DevFest は、Google Developer Group (GDG) コミュニティによって世界各地で開かれるデベロッパー向けイベントです。東京では、Android、Google Cloud Platform（GCP）、Web、Firebase、Machine Learning （ML）、Assistant、Flutter、Go といった様々な技術の最新情報や現場でのノウハウを一日で学べるコミュニティイベントとして開催しています。去年に引き続き 4 回目の開催となります。","https://tokyo.gdgjapan.org/devfest2019 | 名称 | GDG DevFest Tokyo 2019 | | ------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | DevFest Day1 2019 年 12 月 14 日（土） | Sessions、Codelab、After Party 11:00 開始（開場 10:30 予定）18:00 終了 ※終了後、懇親会パーティ開催します。 開催場所：国立大学法人電気通信大学 〒 182-8585 東京都調布市調布ヶ丘 1-5-1 | | DevFest Day1 2019 年 12 月 15 日（日） | Special Hands-on、Office Tour 14:00 〜 17:00 予定 ※14 日にご参加いただいた方の中から抽選で 100 名ご招待 開催場所：Google Japan 〒 150-0002 東京都渋谷区渋谷 3 丁目 21−3 | 私は、DevFest Day1 のみの参加でした。 開催場所は、 電気通信大学 です。スタッフさんの多くは学生さんだったと思います。積極的にサポートされていた姿は、立派だなと勉強になりました。","DevFests","DevFests are community-led developer events hosted by Google Developer Groups around the globe. GDGs are focused on community building and learning about Google’s technologies.","https://devfest.withgoogle.com/ DevFests 自体は、グローバルで活動されている GDG がホストのイベントです。 下の図は、 2019 年 の活動実績&予定です。全国各地で広く活発的に行われていることが分かると思います。 また、コミュニティの Youtube のチャンネルもあります。 https://www.youtube.com/channel/UCXDc-ckqru8BgppXbCt0APw 動画には、文字起こしとして 英語(自動生成)だけでなく、  英語(CC)  もあります。 英語のリスニングが苦手な人でも、文字から理解できるようになっています。こういう配慮はさすがですね。","Google Developer Experts (GDE)","今回登壇されている方の多くが、  Google Developer Experts (GDE)  という言葉を仰っていました。 最初は『Google 社内の何かしらのポジションか？』と思っていましたが、違っていました。","A global program to recognize individuals who are experts and thought leaders in one or more Google technologies. These professionals actively contribute and support the developer and startup ecosystems around the world, helping them build and launch highly innovative apps.","https://developers.google.com/community/experts GDE の人は、端的に言うと『Google のテクノロジを開発者やスタートアップ企業らに対して支援・啓蒙活動をしている Google 外の人』です。 GDE になるためには、Google、Google パートナーからの紹介から入る必要があるそうです。","Sessions","今回のセッションでは、次のようなカテゴリでグループ分けされていました。","Android Assistant Cloud Design Firebase Flutter Go ML Web misc (other かな)","私は、Web が好きな人なので、そのカテゴリを積極的に選んでいきました。","Keynote2: 円周率世界記録への道","Speaker: 岩尾 エマ はるかさん こちらの記事が、今回の話のテーマとなります。 https://it.srad.jp/story/19/03/17/043207/ 岩尾 エマ はるかさんのお話は、おおよそ","に記載があるとおりです。 今回の発表の話にあった、以下の話が印象的でした。","Linkedin 経由で Google からのオファーがあり、『５回面接をして、ようやく合格した』","岩尾さんは、『面接するのは無料』という精神で、何度も Try し続けて合格を勝ち取った人です。同じ大阪出身だそうで、納得しました。（笑） また、元々英語は得意な方ではなかったという話も紹介されていました。 私も英語について悩んでおり、とても共感する部分が多かったです。 "," という当たり前の話があったのですが、 『語学が苦手だけど、Google(海外)で勤務できるようになった』という岩尾さんの経歴を知ると、頑張ってみようと思えました。 とても感謝しています。","Chrome Dev Summit 2019: Recap","Speaker: 矢倉 眞隆さん Chrome Dev Summit(CDS) は、今まで以下のようなテーマが中心でした。","2013","Web API, パフォーマンス","2015","PWA","そして、今回の CDS では今までの  ゴリゴリの JS  話から少し外れたものもいくつかあり、 その紹介をされていました。","HTML と CSS","HTML isn't done.","https://www.youtube.com/watch?v=ZFvPLrKZywA HTML は、まだ成熟されたものではなく、まだまだ改善の余地があるという考えから、 いくつかの改善提案の話が紹介されていました。 丸裸な純粋の HTML でも、わかりやすい UI を標準で表現できるのであれば、ユーザーにとっては ありがたいことですよね。だって、いろんなサイトのいろんな UI を知らなくて済むのですから。 個人的(開発者)には、Edge が Chromium ベースになることが、とても嬉しいです。 リリースが 2020 年 1 月 15 日だそうなので、もう間もなくですね。","Next-generation web styling","https://www.youtube.com/watch?v=-oyeaIirVC0 scroll-snap という機能は、スクロールの制御を CSS で実現しようとしています。 従来は、Javascript でハックな技を駆使していましたが、不要となります。 デザイナーだけでなく、JS を担当するエンジニアも必見です。","JS +SEO","SEO の話がありました。 https://www.suzukikenichi.com/blog/how-to-make-your-content-shine-on-google-search/ Googlebot が最新の Chromium ベースになったことで、Chrome でブラウザを動かすのと同じような振る舞いになるそうです。 今までは、Chrome 41 ベースで Googlebot が動いていたため、新しい JavaScript 構文やブラウザ API を使えなかったそうです。 また、ShadowDOM にも対応しているので、これは WebComponents を推進していることになるのでしょうか。","リアルワールド指向のパフォーマンス","開発環境で Javascript を動かしたとしても、本番環境で動かすと実は遅かったりします。 それは、Javascript による処理が複雑化していることも要因となります。 この現象は、ネットワーク通信が悪い環境（海外）であれば、より明確に実感するはずです。 このようなケースを考慮したテスト環境が必要なのではないかと、私は思います。 以下の記事にある通り、Google は「遅い」と感じるページに警告を出してくれます。 https://jp.techcrunch.com/2019/11/12/2019-11-11-google-chrome-to-identify-and-label-slow-websites/ パフォーマンス計測ツールを活用することで、事前に確認しておきましょう。","web.dev PageSpeed Insights Lighthouse","また、Javascript をシングルスレッドによる低パフォーマンスに対するアプローチ方法を紹介されていました。 https://www.youtube.com/watch?v=7Rrv9qFMWNM 詳しくは、下記をご参考下さい。 https://medium.com/lazy-learning/my-summary-of-the-main-thread-is-overworked-underpaid-chrome-dev-summit-2019-cd65efdf1ce1","ネイティブアプリと Web アプリの差を埋めるには：Project Fugu とマルチスレッドプログラミング","Speaker: 清水 智公さん","ネイティブアプリと Web アプリの差","Web アプリは、ブラウザ上で動作するのものです。ブラウザから提供されている API を通して、Web アプリを構築します。 しかし、ブラウザからマシンのネイティブな部分、例えばマシンにあるファイルにアクセスしたり、ファイルをマシンに 保存したりする操作はできません。この制限は、ブラウザが安全性を担保するためのトレードオフであり、仕方がありません。","Project Fugu","Fugu’s mission is to close the capabilities gap with native to enable developers to build new experiences on the web while preserving everything that is great about the web.","https://www.chromium.org/teams/web-capabilities-fugu Project Fugu とは、ネイティブとのギャップを縮めるために、(Chrome)ブラウザからネイティブな部分を操作する試みのプロジェクトのことです。 ネイティブ部分を操作するため、誤った使い方をするととても危険です。 そのような危険性を、『毒』を持つふぐの名前を借りて Project Fugu というそうです。 提案中の機能一覧は、下記のシートになります。 http://goo.gle/fugu-api-tracker この中には、例えば『Contact Picker API』というものがあります。 名前の通り、ネイティブアプリに登録されている電話帳にアクセスできるようになります。 これにより、例えば『シェアしたいユーザーの情報を電話帳より取得する』ことが実現できます。","Worker","ブラウザは、Task(Event→Scripting→Rendering→Painting)という単位で動作します。 これは、1 つのメインスレッドのみで動作します。 ブラウザにおける fps の目標値は 60fps だそうです。","60fps のフレームレートがなめらかなパフォーマンスの目標値であり、あるイベントに対して必要なすべての更新に与えられた時間は 16.7 ミリ秒です。","https://developer.mozilla.org/ja/docs/Tools/Performance/Frame_rate 1Task を実行するのに 16.7 ミリ秒を超えてしまうと、ガタガタした動作になってしまいます。 そこで、Web Worker という技術で解決しようと考えました。","Web Worker は、ウェブコンテンツがスクリプトをバックグラウンドのスレッドで実行するためのシンプルな手段です。","https://developer.mozilla.org/ja/docs/Web/API/Web_Workers_API/Using_web_workers しかし、スレッド間のメッセージパッシングが複雑化してしまう問題があるそうです。 その問題を、さらに解決するため、GoogleChromeLabs は、comlink なるライブラリを開発しました。","Comlink makes WebWorkers enjoyable. Comlink is a tiny library (1.1kB), that removes the mental barrier of thinking about postMessage and hides the fact that you are working with workers.","https://github.com/GoogleChromeLabs/comlink Web Worker におけるメッセージパッシングの複雑さが comlink によって減少するそうです。 ※ atomics.wait() atomics.notify()の話もありました。","How to Distribute Your Web App? 「インストール」可能なウェブアプリ","Speaker: 宍戸 俊哉さん タイトル通り、Web アプリをどのようにして配布するのかというテーマで、 様々な手段の紹介をされていました。","Web 好きならご存知の Progressive Web App (PWA)のお話です。 Web アプリに対して、よくある PWA 機能は次のとおりです。","オフラインで閲覧 Push 通知 フルスクリーン","PWA は、Web アプリでありながら、ネイティブアプリにとても近い存在に位置しています。 例えば、サンタトラッカーという Web アプリがとても良い例です。 https://santatracker.google.com/ ソースコードも公開されており、PWA として良い参考例になります。 https://github.com/google/santa-tracker-web この Web アプリを『ホーム画面に追加』し、その追加されたアプリを起動してみて下さい。 ネイティブアプリと似たユーザー体験ができるはずです。","ブラウザのブックマークで良いのでは？","わざわざ『ホーム画面に追加』しなくても、ブックマークを使えばよいのでは？ という議論がありました。 話の主となっていたのは、モバイルファーストというキーワードでした。 現在、モバイルからのブックマーク利用率はとても小さいみたいですが、 『え？普通に利用しているけど』と反感してしまいました。私が遅れているのでしょうか。 今の時代、『ホーム画面に追加』が多いのでしょうか。少し疑問です。","インストールを促す手段","PWA をインストールしてもらう場合、Mini-infobar というもので誘導します。 ただ、この Mini-infobar はヘルパーとして使われるため、別途 UI を用意する必要があるそうです。","The mini-infobar is only meant as a helper, and it will go away in the future.","https://developers.google.com/web/fundamentals/app-install-banners/promoting-install-mobile ただ、既にネイティブアプリがある場合、PWA のインストール要求したくありません。 そんなときは、『既にネイティブアプリをインストールしているかどうか』判断する仕組みが既にあるそうです。(origin trials) https://web.dev/get-installed-related-apps/","Desktop PWA","モバイルだけでなく、Desktop にも PWA を適用できます。 先程紹介したサンタトラッカーは、Desktop PWA にも対応しています。 https://santatracker.google.com/ アドレスバーにある+ボタンよりインストールできます。","Trusted Web Activities (TWA)","Trusted Web Activity は、Android アプリ内で Chrome ブラウザを全画面で実行します。","https://developers-jp.googleblog.com/2019/03/trusted-web-activity.html Android アプリでも、PWA が実現できるみたいです。","TWA は、Android アプリの全画面ウェブ コンテンツで WebView では利用できない Chrome 機能を使いたい場合や、Chrome ブラウザとオリジン ストレージを共有することでユーザーのナビゲーションが便利になる場合などに適しています。","私は Android ユーザーはないので、GooglePlay からアプリをダウンロードできません。","Rakuten Pasha OYO Rooms","のようなものが TWA 対応しているそうです。 会社内で使うアプリ(勤怠, 経費, etc)を、TWA として配信することも１つの使い道と紹介されていました。 TWA の開発には、次のライブラリが便利だそうです。 https://github.com/GoogleChromeLabs/llama-pack","Web Packaging","PWA や Destop PWA, TWA といったもので、様々なところから Web アプリを 配布する手段が増えました。では、オフラインの場合はどうでしょうか。 その場合は、Web Packaging の Web Bundles が使えます。 https://github.com/WICG/webpackage Web Packaging には、大きく２つのものが含まれています。","Signed HTTP exchanges Web Bundles","前者は、AMP ページの URL を google ホストから元のホストへ戻す際に有効だそうです。 https://amp.dev/documentation/guides-and-tutorials/optimize-and-measure/signed-exchange/ 後者は、Web のアプリケーションを人まとまりにし、オフライン上で提供することができるそうです。 https://www.youtube.com/watch?v=xAujz66la3Y ※ chrome canary フラグを有効化する必要あり","Yearly Web 2019","Speaker: Jxck さん 2019 に起きた Web に関する Topic をざっくりと紹介されていました。 ","に、詳細が載っていますので、こちらでは項目だけリストアップします。 | Topics | 補足 | | ------------------------------- | ------------------------------------------------- | | Dark Mode, High Contrast Mode | | | portal tag | 画面遷移を CSS でアレンジするアレ。まだバグが多い | | WebAssembly | | | WebAuthN | Authenticator を使った認証 API | | ES2019 | nullish coalescing/ optional chaining | | Intelligent Tracking Prevention | 合意のないトラッキングはダメ!ゼッタイ! | | Project Fugu | | | DNS over HTTPS/TLS | DNS クエリも暗号化 | | Edge Chromium | | | WebPackaging | | | WebTransport WebCodecs | ゲームで役立つ? | | WebComponents v0 → v1 | | | Same Site Cookie Lax by default | Cookie を同じサイトでしか送られなくする | | TLS 1.0/1.1 → 1.2 | |","Perspective of Angular in 2020","Speaker:稲富 駿さん セッション内容は、Angular における 2019 年のアップデート内容と、2020 年とその先の未来についてです。 以下を要チェック！","ちなみに、私は Augular の v2 で止まっています。（笑）","Updates in 2019","Angular の価値(values)は、","Apps That users to use Apps That developers to use Community where everyone feels welcome","という 3 点あるそうです。私にとっては、v2 へのアップデートで大変つらい記憶がありますが...。 2019 年,2020 年における Angular のバージョンは、","2019-05","v7.x v8.x","2020-Q1","v9.x","2020-Q3","v10.x","こうみると、メジャーアップデートの更新がとても早いですね。 追いつくのが大変そうです。","v7.x","Size Budgets by default","バンドル時のサイズを制限する機能 パフォーマンス向上を期待","CDK Drag&Drop","手軽に Drag&Drop を実行可能","Virtual Scroll","画面に見えているものだけ DOM 構築される パフォーマンス向上を期待","Bazel","gulp のようなビルドシステム Opt-in support","v8.x","Differential Loading by Default","ブラウザによって、polyfill の量をコントロール レガシーの部分はそのまま。モダンブラウザの polyfill の量が削減 パフォーマンス向上を期待","Dynamic Import for Lazy Loading Support Module Web Worker ng deploy","production build していない deploy が多くなかった。 かならず --prod となる。","主にパフォーマンス向上に取り組んだ 1 年だったそうです。 Angular は All-in-One なフレームワークなため、どうしてもアプリケーションのコード量が 他フレームワークに比べると多いと思います。そうすると、アプリケーションを読み込む際に、 必要以上にロードされ、パフォーマンスが問題視されていたのでしょうか。","Roadmap 2020","2020 年の Angular はどんなものになるのか、紹介されていました。","Ivy by Default","Ivy は、angular の次世代コンパイラだそうです","CDK Clipboard API CDK Testing Harness","コンポーネントのテストをより抽象化 await 処理が簡単になった","@angular/components Strict Type -checking in templates","2019 年はパフォーマンスというユーザーのための取り組みで、 2020 年は開発者向けの取り組みが多いという印象でした。","Imagine the Future","今までの Angular は、エンタープライズ向けのアプリや、 小規模のアプリに使われていました。 次は、Billion のユーザ向けアプリをターゲットにするそうです。 そのためには、そのアプリに寄り添った機能提供ができるようにと考えているみたいです。 例えば、SEO やアクセシビリティ、国際化といった観点です。","終わりに","Web の進化は早いなと実感した濃い一日でした。 吸収しすぎて、消化が追いつかないですね、ワクワクが止まらないです。 GDE の方々は、どの方も専門領域がとても詳しく、かつ、説明の仕方が上手という印象を持ちました。 私も GDE になってみたいので、得意分野を見つけるところから始めようと思います。 また来年 GDG イベントありましたら、参加したいなと思います！"],"t":"GDG DevFest Tokyo 2019に参加したら、Webの未来にワクワクした"},{"f":"src/routes/blog/contents/go_conference_2019_spring/index.mdx","c":["Go Conference 2019 Spring - 2019年5月18日 参加レポート","https://gocon.connpass.com/event/124530/ https://gocon.jp/ こちらに参加してきましたので、ご報告します！","よかったセッション","H1 (S): Hacking Go Compiler Internals 2","概要","Since the previous talk at Go Con 2014 Autumn, lots of things in the internals have changed. In this talk, I will try to give an overview of Go compiler internals and update the information as much as possible, along with my new hacks.","資料","今回","https://speakerdeck.com/moriyoshi/hacking-go-compiler-internals-2nd-season","前回","https://www.slideshare.net/moriyoshi/hacking-go-compiler-internals-gocon-2014-autumn","感想","このセッションでは、Golang のソースコードが機械語になるまでのステップ、要はコンパイラの動きを紹介されていました。 大きく分類して 11 ステップあり、ざっくり要約すると下記のステップです。 １．コードをトークンに分割 ２．構文木に構築 ３．型チェック ４．インライン化 ５．中間言語(SSA)の生成 ６．機械語の生成 正直、高級言語ばかり使っていたので、機械語に近い低級言語の知識が乏しい私ですが、 今回のお話は、そういった初心者でも分かりやすく説明されていました。 この話を聞いて、純粋に疑問に持ったこととして「どこに最も時間がかかるのか」でした。 ちょうど、同じ疑問を持った方が質問されていて、回答として「型チェック」だそうです。 実際に調べるためのツールが、golang の benchmark があるみたいなので、こちらを使って チューニングをすることができます。簡単なコードを書いて、試してみたいなと思いました。","A4 (S): Design considerations for container-based Go applications","Go 言語でのアプリケーション開発で、特にコンテナを前提とする場合の設計考慮点について話します。 例えば、Go 言語で API を開発する場合、コンテナとして動かすことを前提とするケースが多いと感じます。コンテナベースで動かすことを前提とした場合、コンテナイメージ作成・アプリケーション監視において、考慮すべき点が出てくるでしょう。このトークでは、Go 言語での実装にまで踏み込んだ上で、コンテナベースアプリケーションにおける設計の考慮点について話します。","https://speakerdeck.com/hgsgtk/design-considerations-for-container-based-go-application https://www.redhat.com/ja/resources/cloud-native-container-design-whitepaper","Golang の話というより、コンテナで開発する上での Tips の話でした。 Tips は３つ紹介されていて、「Configuration」「Logging」「Monitering」です。 どれも、資料にあるベストプラクティスに沿った方法で、良い手法だなと勉強になりました。 ・Configuration 設定情報をソースコードで管理するのではなく、環境変数を使うこと → 本番/検証等でもソースが変わらない ・Logging ファイルに出力するのではなく、ストリーミングし外部サービスに流す → コンテナを使い捨てしやすくなる ・Monitering ヘルスチェックのエンドポイントを提供する → 外部サービスと連携しやすくなる 他にもベストプラクティが資料に載ってあるので、時間があるときに読んでみたいなと思います。","B8 (L): CPU, Memory and Go","基本的な CPU やメモリを簡単に触れ、Go の最適化、コンパイラの最適化、Go で実装したときの CPU やメモリの振る舞いを紹介します。 またこれら最適化の様子やパフォーマンスを実際に Go の標準ツールを使いながら確認していきます。","https://speakerdeck.com/sonatard/cpu-memory-and-go","Golang におけるパフォーマンス・チューニングについて勉強になりました。 Golang だけの話なのかわかりませんが、コーディングする際に気にしたほうが良いと思います。 ・動的配列を使うのではなくて、静的配列を使う → 動的配列だとメモリ確保のコストが高くなるので、遅くなってしまう ・環境変数を使うのではなくて、定数を使うこと → 実行時にならないと処理が決まらず、コンパイラの最適化がされない ・メモリに割り当てる際は 8byte ずつで割り切れるようにすること(64bit の場合) → 隙間があった場合、パディングが発生して遅くなる(メモリアライメント) そもそも低レイヤーについて全くわからない人なので、 CPU と Memory について知れてよかったです。","全体的な感想","Golang のセミナーに初めて参加しました。 スポンサーの話を聞いていると、Golang を採用した理由は、 どの企業も「パフォーマンスの良さと、学習コストの低さ」 という理由が多かった印象があります。 また、Docker や Kubernetes が Golang で作られていたりと、 Golang はドンドンと人気になっていく言語なのかなと期待しています。 また、スポンサーの中で「既存システムを Golang に再構築した」や 「Golang の知識を得るために勉強会を開催した」など、各社 Golang へ 積極的に活動を試みていることをお聞きしました。","反省","Google Team である Katie Hockman の speaking が英語だったために、ほとんど聞き取ることができませんでした。 実にもったいないと感じました。 ※ 資料まとめ https://engineer-fumi.hatenablog.com/entry/2019/05/18/172000"],"t":"Go Conference 2019 Spring - 2019年5月18日 参加レポート"},{"f":"src/routes/blog/contents/good_and_bad_in_offline_and_online/index.mdx","c":["リモートワークになってから『気軽にすぐ聞く』ことが難しくなった","リモートワークが普及しつつある今、オンラインでの仕事に慣れているエンジニアも多いのではないでしょうか。 私も、そのエンジニアの一人であり、約 1 年はリモートワークしています。 そんな中、久々に会社へ出社すると、","気軽に話しかける楽さ"," を実感しました。この体験について、深堀りしたいと思います。","リモートワーク前","元々、コロナが流行り出す前(2020 年 2 月以前)は、会社に出社して仕事をしていました。 当時、同じグループやチームなどと一緒に仕事をしていまして、なにか気になることがあれば、 近くの席で座っている人に話しかけることをしていましたし、事務関係で他部門の方へ質問するときも、 ちょこっとオフィスを歩き回る程度(数メートルの距離)の近くにいるので、気軽に話しかけていました。","リモートワークが普及し始める","2020 年 3 月ぐらいからコロナ騒動となり、いつの間にかリモートワークする回数が増えていきました。 そして、緊急事態宣言があったりとで会社へ出社する回数が極端に減りました。 出社するとしても、出社する人の人数制限されている状況です。","久々のオフィス出勤と、気づき","そして現在(2021 年 3 月)、関西に住むエリアでは緊急事態宣言が解除されました。 その影響で、会社へ出社することになり、密にならない程度に、オフィスで仕事する機会も増えました。 そんな中、オフィスで仕事をしていると、","誰かとすぐ気軽に口頭で相談すること(話しかけること)が楽"," と感じるようになりました。 なんでそう思うんだろうって、少し考えるようになりました。","オンラインでも気軽に相談できるんじゃ？","オンラインでも気軽に相談できると思いますが、オフラインほど気軽じゃないと思います。 オンラインだと、例えば、次のような手順を踏むことがあると思います。","相談したい相手のカレンダーで予定が空いているか確認する チャットツールで相手に相談したい旨を連絡する ビデオ会議の仮想部屋を用意する 相談相手との予定を合わせる 相談する","どれもちゃちゃっとすれば、数分も掛からないと思いますし、いくつか上の手順を省略しても良いと思います。 しかし、オフラインで人の話しかける場合は、例えば、次の手順ぐらいかなと思います。","相談したい人をオフィス内で探す 相談する","何が言いたいかというと、オンラインに比べて、オフラインで相談する方が ","調整毎が少ない"," んです。","口頭じゃなくて、テキストでも相談できるでしょ？","わざわざ口頭じゃなくて、テキストで伝えれば良いじゃん？って思ったりもするのですが、それも気軽じゃないです。 確かに、チャットツールでちゃんと 話したい内容や背景、目的をテキスト化に落とし込むのは、ログが残ります。 そうすると、後々言った言わないみたいな話にならずに済むので、良いところもあります。 が、テキスト化することは、気軽さから外れます。 テキスト化するのが難しい場面(e.g. 分からないことが分からない場面など)だってあるはずで、 それを無理にテキスト化するのは、やっぱり大変です。 テキスト化するのは、やれば良いかもしれませんが、","気軽さの障壁が高くなります"," 。","すぐにって重要？","気軽さを求めているのは、すぐ聞きたいときが多いんです。すぐじゃないなら、 自分で社内資料を漁ったり、コードを読んだりで、自分なりに調査をします。 が、そういう手間が、人に教えてもらえるだけで不要になるなら、人に聞いて知った方が効率的と思います。 すぐに聞きたいときって、オンラインだと、カレンダーの予定からしか相手の状況が見えないので、チャットツールでメンションすることってありますよね。 でも、そういう行為は、相手の仕事を妨害しちゃいます。オフラインだと、相手の仕事姿を傍から見て、あ、今なら話しかけても大丈夫そうだなみたいな状況が分かります。 ※ 『質問される側は、作業中断しちゃうんだけど！』はい、そ、そうですね。すみません。","気軽さを追求するには？","例えば、カレンダー・チャットツール・ビデオ会議ツールが全て連動して、 1 クリックだけで、相手に相談できるようになる Web アプリを社内で開発・導入したとして、 これがあれば、オンラインでも気軽に話せる場面が増えるかもしれません。 けど、面倒だと思うのは、相談したいときにわざわざその Web アプリを起動する面倒さがちょこっとありそうです。 何が言いたいかというと、常時、気軽に話しかけれる環境下で仕事をしたいなと思っています。 所謂 ","バーチャルオフィスツール"," (Remotty,Sococo,etc)を常時使っていれば、気軽に話せる状態になれるのかなと思ったりしました。 そのツールだと、さっきの『すぐにって重要？』で話したようなカレンダーからの状況だけだと相手の状況が適切に見えない問題を解決してくれたりします。","バーチャルオフィスツール使えば、全て解決？","そのバーチャルオフィスツールを社内で導入したとしても、","利用者が少ない","→ 相談したい人がいない","→ 利用しなくなる","利用ルールが整備されていない","→ 離席しているかどうかのルールが徹底されていない","→ 信頼できなくなる","利用する時間帯が限らている","→ 相談したいときに、相手がバーチャルオフィスから退出している","などの課題があるかもしれません。バーチャルオフィスツールを導入したものの、イマイチ良い効果が生まれなかったかもしれません。 私は、その経験がありますが、今になってやっぱりバーチャルオフィスツールって必要かもと思ったりします。 それは、 ","リモートワークを経験した今だからこそ"," バーチャルオフィスツールの便利さが分かるのかもしれません。 が、ツールを導入したところで、そのツールを活かす文化(ex.上で示したような課題を解決する)が整っていないと、結局、気軽に話し始めるのは難しいんじゃないですかね。","結局、何が言いたかったのか","気軽に話すというのは、オフラインだと意識したことがなかったのですが、 オンラインでの仕事を経験することで、気軽さについて考えるようになりました。 気軽さというのは、","本当に繊細で、細かいところの配慮"," がないと、いけないなと思います。 そもそも、チャットツールで気軽に相談できるないのか！っと言われると、あ、すみませんってなるのですが、 人それぞれ個性があって、それを強制するのは、強制された側のストレスが貯まります。 人によって『気軽さ』ってそれぞれだと思うので、各企業は、そのことをしっかり分析してみてはいかがでしょうか。 その上で、バーチャルオフィスツールを試してみてはどうでしょうか。"],"t":"リモートワークになってから『気軽にすぐ聞く』ことが難しくなった"},{"f":"src/routes/blog/contents/graphql_guild_ecosystem_is_useful/index.mdx","c":["GraphQL Guildのエコシステムって便利だね","GraphQL Guild ってご存知ですか？ GraphQL 界隈だと、Code Generator が有名と思いますが、GraphQL Guild は、それら GraphQL 関連の OSS を開発しているグループです。(",") GraphQL Guild のエコシステムって便利だな〜って感じたことがあったので、紹介します。 試したソースコードは、こちらにあります。","https://github.com/silverbirder/playground/tree/main/node/supabase-graphql-guild-app","GraphQL Schema をダウンロードできる","スキーマ駆動開発をすると、GraphQL のクライアントとサーバーのリポジトリ(GraphQL Schema が置いてある)が分かれることがあります。そうすると、クライアントのリポジトリに、サーバーのリポジトリにある GraphQL の Schema が欲しくなると思います。その状況では、次の解決手段が想像できると思います。","git submodule で、サーバーリポジトリをクライアントリポジトリに入れる git clone で、GraphQL Schema ファイルをダウンロードするスクリプトを書く","git を扱うと、CI/CD のプロセスやいくつかの場面で、面倒なことがあります。 そこで、GraphQL の SchemaURL を指定するだけで、Schema をダウンロードする機能が、"," にあります。","https://www.graphql-cli.com/introduction","具体的には、"," を作成し、"," と実行します。","https://the-guild.dev/graphql/config/docs","Schema をダウンロードするために、GraphQL のエンドポイント URL を指定します。 Config ファイルの形式は、yml、json、js、ts のどれかを選べます。 私の場合、(supabase を使っている関係で)GraphQL のエンドポイントへアクセスする認証情報を環境変数から読み込みたかったため、Typescript(ts)を選びました。 具体的には、次の Config ファイルを生成します。","必要なパッケージをインストールした状態で、"," と実行すると "," が生成されます！","GraphiQL が Config だけで動く","GraphQL を利用する側としては、どのようなクエリが書けるか試せす場所が欲しくなります。 サーバー側から GraphiQL を用意頂くでも全然良いのですが、"," というものを使えば簡単にできます。","https://the-guild.dev/graphql/yoga-server"," したあとに、先程の "," が存在すれば、"," するだけで GraphiQL が手に入ります！一切、サーブするコードを書いていません。最高でした。","GraphQL CLI には色々便利な機能がある"," には、GraphQL 関連で便利な機能があります。","https://github.com/Urigo/graphql-cli","具体的には、次の 3 つです。","Document のオペレーションを元に、Schema がどれくらい使われているかわかる","ローカルとリモートの GraphQL Schema の違いを教えてくれる","Document のオペレーション が、GraphQL Schema 定義に反していないかチェックしてくれる","終わりに","GraphQL Guild は、GraphQL Config が中心になっている印象を受けました。 Config があれば、他のエコシステムはそれを見て機能が動くため、準備するものが少なくて済みます。 関係ないですが、supabase で GraphQL を使うのもすごく簡単で、ありがたいです。"],"t":"GraphQL Guildのエコシステムって便利だね"},{"f":"src/routes/blog/contents/how_to_display_embedded_twitter_content_after_rendering_on_iframe/index.mdx","c":["iframeでTwitterの埋め込みコンテンツの描画後に、画面表示する方法"," から "," オブジェクトを見つけて、"," の第二引数に、表示処理を書くことです。 背景 "," のような URL から、埋め込みコンテンツをブログサイトなどに表示したいです。 "," のレスポンスの中の html が、埋め込みコンテンツになります。これを iframe の srcdoc に設定することで、埋め込みコンテンツを表示することができます。 tags: [\"Twitter\", \"How to use\"] cover_image: https://res.cloudinary.com/silverbirder/image/upload/v1653222658/silver-birder.github.io/blog/jeremy-bezanger-Jm1YUfYjpHI-unsplash.jpg 結論"," の第二引数に、表示処理を書くことです。","背景"," のような URL から、埋め込みコンテンツをブログサイトなどに表示したいです。"," のレスポンスの中の html が、埋め込みコンテンツになります。 これを iframe の srcdoc に設定することで、埋め込みコンテンツを表示することができます。","課題","iframe で srcdoc を読み込んだ後、埋め込みたい Tweet の文字列だけが、チラっと見えてしまいます。 下の例であれば、"," がチラっと見えるはずです。reload をしてみると分かります。 チラっと見えてしまうのを阻止したいです。","解決","埋め込みコンテンツの描画後イベント "," というものがあります。これを使います。 https://developer.twitter.com/en/docs/twitter-for-websites/javascript-api/guides/javascript-api 実装の順番は、次のとおりです。","iframe から、"," イベントを検知"," から、"," オブジェクトを見つける"," で、描画後の処理を書く","実際に、コードを書くと、次のとおりです。","html は、","で隠しておきます。(手段は問いません)","解決した結果が、こちらです。"," のチラっとが見えなくなっているはずです。"],"t":"iframeでTwitterの埋め込みコンテンツの描画後に、画面表示する方法"},{"f":"src/routes/blog/contents/increased_TOEIC_score_by_285_points_in_6_months/index.mdx","c":["6ヶ月でTOEICのスコアを285点アップ","import { Image } from \"~/components/image/image\"; 英語の勉強を始め、TOEIC のスコアが 6 ヶ月で 285 点もアップしました。この経験を共有し、自分の学習の記録してこの記事を書きます。","TOEIC の点数と学習時間","まずはじめに、TOEIC の点数について、時系列順に列挙します。 | 受験日 | 点数 | リスニング/リーディング | | ------------------ | ---- | ----------------------- | | 2023 年 1 月時点 | 355 | 215/140 | | 2023 年 2 月 26 日 | 510 | 280/230 | | 2023 年 5 月 27 日 | 640 | 320/320 | 見て分かる通り、2 月と 5 月のそれぞれの受験にて、どちらも 100 点はアップしています。元々、点数が悪ったというのもありますが。。。 次に私の学習時間について、以下の図で示します。これらのデータは、私が学習の管理に使用しているサービス、","から取得しました。学習期間は、2023 年 1 月から 2023 年 6 月までです。 図からわかるように、私は毎月 30 時間以上を学習に費やしています。日で分割すると、一日 1 時間以上は勉強していたことになります。 6 月は特別な事情があり、学習時間を確保することができませんでした。","英語学習法","私の TOEIC スコアが 300 点台だった頃、私は英単語をひたすら覚えていました。特に意識していたのは以下の 3 つです。","英単語の品詞を覚える 覚えるときは必ず耳と口を使う 少しずつではなく、大量に見て繰り返し覚える","1 つ目は、英単語の訳を覚えるだけでなく、その単語が動詞（他動詞/自動詞）、名詞、形容詞など、どの品詞に属するのかも覚えるようにしました。これは英文法を理解する上で重要なため、常に意識しています。 2 つ目は、英単語を覚えるときに、英単語の発音を耳で聞き、自分の口で発音し、それを自分の耳で聞くという方法を取りました。発音を覚えることで、リスニングするときの聞き取りが容易になります。 3 つ目は、1 日に 1 つずつ丁寧に覚えるよりも、1 日に 100 個の英単語を覚えるようにしました。初日は完全に覚えられないかもしれませんが、同じことを 1 週間繰り返すと、目と耳に記憶が残り、効率的に覚えられるようになります。 私が最初に使用した英単語の教材は「ユメタン 0」でした。レベルとしては非常に簡単なものですが、意外と覚えていなかった単語もあったため、この教材を使って良かったと感じています。","次に、キクタンを試してみました。これは非常に有用でした。特定の量の英単語を音声で聞き、発音するトレーニングが可能でした。","英語学習の継続について","英語学習における最大の課題は、英語学習の継続でした。私は毎日、最低でも 1 時間は英語の勉強をすると決めていました。しかし、仕事が忙しくなると、どうしても英語の勉強を疎かにしてしまうことがありました。 さらに、英語の勉強は淡々と進めるもので、時として非常につまらないと感じることもありました。成果がすぐには見えないため、努力が報われないように感じることもしばしばでした。 英語の勉強を継続しようと思っても、モチベーションが低下し、学習をやめてしまいます。好きなことなら、時間をいくらでも使えるのですが、英語の勉強となると、そこまで好きとは言えない自分がいます。 学習継続が難しいときには、以下の 3 つの方法が有効です。","できなかった時間よりもできた時間を考える","：数分でも継続が大事です。納得感を大事にしましょう。","共同体を作る","：誰かと褒め合ったり、愚痴り合ったりすることで、学習のモチベーションを保つことができます。","定期的に TOEIC や単語テストなどで結果の数値で可視化する","：自分の進歩を具体的に見ることで、学習意欲を維持することができます。","英語学習の喜びとその影響","英語の勉強を継続していく中で感じた喜びは、TOEIC の点数が上がったことだけではありませんでした。日常生活で英語に触れる機会が増えるたびに、聞き取れるようになったり、読み取れるようになったり、新たな意味を理解できるようになったりと、自分の成長を実感する瞬間が増えました。 業務での経験も大きな喜びがありました。日本語が母国語ではない外国人の方と 1 対 1 で急な会議を行い、その際、私は相手に迷惑をかてしまいましたが、それでも何とか会話を進め、目的とする話を終えることができました。その時、自分の言葉が相手に通じる喜びを強く感じました。 これらの小さな喜びを体験する度に、私のモチベーションは高まりました。これらは、英語学習を続ける上で非常に重要な経験だと感じています。","次にすること","私の目標は TOEIC で 700 点を獲得することです。そのためには、リスニングとリーディングのスキルを磨くために、これらの学習を繰り返し行います。 700 点を超えたら、次のステップとしてスピーキングの練習に移ります。そのためには、瞬間英作文トレーニングや、適切なサービスを探すことを計画しています。"],"t":"6ヶ月でTOEICのスコアを285点アップ"},{"f":"src/routes/blog/contents/intelliJ_typescript_docker_remote_debug/index.mdx","c":["IntelliJ + TypeScript + Docker で Remote Debug (Break Point)"," を実行 tags: [\"IntelliJ\", \"Typescript\", \"Docker\", \"Remote Debug\"] cover_image: https://res.cloudinary.com/silverbirder/image/upload/v1614345272/silver-birder.github.io/blog/ts-node-dev.png socialMediaImage: https://res.cloudinary.com/silverbirder/image/upload/v1614345272/silver-birder.github.io/blog/ts-node-dev.png","import { Image } from \"~/components/image/image\";","TL;DR","Docker コンテナ上で、 "," を実行","IntelliJ 上で、","IntelliJ 上で BreakPoint を貼り、ブラウザにアクセス","※ Docker コンテナでは、アプリ用ポート(8080)と、inspect 用ポート(9229)を開放する必要あり"],"t":"IntelliJ + TypeScript + Docker で Remote Debug (Break Point)"},{"f":"src/routes/blog/contents/intro_google_account_photo_api/index.mdx","c":["Googleアカウント画像を返却するだけのAPIを作った","みなさん、ご自身のプロフィール画像ってどう管理していますか？例えば、zenn のプロフィール画像って、更新していますか？ 私は、プロフィール画像の更新は面倒なので、放置することが多いです。(Gravatar みたいな SaaS が使えたら良いのに...) 最近、自身の","刷新を検討しており、プロフィール画像をどうするか悩みました。ポートフォリオのベースドキュメントは、Markdown を採用しています。 プロフィール画像を固定で保持させず、API 経由でプロフィール画像を設定できないかと思い、今回、","Google アカウント画像を返却するだけの API、Google Account Photo API","を作成しました。 API のソースコードは、","です。1 時間程度で作ったので、正常パターンしか見ていません。(笑) ご了承ください。","Google アカウント画像ってどれ？","Google のアカウント画像は、"," で表示されている右上の画像です。(ログインしている方のみ)","API の使い方","API を呼び出すために、あなたの Google アカウント ID というものを用意する必要があります。","Google アカウント ID の調べ方","あなたの Google アカウント ID は、"," するだけで分かります。 実行すると、"," というフィールドが返却されるので、そこに書いてある account_id が、あなたのモノになります。","API を呼び出す","API は、次の URL に GET 呼び出しします。YOUR_ACCOUNT_ID は、さきほど手に入れた account_id になります。","呼び出すと、画像を返却されます。私の場合は、次の画像が返却されます。","Markdown で活用する","この API を活用すれば、次のような Markdown を書くだけでプロフィール画像を表示することができます！","これだけだと、ちょっと味気ないので、Cloudinary を使います。Cloudinary は、URL のパラメータを設定するだけで、画像を加工できます。例えば、画像を円にする場合は、次の URL を書きます。","Cloudinary についての説明は、割愛します。 私の場合は、次のような画像が表示されます。 Cloudinary について、詳しくは次の URL を確認ください。","終わりに","サクッと API を構築できちゃうのって、便利な世の中だな〜と思いました。"],"t":"Googleアカウント画像を返却するだけのAPIを作った"},{"f":"src/routes/blog/contents/intro_rocket/index.mdx","c":["Markdownで執筆するなら、WebComponentsが使えるSSG、Rocketがオススメ！","Markdown でブログやドキュメントを書いていますか？ 執筆活動に集中したいのに、Markdown だけだとかゆいところに手が届かないもどかしさ、感じたことありませんか？ そんな方に、Markdown と WebComponents がシームレスに統合できる静的サイトジェネレータ(以降、SSG と呼ぶ)、Rocket をおすすめします。","対象読者","(ブログなど)執筆活動に集中したい人","執筆に、Markdown を利用している","執筆したコンテンツを SSG で公開している人 SSG の移行コストを極力減らしたい人","そもそも、Markdown って何？","Markdown は、Qiita や Zenn、はてなブログなどの各サービス(以降、執筆サービスと呼ぶ)、に使われていたり、Git リポジトリの説明書として README.md を書いたりしますよね。 その Markdown ですが、どういう目的で作られたモノなのでしょうか。"," から引用します。","Markdown is a text-to-HTML conversion tool for web writers. Markdown allows you to write using an easy-to-read, easy-to-write plain text format, then convert it to structurally valid XHTML (or HTML).","Markdown は、","Web ライター","向けに開発された PlainText から HTML へ変換するためのツールです。 また、Markdown には","書きやすさ、読みやすさが大切","です。 Web ライターは、ブログ記事やネット広告の文章など、Web 向けコンテンツを執筆する人です。 そう、","執筆","です。Markdown は、執筆のための道具です。 そのため、ブログ記事や Git リポジトリの説明書に Markdown を用いるのは、目的に合っています。 逆に、構造的な特徴を利用して、一種のデータファイルとして Markdown を使ったり、ショッピングやゲームといったアプリケーションに Markdown を使うのは、目的に反します。","Markdown と HTML","Markdown には、見出しや箇条書き、テーブルなどの記法(シンタックス)があります。 これらの記法を用いて、構造的に記事を書くことができます。 執筆で欲しい記法がなかった場合は、どうしたら良いでしょうか。"," より引用します。","For any markup that is not covered by Markdown’s syntax, you simply use HTML itself. There’s no need to preface it or delimit it to indicate that you’re switching from Markdown to HTML; you just use the tags.","Markdown には HTML が使えます。執筆サービスの Markdown で、HTML を書いてみると、恐らく使えるはずです。 Markdown は HTML へ変換するという目的を考えると、HTML が使用できるというのは納得できると思います。 ただし、","HTML を使用することで、読みやすさや書きやすさは少し悪くなってしまうため、多用は避けなければいけません","。","HTML では物足りない","執筆サービスを使ってみるとわかると思いますが、おおよそ次の機能が提供されています。","埋め込み(Embed)コンテンツ","URL を書くと、Description やタイトル、画像を表示してくれる","目次(TOC)生成","文章の見出しを収集し、目次を生成してくれる","これらの機能によって、執筆したコンテンツが読みやすくなったり、執筆の効率性が向上したりします。 当たり前ですが、Markdown には、そのような機能が存在しません。 Markdown は、記法を定義しているだけなので、Markdown に機能拡張を望んでいる訳ではありません。 しかし、執筆をしていくと、それらの機能が","どうしても欲しくなってきます","。 機能がなくても、Markdown 記法を駆使すれば、埋め込みコンテンツっぽく表示できますし、目次も手動で生成できます。 ただ、本来執筆に集中したいのに、見出しが増えるたびに、目次を手動更新するというのは、非効率的です。 その非効率、どうしたら良いでしょうか。","案 1. Markdown から HTML への変換処理で、機能拡張する","Markdown から HTML への変換処理で、埋め込みコンテンツや目次生成といった機能を拡張します。 具体的な話をした方が分かりやすいと思うので、目次生成を例にして、説明します。 説明しやすいために自前で変換処理を書きますが、本来は、Hugo や GatsbyJS、MDX などを想定しています。"," がちょうど分かりやすかったので参考にします。 Markdown と変換処理の transform.js を、次のものとします。","transform.js は、とてもシンプルです。README.md を html に変換して標準出力するだけです。 実行してみましょう。","期待通り、HTML が出力されました。次は、目次生成です。 はてなブログでは、目次生成に "," というマーカーを書くと、そこが目次となります。 脱線ですが、"," という、Markdown に変換処理をしてくれるツールがあります。 目次生成のサンプルコードを書いていきます。","とても馬鹿げているコードだと思いますが、伝えたいことが書けているので、これで良いです。 実行してみます。","期待通り、Markdown の目次が生成されています。 これは簡単な例ですが、機能拡張していくと、transform.js の処理が増えたり、README.md にマーカーがたくさん書かれていきます。 このように変換処理に機能拡張するのは、変換処理に機能を一任できるというメリットがあります。 ですが、","Markdown が変換処理に依存してしまう","こととなってしまいます。 これは、変換処理を違うものへ移行するときに","移行コスト","が発生してしまいます。 また、Markdown 自体に、","Markdown 記法や HTML でもないマーカーを埋める","というのも、ちょっと違和感を感じます。","案 2. WebComponents で、機能拡張する","WebComponents は、Web 標準技術の 1 つで、HTML 要素を独自にカスタマイズできる機能(Custom Elements)があります。 例えば、目次生成するための HTML 要素、","を WebComponents で開発したとします。 この HTML 要素は、全ての見出しテキストを収集し、箇条書きで表示するだけの WebComponents だとします。 Markdown のイメージは、次のとおりになります。","この Markdown を、任意の HTML 変換処理(さきほどの transform.js でも可)をすると、次の結果になります。","Markdown は HTML を許容するため、","が、そのまま HTML 出力されます。 このままだと、ブラウザが "," を識別できません。そのため、","を定義したコード、つまり WebComponents を読み込む必要があります。 例えば、次のようなコードを読み込みます。","これで、ブラウザは ","を識別できるようになったため、期待通り目次が表示されます。 WebComponents を利用するメリットは、","変換処理に依存せず WebComponents に依存します","。ブラウザの標準技術に依存するというのは、全く問題ありません。 変換処理の移行をしても、WebComponents のコードがあれば、同じ動作が実現できます。 また、再掲ですが、Markdown に次の文章があったとしても、Markdown の仕様に反しません。","Markdown の目的や仕様、Web というプラットフォームを考慮すると、Markdown と WebComponents の組み合わせは、相性が良いと思います。","ようやく登場、Rocket","お待たせしました、ようやく Rocket の登場です。 Rocket は、Markdown と WebComponents をシームレスに統合できる SSG です。 Modern Web と呼ばれる Web 標準技術の開発支援を行うプロジェクトがあり、その中のサブプロジェクトとして","があります。 他のサブプロジェクトとして、","と","の","、WebComponents の開発、テスト、リンターなどの","があります。 Rocket の事例は、次のものがあります。","https://modern-web.dev/ https://rocket.modern-web.dev/ https://open-wc.org/ https://apolloelements.dev/","Rocket は、技術的には、Eleventy という SSG の Wrapper になります。 Eleventy は、Markdown を HTML へ変換してくれます。Rocket は、その Eleventy に Modern Web の技術(WebComponents,TestRunner,DevServer)を混ぜています。","Modern Web って？","Javascript を使って開発すると、Babel のトランスパイラ、ESLint のリンター、Jest のテスター、Webpack のビルダーなど、扱うツールが多く、必要以上に複雑になり、開発者は疲弊してしまいます。 本来、開発に注力すべきなのに、それらの複雑さによって、アジリティ低下につながることを、開発者は知っています。 そこで、WebComponents や、ESModules といった Web 標準技術で開発することで、複雑さといったものを軽減していく狙いが、Modern Web にはあります。 ※ JSDOM のようなブラウザ API をモックすることでテストするのではなく、本来動いているブラウザでテストするテストランナーもあります。 Modern Web は、そういった Web 標準技術の開発を支援します。","Rocket の特徴","に、Rocket の特徴を 6 つ書いてあります。 しかし、本記事の流れ的に Markdown と WebComponents の統合についてを説明すべきだと思うので、次の 1 つだけ特徴を紹介して、その他は割愛します。","Meta Framework","Build on top of giants like Eleventy, Rollup, and Modern Web.","Eleventy や(話題にしていませんでしたが)Rollup、Modern Web という巨人の肩に乗ることで、Rocket の魅力があると思っています。 これまでの話で、『Eleventy で Markdown を HTML に変換して、WebComponents を読み込ませればよいでしょ？Rocket 必要？』と思う方がいるかもしれません。実際、その 2 つだけあれば充分だと思います。 ただ、Modern Web というプロジェクト支援があると、開発アジリティは向上します。 具体的には、Markdown や Javascript 変更による自動リロード、","、","などがあります。 まあ、必須ではないので Eleventy と WebComponents でも良いと思いますが、私は Rocket を使います。","Markdown Javascript","Markdown と WebComponents の統合について説明します。 Rocket には、Markdown Javascript という機能があります。これは内部的に MDJS というライブラリを使っています。 以下に、MDJS についての InfoQ の記事がありますので、よければご参照ください。","Markdown Javascript は、Markdown に Javascript を記入でき、インタラクティブに実行できる機能を備えています。 例えば、次のような Markdown を書いたとします。","これを書いて、Rocket で実行すると、ブラウザの開発ツールのコンソール画面に ","と表示されます。 これを応用して、WebComponents を定義することもできます。","これを Rocket で実行すると、画面に "," と表示されます。 このように、Markdown 上に WebComponents を定義し、インタラクティブに実行されるため、","即座に WebComponents を使うことができます","。 使い捨ての WebComponents であればこれで良いのですが、使いまわしたいときがあると思います。 そういう場合は、共通する箇所に WebComponents を定義するのが良いでしょう。 Numjucks の script ヘッダに、共通化したい WebComponents を書いてあげると、どの Markdown からでも定義した WebComponents を使えます。","Bare Import のサポート","Rocket は、Modern Web の","を内部で使用しています。開発サーバーには、","。 Bare Import の例を示します。 事前に "," インストールしていることを前提とした場合、次の Markdown は","が実行されます。","このように、相対パスや絶対パスを意識せず Bare で指定できるようになります。 ちなみに、canvas-conftti は、紙吹雪を出してくれます。見たほうが早いと思うので、押したら紙吹雪が出るボタンを下に表示しますね。 Click Me!","WebComponents のコミュニティからライブラリを使う","独自に WebComponents を書かなくても、次の WebComponents のコミュニティサイトから良さそうなものを使うこともできます。","例えば、","という WebComponents を使ってみたいとします。emoji-picker-element は、絵文字キーボードの UI に似ています。Mac なら、command + control + スペースキー で表示されます。 使い方は、簡単です。 先ほどと同じく、"," でインストールしておけば、次の Markdown を書くだけで ","が使えます。","を下に出しておきますね。","宣伝","WebComponents についての入門書を Amazon で、500 円で販売しています。 今回の Rocket については書いていませんが、","のテストについて触れています。","また、私のポートフォリオページを Rocket で作成しています。このブログも Markdown で執筆しています。よければご覧ください。","このブログの Markdown ファイルは、","終わりに","Rocket の紹介が、随分と後ろの方になってしまいました。前置きが長すぎたかもしれません。 少しでも誰かのお役に立てればと思います。"],"t":"Markdownで執筆するなら、WebComponentsが使えるSSG、Rocketがオススメ！"},{"f":"src/routes/blog/contents/intro_to_LLVM-JIT_compilation_of_javascript_with_LLVM_Rust_inkwell/index.mdx","c":["LLVM入門 - javascriptをLLVM(Rust:inkwell)でJITコンパイルするまで","import { Image } from \"~/components/image/image\"; コンパイラ基盤である LLVM について、全く知識がない私が、 javascript ソースコードをパースし LLVM でコンパイルできるようになりました。 LLVM の記事は数多くありますが、初心者向けの記事が少なく感じたため、 本記事では、できる限り分かりやすく LLVM について紹介できる記事を書こうと思います。 ソースコードは、こちらに置いています。 https://github.com/silverbirder/rustscript","自己紹介","ふだん、javascript や python などインタプリタ言語を使うエンジニアです。 LLVM について、全く知識がなかった人間です。","背景","過去に、おもちゃのブラウザ自作をやってみました。(",") HTML と CSS を解析し、レンダリングするところを書き、基本的な動作を知ることができました。 HTML と CSS とくれば、次は JS だと思い、JS を実行するエンジンを書いてみたくなりました。 ただし、Web ブラウザの API と JS 実行エンジンをバインディングする箇所(EX.DOM 操作)は難しいので、 まずは、単純な処理、四則演算や fizzbuzz が処理できるものを作ろうと思いました。","コンパイラとは","コンパイラとは、","compiler is a computer program that translates computer code written in one programming language (the source language) into another language (the target language). The name \"compiler\" is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g. assembly language, object code, or machine code) to create an executable program.","※ ","に書かれている通り、あるコードを別のコードに変換するプログラムのことをコンパイラと指します。 主に、高級言語(ex. javascript)から低級言語(ex. アセンブリ言語)への変換という意味でコンパイラが使われます。 プログラムをコンパイルするというのは、主に次の順番で処理されます。","ソースコード 字句解析 構文解析 構文木 中間言語 コード生成","字句解析 ~ 構文木は、lex や yacc というソフトウェアが有名だと思います。 今回は、swc_ecma_parser というものを使います。swc_ecma_parser は、","で使われるパーサです。","EcmaScript/TypeScript parser for the rust programming language. Passes almost all tests from tc39/test262.","tc39/test262 のテストケースをほとんどパスしているようです。 ","は、次の仕様動作を保証するテストスイートです。","実際のテストコードは、","にあります。 パーサ部分を自作しようか悩みました。 自作するには、次の手順を踏むことになります。","言語文法の理解 パース処理の実装 BNF や PEG からパース自動生成も可","① 番の言語文法について知るために、ecmascript の BNF ってどれだろうなと調べていました。 そうすると、私の調べた範囲では、次のページにたどり着きました。","これは、先程の","のテストスイート対象","であったので、あえて再構築する気になれず、自作は諦めました。 中間言語 ~ コード生成については、LLVM というコンパイル基盤を使おうと思います。","LLVM とは","LLVM とは、公式ページより、","The LLVM Project is a collection of modular and reusable compiler and toolchain technologies.","LLVM プロジェクトとは、再利用性が高いコンパイラとツールチェイン技術の総称です。 LLVM は、次の特徴があります。","LLVM is a set of compiler and toolchain technologies, which can be used to develop a front end for any programming language and a back end for any instruction set architecture. LLVM is designed around a language-independent intermediate representation (IR) that serves as a portable, high-level assembly language that can be optimized with a variety of transformations over multiple passes.","LLVM は、任意のフロントエンド言語(コンパイラという文脈でいう変換前の言語)から任意の命令セットアーキテクチャ(以下、ISA)バックエンドへ変換できます。 また、非言語依存な中間言語(以下、IR)を中心として設計されています。 命令セットアーキテクチャは、次の意味になります。","命令セットとは、あるマイクロプロセッサ（CPU/MPU）を動作させるための命令語の体系。プロセッサが直に解釈して実行できる機械語（マシン語）の仕様を定めたもの。","プロセッサを動作させるための命令は、例えば Load(LDR)と Store(STR)です。Load は、メモリからレジスタへセットし、Store は、その逆です。 後で紹介する","に一覧があります。 今回、LLVM のフロントエンド言語は、タイトルにある通り、Rust で書こうと思います。 単に Rust でやってみたかっただけです。 LLVM ライブラリとして、","を使用します。 これは、LLVM の C API を安全に使えるようにする薄いラッパーライブラリです。 LLVM のバックエンドは、ローカルマシンで動かすこととします。 具体的には、"," になります。 試していないですが、WASM もバックエンドとして選択できるようです。 というのも、過去の記事(",")ですが、LLVM がバックエンドとして WebAssembly(以下,WASM)をサポートしました。","ちなみに、WASM は、仮想的な ISA として設計されています。","WebAssembly, or \"wasm\", is a general-purpose virtual ISA designed to be a compilation target for a wide variety of programming languages.","LLVM 開発で、知っておくべきこと","LLVM では、IR を生成します。 その IR では、"," という構成になっています。 これを知っていないと、LLVM のコードを見ても、理解しにくいと思います。(自身が持つ言葉で解釈して誤った理解になりかねません) 小さな C 言語コードと IR で例を示します。 Rust じゃなく、C を選んだのは、clang から手軽に IR を出力できるからです。","これを IR に変換","出力されたファイルは、","という IR ファイルです。 そこから、","コードを見ます。","IR を Module,Function,Block,Instruction で区切って見ると、次の画像のとおりです。 それぞれ、どういうものか簡単に説明します。","Module","LLVM programs are composed of Module’s, each of which is a translation unit of the input programs.","モジュールは、入力プログラムの変換単位になります。 モジュールには、関数、グローバル変数、シンボルテーブルエントリを持ちます。","Function","LLVM function definitions consist of the “define” keyword. A function definition contains a list of basic blocks.","関数は、複数のブロック(Block)を持ちます。","Block","Each basic block may optionally start with a label (giving the basic block a symbol table entry), contains a list of instructions, and ends with a terminator instruction (such as a branch or function return).","ブロックは、ラベルから始まり、複数の命令(Instruction)を持ちます。","Instruction","The LLVM instruction set consists of several different classifications of instructions: terminator instructions, binary instructions, bitwise binary instructions, memory instructions, and other instructions.","命令は、バイナリ命令やメモリ命令など、様々な命令があります。","参考資料","参考になる資料たちは、次のとおりです。","チュートリアル","C++ ","Rust ","codegen が動かないため、途中までしか使えません","Rust + inkwell ","LLVM リファレンス","LLVM をやってみよう","前置きが長くなりましたが、実際に LLVM をやっていきたいと思います。","開発環境","私の環境(Mac)はこちらです。","llvm のインストールは、Mac ユーザなので、","をインストールします。 ","もできるようです。 インストールが完了すると、clang や llc といったツールが使えます。","Mac には Xcode に clang が含まれているようです。こちらを使っても問題ありません。 (ただ、xcode の clang には、",")","Cargo.toml の","は、次のとおりです。","\"Hello World\" を出力","まずは、Hello World を出力します。 Rust のコードは、次のものになります。","実行してみます。","LLVM の JIT コンパイラで実行できました。 ちなみに、IR がどんなものか確認したい場合は、"," を使いましょう。 実際に出力してみると、次の結果になります。","Rust の","は、IR の","関数を実行しています。 ","関数では、","関数を実行していますが、それは、C 言語の","になります。 IR のコードに関する調査は、"," が重宝します。 ","を調査してみると面白いです。","SUM","次は、3 つの数値を引数とし、足し算した結果を返す関数 SUM を作成してみます。 Rust のコードは、次のものになります。","見事、","の足し算ができました。 ちなみに、IR も出力しておきます。","前回同様、Rust の","関数に該当します。 足し算の","が使えました。","FizzBuzz","では、次は FizzBuzz をしてみます。割り算や if の命令が新しく使います。 Rust のコードは、次のものになります。","if 文では、","と","がどうやら使うそうです。 ","で書いてありましたので、使ってみました。 実行してみます。15 を引数として呼んでいます。","成功です！ ちなみに、IR も出力しておきます。","Block がめちゃくちゃ増えました。それは FizzBuzz の if,else が多いからですね。 LLVM について、少し自信がついてきました。 これまで","を LLVM でやってみました。 少し戻って、","の部分、つまりパース処理をやってみます。","四則演算する Javascript をパース","javascript をパースしてみます。","を使います。 パースする javascript は、次のものになります。","Rust のコードは、次のものになります。","それっぽい結果(20.0 や 10.0)が出力されましたね！","四則演算する Javascript を LLVM で実行","最後に、swc_ecma_parser と LLVM を組み合わせて、","を繋げ、四則演算する JS をパースし、LLVM で実行してみます。 パースする javascript は、次のものになります。","つまり、","が出力されました！やった！","終わりに","これにて、簡単な javascript コードをパースし、LLVM で実行できるところまでたどり着きました。 当初、LLVM の使い方って全然わからなかったのですが、段階的にできる部分が増えると、分かる領域が増えて、モチベーションが高まります。 LLVM の勉強をされている方、参考にしてみてください。"],"t":"LLVM入門 - javascriptをLLVM(Rust:inkwell)でJITコンパイルするまで"},{"f":"src/routes/blog/contents/know_the_history_before_learning_React/index.mdx","c":["Reactを学ぶ前に歴史を知る","React は、どうして生まれたのか歴史について簡単に紹介します。(ちょっと調べただけ)","背景","最近、React を学び始めました。その過程で、どうして React って学ぶべきなのか、どういう特徴があるのか気になった次第です。","歴史","言語やフレームワーク問わず、何かしら学ぶ際には、学ぶ対象の歴史や背景を知ることは大切だと思います。 対象ソフトウェアの公式ドキュメントに、","や","、"," を読むと良いです。 しかし、React の公式ドキュメントは、そのような情報を発見できませんでした。 React の作成者 Jordan Walke の","を発見したので、それを読みました。 ざっくり要約すると、","Facebook の広告系のプロダクト(Ads)のコードベースが大きくなった コードベースツリー(DOM ツリー?CSSOM ツリー?)の下位の変更で、全体の再描画(カスケード更新)が必要になり、メンテナンスが大変になった Reactive 特性(状態変化に基づく自動更新) を活かした FaxJS (後の React)が誕生","カスケード更新","カスケードとは、"," より","カスケードとは、何段も連なった小さな滝のこと。転じて、同じものがいくつも数珠つなぎに連結された構造や、連鎖的あるいは段階的に物事が生じる様子を表す。","Web ページの見栄えを定義する CSS（Cascading Style Sheet）で、ある要素に適用されるスタイルを、大域的に定義されたものから局所的に定義されたものへと順番に引き継ぎながら適用していくことをカスケードあるいはカスケーディングという。","ブラウザにおける CSS や DOM の更新が、まさにカスケードです。 例えば、DOM の場合、親 Node から子 Node へ(API 等で取得した)データを伝搬します(prop)。 データ変更があった場合、親 Node から子 Node を順々に、再描画されます。(レンダリングエンジンの処理に従い) CSS や DOM のツリーの縦横が大きくなるにつれて、カスケード更新が大変になるのは、想像しやすいと思います。","React の目的","React は、この","コードベース規模が肥大化","した際における","カスケード更新を簡単にできること","を目的としたフレームワークだと、私は思いました。 逆を言えば、","Not コードベース規模が肥大化"," OR ","Not カスケード更新"," ならば React は不要なのかなと思います。","仮想 DOM と差分検出処理(reconciliation)","DOM 自体のレンダリングは、ブラウザのレンダリングエンジンに依存します。 React は、DOM ではなく、仮想 DOM と呼ばれる概念を生み出しました。","仮想 DOM とは？ 仮想 DOM (virtual DOM; VDOM) は、インメモリに保持された想像上のまたは「仮想の」UI 表現が、ReactDOM のようなライブラリによって「実際の」DOM と同期されるというプログラミング上の概念です。このプロセスは差分検出処理 (reconciliation)と呼ばれます。 このアプローチにより React の宣言型 API が可能になっています。あなたは UI をどのような状態にしたいのか React に伝え、React は必ず DOM をその状態と一致させます。これにより、React なしではアプリケーションを構築するために避けて通れない属性の操作やイベントハンドリング、および手動での ","DOM 更新が抽象化されます","。","カスケード更新をブラウザに任せるというより、フレームワーク(React)で仮想 DOM を管理し、(全体の再描画する必要なく)必要箇所のみ DOM を更新(再描画)するのだと思います。","React とカスケード更新","React を学ぶ前には、この","カスケード更新という単語を脳の片隅に置いておく","のが、良いと思います。 例えば、React.memo という関数が存在します。","もしあるコンポーネントが同じ props を与えられたときに同じ結果をレンダーするなら、結果を記憶してパフォーマンスを向上させるためにそれを React.memo でラップすることができます。つまり、React はコンポーネントのレンダーをスキップし、最後のレンダー結果を再利用します。","この関数も、React の目的であるカスケード更新を改善するための関数だと、思います。","小話","何かを学ぶ際に、次の 3 つの軸を考えると良いです。","時間","過去、現在、未来","どういう経緯で現在に至るのか。未来は、どうありたいか。","周辺","類似するモノ、競合他社。","React でいうと、Vue.js のようなコンポーネントベースのフレームワークとか。","社会","チーム、グループ、会社、社会。","React は、チームでなぜ選ばれたか。組織的な理由(採用など)も、含まれる。"],"t":"Reactを学ぶ前に歴史を知る"},{"f":"src/routes/blog/contents/know_your_browser_layout_and_paint/index.mdx","c":["ブラウザのレイアウトとペイントを知る","import { Image } from \"~/components/image/image\"; ブラウザのレンダリングエンジンにおけるレイアウトやペイントについて気になったので、調べました。 その内容をまとめます。レンダリングエンジンは、Chrome の Blink を題材とします。","レンダリングエンジンの処理工程","レンダリングエンジンの処理工程は、次の記事が参考になります。","https://web.dev/rendering-performance/ https://blog.leap-in.com/lets-learn-how-to-browser-works/ https://silver-birder.github.io/blog/contents/learning_browser_engine/ https://developer.chrome.com/blog/inside-browser-part3/","(図には書いていないけど)Parse","HTML と CSS をパース DOM Tree と Style Rules を生成","JavaScript","視覚的な操作を処理","Style","HTML 要素が、どの CSS ルールが割り当たるかを決定 DOM Tree と Style Rules を紐付けた Render Tree を生成","Layout","HTML 要素の位置と大きさを決定 Layout Tree を生成 Reflow とも呼ぶ","Paint","ブラウザに表示するピクセルを塗る レイヤーを分ける Draw とも呼ぶ","Composite","正しい順序で、レイヤーを重ねていく メインスレッドからコンポジットスレッド・ラスタースレッドに切り替わる","コンポジットスレッドから、ページを各タイルに分割して、ラスタースレッドに送る ラスタースレッドは、ラスタライズして GPU に格納する","この工程が、実際に動いているところを見てみましょう。","DevTools でレンダリング工程を見てみる","次のシンプルな HTML を Chrome で開いてみましょう。","開いたページで DevTools を開き、Performance タブをクリックします。 左上にある reload ボタンを押して、計測してみましょう。 計測の結果、 Main を見てみましょう。 さきほど説明したレンダリングエンジンの工程(色も一致)が、見えると思います。","青色 ","紫色 ","(黄色は JavaScript 関係) (緑色は Paint/Composite 関係)","視覚的に見やすい一方で、全体を網羅してみるのは難しいです。 そこで、 "," を開きます。 レンダリングエンジンのイベントログが、色とともに表示されています。 ここには、さきほど見れなかった黄色や緑色のものもあります。","Tips: Performance タブに慣れよう","Performance タブには、様々な情報があります。 いきなりプロダクションリリースされているものに対して、Performance 計測すると、何を見たらよいかわからなくなります。 まずは、最小セットの HTML で見ていくと、情報量が絞られて、読みやすくなります。 また、計測の各場所には、工程の色が使われています。色も合わせて見ると、読みやすくなります。","ブラウザとリフレッシュレートと 60fps","ブラウザでアニメーションなど動きを出すときに、60fps を目標とすると良いです。 http://jankfree.org/ というサイトから引用します。","Modern browsers try to refresh the content on screen in sync with a device's refresh rate. For most devices today, the screen will refresh 60 times a second, or 60Hz. If there is some motion on screen (such as scrolling, transitions, or animations) a browser should create 60 frames per second to match the refresh rate.","ブラウザは、リフレッシュレートと同期してコンテンツを更新します。 最近のデバイスは、1 秒間に 60 回更新できるようです。そのため、ブラウザは 60fps で動作すべきと書いています。 DevTools から、fps を確認できます。 Rendering タブにある ","にチェックを入れます。 そうすると、画面に次の画像が表示されます。 今、ブラウザは 18.6 fps のようです。 fps が少ないと、どうなるんでしょうか。ジャンクと呼ばれる現象が発生します。","Jank is any stuttering, juddering or just plain halting that users see when a site or app isn't keeping up with the refresh rate. Jank is the result of frames taking too long for a browser to make, and it negatively impacts your users and how they experience your site or app.","リフレッシュレートに、画面が追いついていないと、ジャンクと呼ばれる滑らかではない動作になってしまいます。これは、ユーザーへの悪い体験をさせてしまいます。 https://googlechrome.github.io/devtools-samples/jank/ が、まさにそのジャンクの体験ができます。","レイアウトスラッシング","JavaScript や CSS を書いていると、DOM を追加してレイアウトが実行されたり、color を変えて、ペイントを実行されたりします。 レンダリングエンジンは、シングルスレッドで動いているため、レイアウトの実行やペイントの実行をしていると、他の工程が動作されません。 次のサイトにある JavaScript の関数を使うと、そのときのレイアウト情報を計算する必要があり、レイアウトが強制的に再計算されます。これがレイアウトスラッシングと呼ばれます。 レイアウトスラッシングは、FPS の低下につながります。","https://gist.github.com/paulirish/5d52fb081b3570c81e3a","例えば、clientWidth","例を示しましょう。ボタン要素にスタイル変更し、clientWidth を参照したコードです。","clientWidth を実行すると、そのときのレイアウト情報が必要になるため、強制的にレイアウトが実行されます。 強制レイアウトが発生しているのが、みてとれます。"," をコメントアウトすれば、Layout Forced は発生しません。 もっと、明らかに警告となるサンプルを用意しました。","DevTools の Performance タブから見ると、"," と警告が出ているのが分かります。 対策としては、次があげられます。","レイアウトスラッシングを発生させる関数を実行しない、もしくはキャッシュする"," を利用する","参考までに","https://web.dev/avoid-large-complex-layouts-and-layout-thrashing/#avoid-forced-synchronous-layouts","DEMO は、次のページにもあります。","https://googlesamples.github.io/web-fundamentals/tools/chrome-devtools/rendering-tools/forcedsync.html","Paint と Composite","Paint もコストがかかります。そこで、Composite に任せることで、メインスレッドを開放し、パフォーマンスが良くなります。 具体的には、コンポジットで動作する transform や opasity とかがあります。 具体的な例を出しましょう。 次の例は、四角のボックスを左右に動かすサンプルです。 左右に動かす手段に、CSS の left のパターンと、transform のパターンを試してみます。","transform の場合は、left の部分をコメントアウトし、transform 部分をコメントアウトを外します。 このファイルをブラウザで開き、Performance タブで計測し、"," を確認します。 left の場合、layout,paint,composite が発生しています。 transform の場合、composite のみ発生しています。 このように、composite のみで動く CSS プロパティを選ぶと、軽量になります。 次のサイトには、CSS のどのプロパティがレイアウト・ペイント・コンポジットどれを更新するのか分かります。","https://csstriggers.com/","また、DevTools の Layers タブを開くと、ペイントのカウント回数やレイアウトが見れます。 left の場合の Layers は、次の画像です。 数秒経過しただけで、ペイントカウントが、数百を超えました。 transform の場合の Layers は、次の画像です。 ペイントカウントが、たったの 2 回に留まりました。","終わりに","レイアウトやペイントについて、調査をしていると、意図せずレイアウトやペイントを実行させていた人も、いるかもしれません。 パフォーマンスは、必要になったときにチューニングすればよいと思いますが、基本知識として本記事についての情報は、知っておいて損はないと思います。","参考","https://gist.github.com/paulirish/5d52fb081b3570c81e3a https://dev.opera.com/articles/efficient-javascript/"],"t":"ブラウザのレイアウトとペイントを知る"},{"f":"src/routes/blog/contents/kubernetes_meetup_tokyo_19_osaka_satellite/index.mdx","c":["【大阪・梅田】Kubernetes Meetup Tokyo #19 大阪サテライト- 2019年5月31日参加レポート","import { Image } from \"~/components/image/image\"; 大阪から Kubernetes Meetup Tokyo に参加できるとのことで、こちらに参加してきました。 Kubernetes の生みの親である 3 人の内の 1 人の Joe Beda から、","Kubernetes の歴史","の経緯について教えて頂きました。 その話がとてもわかりやすく、なるほどなと思ったので、ぜひとも共有したいと思います。 https://k8sjp-osaka.connpass.com/event/131981/ ※ 以降の内容は、私なりの解釈が入っており誤った認識かもしれません。ご了承下さい。 発表の内容は全て Youtube にありますので、そちらが正しいものです。ご参考下さい。 https://www.youtube.com/watch?v=ETHGx8_Q-1k","Who is Joe Beda ?","Joe Beda は、Kubernetes の co-founder（共同創設者/最初に開発した 3 人のうちの 1 人）/ 昨年 VMware に買収された Heptio の CTO / O'Reilly「Kubernetes: Up & Running」 (邦題「入門 Kubernetes」）の共著者で、現在も Kubernetes をリードしている 1 人です。今回は、Kubernetes のこれまでと未来についてお話いただきます。","※ ","Kubernetes の最初のコミッターで、超有名人。 Google で働いていたときは、Kubernetes や Compute Engine を作っていたそうです。 Joe さん曰く、プラットフォームで開発する上でおもしろいことは、下記２点のバランスだと仰っていました。","ユーザーが","簡単","に使ってもらえる事 想定していなかった使われ方があった場合の柔軟性","私なりの解釈で言うと、例えば、GCP というプラットフォームの中で、GKE を使うとします。 ボタンをポチポチするだけでクラスターが作成されますよね。簡単で使ってみたくなります。 ただ、簡単だけだと細かい要求を満たせないので、オプションを設定できるようにしたり、 カスタマイズしやすいものへ改善されていきます。柔軟性ってことでしょうか？ この柔軟性をしすぎると複雑になってしまい、ユーザーが使ってくれなくなる恐れがあります（マニアックなユーザーは残るかもしれないけど）。 そこのバランスが大切なのかなと思いました。 Joe さんの詳細な説明は","です。","The origins and future of Kubernetes (en/英語)","Joe さんは英語で話されてました。 CPCAmerica(?)の田中さんが通訳をされていたのですが、ものすごくわかりやすかったです。感謝です！ あと、記憶力はんぱねぇ...。 https://twitter.com/mumoshu/status/1134438272518635521?s=20 ※ 以下、"," さんの要約 Tweet を参考にしました。神!!! https://twitter.com/silverbirder/status/1134406467744804864?s=20","kubernetes の歴史","Borg の誕生","Google では、BigData を処理するための","を開発していました。 MapReduce を扱うために、","(GWQ)というものを開発され、これは主にバッチのために作成されたものでした。そこからバッチだけでなく、リアルタイムに実行したい(検索など)サービスに対応するために生まれたのが Borg だそうです。 Google のような大規模な検索であれば、数％の効率 Up でも大きなコスト削減につながるメリットがあります。 これが、Kubernetes の元となりました。","Kubernetes の誕生","Google で Borg を開発を進めていく中で、世の中は仮想マシンを扱うユーザーが多かったそうです。 Borg はプロプライエタリなソフトウェアだったため、Borg の世界を知ってほしい、開発者を引き込みたいという願いから、 OSS として Kubernetes が誕生しました。 また Kubernetes は、API ドリブンで開発者の生産性を上げるというのが先で、効率やセキュリティは後からついてきたそうです。","Kubernetes の魅力","Kubernetes とは、「コンテナオーケストレーター」と多くの人は知っていると思います。普及した大きなポイントですね。 他の観点で「１つのデータベースだけでクラスタを管理している設計」が魅力的だという話がありました。 （勝手な解釈かもしれません。すみません） Kubernetes では、クラスタの状態を管理するために分散型 KVS である","を使っています(その他の状態管理はキャッシュしているそうです。)。 etcd には、APIServer を経由しなければアクセスできないため、一貫したデータの維持が実現できます。 その etcd の周りにある、ビジネスロジックを実現するコントローラー(",")が価値を提供します。 例えば、Pod を Node にアサインしたり、エンドポイントを提供したり、レプリケーションしたりなどなど...。 kubernetes の control plane である、APIServer, Scheduler, Controller Manager があれば、シングルノードでもマルチノードでも動きます。 kubernetes を DockerForMac で動かしたときは、そういえばシングルノードでしたね。マルチノードってイメージでしたけど。 Kubernetes はコンテナオーケストレーションとよく言われますが、事前にすべてがプランされたオーケストレーションではなく、ジャズのように即興で計画して組み立てていくものに近い思想だそうです。 私は音楽に疎い人なのですが意味は理解しました。（笑）性格的には即興は苦手っす。","CRD と Operators","Pod や Replication,Deployment など様々なリソースがあります。 ただ、Kubernetes が持っていないものを実装するにはどうすればよいのでしょうか。 そこで、Custom Resource Definitions (CRD)です。 なんだそれは...? https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/ https://qiita.com/cvusk/items/773e222e0971a5391a51 要は、Pod や Deployment のようなリソースを独自に作ることができるのですね。おぉなんだそれ！ 独自に機能を作るためには、Custom Resource と Costom Controller が必要になり、両方をあわせて Operators というものが生まれました。 例えば、下記のようなものがあります。 https://github.com/oracle/mysql-operator https://github.com/kubeflow/tf-operator Yahoo では、gimbal という OSS を使って Kubernetes を導入したみたいです。 https://github.com/heptio/gimbal https://techblog.yahoo.co.jp/advent-calendar-2018/oss-gimbal/ 詳しくは分かりませんが、こういった拡張しやすい機能があるおかげでドンドン普及するのだなと勉強になりました。","Q&A","Q1. StatefulSets には今回触れられなかったが、どういう扱いなのか","https://twitter.com/apstndb/status/1134409892033261569?s=20","Q2. スケーラビリティに関して","https://twitter.com/apstndb/status/1134410827627487232?s=20","Q3. Kubernetes はなぜ etcd を使っているか","https://twitter.com/apstndb/status/1134411776009785345?s=20 https://twitter.com/apstndb/status/1134412148237512705?s=20 https://twitter.com/apstndb/status/1134412317439844352?s=20","Q4. Virtual Kubelet とか k3s みたいなエッジで活用する動きがコミュニティでは感じられるが、どう見ている?","https://twitter.com/apstndb/status/1134413224839745536?s=20 https://twitter.com/apstndb/status/1134413431316987904?s=20","そのほか","参加者からの質問は、どれも鋭いものばかり。 適度な質問をしたいなとつぶやきました...。届かなかったけど...。 https://twitter.com/silverbirder/status/1134412867988480000?s=20","Osaka 会場","会場提供は、株式会社 Aiming さんでした。 https://aiming-inc.com/ja/ 会場場所は、グランフロント大阪タワー B の 18 階にありました。(高い!) 今回使わさせて頂いた場所は、会議室でしょうか。 30,40 人ぐらい入れるスペースで、清潔感がありました。 東京との中継は、ときどき音声が途切れてしまうときもありますが、しっかりと写っていました。 ただ、コンテンツとしては、YouTube にあげらているので、わざわざ Osaka に出席しなくても良いのでは？とも思いました。 しかし、それでも Osaka に出席しても良い面もあるのかなと思います。","他の方とのコミュニケーションが取れる 一緒に発表を聞いて議論ができる","まあ、私はコミュ障なので、ほぼなかったですが...。 改善ポイントとしては、","中継地からも質問ができる","ようになってくれたら良いなと期待しています。","最後に","Kubernetes について、どういった経緯で誕生したのか、また CRD についても勉強になりました。 また、Kubernetes とは違うのですが、「","OSS のちから","」というものがエンジニアの世界では大事だと強く感じました。 普段エンジニアが開発する上で、ほぼ間違いなく OSS を使っています。 エンジニアにとって、OSS は不可欠な存在であり、利用するばかりです。 Google がしたように、「広く使ってほしい、エンジニアを巻き込みたい」という願いから、 OSS として Kubernetes が広まっていった一要因と思いました。これが有償ならどうだったのでしょうか。 ここまで普及したのでしょうか。 OSS に貢献する企業は、日本にも多く存在します。 個人でも OSS へ貢献できますし、OSS Gate という初心者向けのものもあります。 Kubernetes のコントリビューターは、ちょっとハードルが高いですが、 私もエンジニアとして OSS へ貢献し続けていこうと思います。","拙い文章なのに、最後まで読んでいただき、ありがとうございます。 twitter をしていますので、フォローしてもらえるとうれしいです。(",")"],"t":"【大阪・梅田】Kubernetes Meetup Tokyo #19 大阪サテライト- 2019年5月31日参加レポート"},{"f":"src/routes/blog/contents/learn_point_from_building_tiktok_scrape_platform_on_gcp/index.mdx","c":["TikTokスクレイプ基盤をGCP上で構築してハマったこと","import { Image } from \"~/components/image/image\"; TikTok へスクレイプするバッチを GCP 上で構築しました。 GCP 構築のシステム設計話と、その構築時に、ハマったことを共有します。","きっかけ","2020 年、最もダウンロードされたアプリが Facebook を抜いて","TikTok","が一位になったそうです。 https://gigazine.net/news/20210811-tiktok-overtakes-facebook/ 私も TikTok を利用しています。 ネットサーフィンをしている時に、","というライブラリを","で発見しました。これを使って、TikTok の情報収集できるんじゃないかなと思い始めましたのがきっかけです。 ※ スクレイプは私的利用であることが前提です。また、TikTok へ負荷をかけないようスクレイプ間隔に配慮しましょう。","tiktok-scraper","https://www.npmjs.com/package/tiktok-scraper","Scrape and download useful information from TikTok. No login or password are required. This is not an official API support and etc. This is just a scraper that is using TikTok Web API to scrape media and related meta information.","上記とおり、TikTok の WebAPI を通してスクレイプします。 ライブラリでは、特定の TikTok 動画をダウンロードすることができますが、次の切り口で、TikTok 動画を一括ダウンロードすることもできます。","ユーザー ハッシュタグ トレンド 音楽","加えて、メタ情報(フォロワー数やいいね数など)も手に入ります。 中には、ユーザー画像や動画カバー画像などの TikTok CDN へのリンクもあります。(",") リンクには、有効期限を示す文字が含まれており、一定の時間が経過すると "," となります。 手に入れられない情報は、","ログインが必要なもの","です。 例えば、私がフォローしているユーザーとかです。 その情報が欲しかったので、どうにかして手に入れました。(詳細は省きます) そのユーザー情報を使って、先程のユーザーという切り口で TikTok の動画やメタ情報を収集するバッチを作ろうと考えました。 ※ WebAPI を叩きすぎると、TikTok 側のブラックリストに追加され、アクセス拒否されます。","システム設計","バッチを動かす環境ですが、プライベートでよく使っている GCP 上で構築しようと思いました。 バッチで収集したデータを閲覧する Web アプリケーションも作ろうと考え、Netlify と React で動かすことにしました。","目的","私がフォローしているユーザーの TikTok 動画やメタ情報を集めること。","I/O","インプット","ユーザー情報","アウトプット","TikTok 動画 メタ情報","GCP リソース選定","TikTok 動画","Cloud Storage へ保存","メタ情報","Cloud SQL へ保存","コンピューティングリソース","Cloud Run","設計図","実際に構築した GCP のシステム設計図が、次の画像のとおりです。 GCP リソースの用途は、次のとおりです。 | GCP リソース | 用途 | | --------------- | ------------------------------ | | Cloud Scheduler | バッチ起動のスケジュールを管理 | | Cloud Worlflows | バッチのワークフローを制御 | | Cloud Run | 役割に応じて処理 | | PubSub | Cloud Run を繋げる | | Cloud Storage | 動画を保存 | | AutoML Vision | 動画のカバー画像をラベル検出 | | Cloud SQL | 全てのメタ情報を管理 | 各 Clour Run の役割は、次のとおりです。 | Cloud Run 名 | 役割 | | ------------ | ------------------------------------------------ | | Loader | ユーザー情報を読み込む | | Processor | 一連の処理を行む | | Scraper | TikTok へスクレイプする | | Storer | 渡された情報を保存する | | Uploader | 動画をダウンロードし、Storage へアップロードする | | Visioner | 画像を(Vision API を通して)ラベル情報を抽出する | | API | Cloud SQL とのインターフェース |","ハマったこと","Cloud Workflows の制限が厳しい","当初、PubSub は使わずに、Cloud Run の連携は Cloud Workflows で行おうと考えていました。 PubSub でワークフローを制御するよりも、Cloud Workflows の yaml でワークフローを制御した方が分かりやすいと思ったからです。 具体的には、Cloud Run へ HTTP リクエストし、HTTP レスポンスに応じて、次の Cloud Run を呼び出そうと考えていました。 ただ、Cloud Workflows には、次のページに書いてあるとおり、いくつかの制限があります。 https://cloud.google.com/workflows/quotas?hl=ja 特に困ったのが、全ての変数のメモリ合計が、","64kb"," だということです。 HTTP レスポンスの Body を変数保持する構成を取ると、そのサイズを考慮しなければいけません。 いくつかやり方を見直してみたのですが、思うような形に仕上げることができず、断念しました。 結果、PubSub を使って Cloud Run を連携することになりました。 Cloud Workflows は、バッチのキック、通知などをすることとなりました。","Firestore のページカーソルに ±2 ページ以降への移動が難しい","GCP でデータストレージで、無料枠がある Firestore を当初使っていました。 理由は、単純に GCP 無料枠として Firestore があったからです。 当初、Firestore を使って、バッチと Web アプリを書いていました。 Web アプリには、バッチで収集した TikTok の動画を一覧表示する View を用意しました。 閲覧する TikTok 動画が多くなると、ページネーションが欲しくなりました。 そこで、Firestore でページネーションの実現方法を調べてみると、次の資料を発見しました。 https://firebase.google.com/docs/firestore/query-data/query-cursors?hl=ja これを見ると、ページネーションは、現在位置から ±1 ページの移動は簡単です。 資料にあるサンプルコードのように、","を使えばよいだけです。","しかし、現在位置から ±2 ページ目以降への遷移がしたい場合は、どうすれば良いでしょうか。 上記のサンプルコードで言えば、","をコピペして","変数を生成するのでしょうか。 それよりも、","メソッドがほしいところです。 しかし、次の資料を発見し、諦めることになります。 https://firebase.google.com/docs/firestore/best-practices?hl=ja","オフセットは使用しないでください。その代わりにカーソルを使用します。オフセットを使用すると、スキップされたドキュメントがアプリケーションに返されなくなりますが、内部ではスキップされたドキュメントも引き続き取得されています。スキップされたドキュメントはクエリのレイテンシに影響し、このようなドキュメントの取得に必要な読み取りオペレーションは課金対象になります。","という訳で、クエリカーソルを推奨されています。 解決策としては、順序を示すフィールドがあれば、解決するかもしれません。 例えば、","というフィールドを用意し、1,2,3 とインクリメントしたデータがあれば、クリアできるかもしれません。 ","の引数は document オブジェクトだけではなく、orderBy 句で指定したフィールドの変数を含めることができます。","これだと、1 ページ 25 個のデータを表示するならば、3 ページ目(51~75)を取得できます。(","は開始点を含めません) https://cloud.google.com/nodejs/docs/reference/firestore/latest/firestore/query そもそも、ドキュメントベースの設計よりも、RDB の設計に慣れていた私は、 Firestore よりも、Cloud SQL の方が扱いやすいと思いました。 そこで、データストレージを Firestore から Cloud SQL へ切り替えることとしました。 改修自体、Cloud Run の役割が明確に分離されていたので、一部の処理を書き換えるだけで、簡単にできました。","Eventac のリソース選択が物足りない","Cloud Run と PubSub の連携には、Eventac を使用します。 https://cloud.google.com/blog/ja/products/serverless/eventarc-unified-eventing-experience-google-cloud","昨年 10 月、60 を超える Google Cloud ソースから Cloud Run にイベントを送信できる新しいイベント機能、Eventarc を発表いたしました。Eventarc は、さまざまなソースから監査ログを読み取り、それらを CloudEvents 形式のイベントとして Cloud Run サービスに送信します。また、カスタム アプリケーションの Pub/Sub トピックからイベントを読み取ることもできます。","この Eventarc のソースとして、Cloud Storage の Object.create をトリガーとして設計を考えていました。 しかし、そのイベントをフィルタリングする選択肢は、2 つしかありません。 https://cloud.google.com/blog/ja/products/serverless/demystifying-event-filters-eventarc できるのは、執筆時点(2021 年 8 月)で、次の 2 つです。","All resource Specific resource","All resource は、Cloud Storage の全てのバケットにおける Object.create イベントがトリガーとなります。 Specific resource は、特定の Obeject 名が Object.create された場合のみ、トリガーとなります。 欲しいなと思ったのは、Specific resouce の正規表現によるフィルタリング、任意のバケットやフォルダの配下で限定など のフィルタリングです。例えば、"," のような形式です。現状は、","とするしかありません。 今回は、PubSub のイベントのみでトリガーするようにしました。","PubSub をトリガーとする CloudRun で HTTP レスポンス 500 を返却すると、PubSub が再試行される","Cloud Run で、5XX 系のエラーとなった場合、PubSub の再試行されます。 https://cloud.google.com/pubsub/docs/admin?hl=ja#using_retry_policies 何度も PubSub が実行されると、Cloud Run のコンピューティングリソースが消費され続けます。 そうすると、課金が発生するので、対策が必要です。","Cloud Workflows の処理は、あまりカスタマイズできない","Cloud Workflows は、あくまでワークフローの管理です。 変数処理などは、基本的に使わず、ワークフローのタスクを連結するだけにした方が良いです。 次の資料には、Cloud Workflows で使える標準機能です。 https://cloud.google.com/workflows/docs/reference/stdlib/overview ワークフローのタスクを並列処理する機能は、まだ実験段階なので、本番環境は使えないようです。 https://cloud.google.com/workflows/docs/reference/stdlib/experimental.executions/map","終わりに","システム設計変更が度々変更がありつつも、目的とする TikTok 動画やメタ情報を収集することは達成できました。 変更があったとしても、役割をできる限り小さく保つことで、変更に柔軟に対応することができます。 また、実際に動かすことで、気付けるポイントもあるので、フィードバックサイクルを短くすることも大切です。 まだまだ改善する余地はあります。ユーザー情報という切り口で情報収集していましたが、トレンドやハッシュタグなどからも 取得できるようにしたいです。また、ユーザーの RSS を作ることで、金銭的な節約もしてみたいと思っています。"],"t":"TikTokスクレイプ基盤をGCP上で構築してハマったこと"},{"f":"src/routes/blog/contents/learning_browser_engine/index.mdx","c":["ブラウザの仕組みを学ぶ","import { Image } from \"~/components/image/image\"; Web フロントエンジニアたるもの、","ブラウザの仕組みに興味を持つのは自然の摂理","です。本記事では、私がブラウザの仕組みを学んでいく過程を備忘録として残します。","みんな大好き Chrome","Web フロントエンジニアに愛されているブラウザといえば、~~IE~~Chrome ですよね。 ブラウザで HTML,CSS,JS の動作確認するのは、日常茶飯事です。 ブラウザによって動作が異なることは、Web フロントエンジニアなら周知の事実です。 じゃあ、なんで動作が違うのかというと、","「レンダリングエンジンが違うから〜」 「Javascript エンジンが違うから〜」","ぐらいは知っているんじゃないかなと思います。 じゃあ、そのレンダリングエンジンってどういう仕組みで動いているのでしょうか。 気になりますよね。","Chromium について","Chromium も、たぶんご存じの方多いのかなと思いますので、簡単に説明します。 Chromium は、オープンソースのプロジェクト名であり、ブラウザ名でもあります。 Chrome は、Chromium を元に開発されています。 詳しい説明は","を見てください。 オープンソースってことは、ソースコードが誰でも読めちゃうってことですよね。 だったら、ブラウザの動作を知ることができちゃうじゃないですか！ わーい！😎","Chromium のリバースエンジニアリング","ではさっそく、Chromium のソースコードを見ていきましょう。 これです。","によれば、Chromium のソースコードは約 3,500 万行あるそうです。 しかも、言語は C++。私はあまりそれを詳しくないのです 😞。 実際にソースコードをローカルマシン(Macbook Air)へチェックアウトし、ビルドをしてみました。 マシンが貧弱だというのもあるんですが、ビルドに半日ぐらいかかってしまいました。ヘトヘトです。 これじゃあさすがに、手軽にブラウザの動作確認はできそうにないです。","ブラウザの仕組み資料を読む","ちょっと趣向を変えて、次のような資料を読むことにしました。","記事の公開日が 2011 年 8 月 5 日なので、色々古いかもしれません。","では、さっそく見ていきます。 最初に目につくのが、ブラウザの主な構成要素です。 構成要素の内、ユーザーインターフェース、ブラウザエンジン、レンダリングエンジンに着目します。 それぞれ、次の役割があります。","ユーザーインターフェース","アドレスバーや戻る/進むボタンのような UI を担当","ブラウザエンジン","UI とレンダリングエンジンの間の処理を整理","レンダリングエンジン","要求されたコンテンツ(HTML など)の表示を担当","ちなみに、Chromium のレンダリングエンジンには、webkit を使っていましたが、blink に変わりました。","ブラウザの基本的なフローは、次の図の通りです。","Parsing HTML to construct the DOM tree Render tree construction Layout of the render tree Painting the render tree","それぞれ見ていきます。 ① Parsing HTML to construct the DOM tree 1 は、HTML をレキサ(字句解析. ex:flex)・パーサ(構文解析. ex:bison)を使って DOM ツリーを構築します。 レキサでは、ステートマシンによって読み込み状態を管理しつつトークンを識別します。空白とかコメントなどは削除されます。 レキサから識別されたトークンをパーサに渡し、構文解析していきます。 HTML は DTD（Document Type Definition）で文脈自由文法なため、機械的に解析できます。 ただ、HTML は寛大な仕様で、次のようなパターンも許容するようになっています。","の代わりの","迷子のテーブル 入れ子のフォーム要素 深すぎるタグ階層 配置に誤りのある html または body 終了タグ","パーサから DOM（Document Object Model）を構築します。 DOM は、これまでの単なるテキストから、API を持たせたオブジェクトモデルを作ることで、 以降は DOM を使って処理しやすくなります。 これまでは HTML の話をしていましたが、HTML と並行して CSS も同様に処理していきレンダーオブジェクトというオブジェクトを作っていきます。これは、スタイル情報を付与したオブジェクトになります。 基本的に、CSS と HTML は互いに独立しているので、並列処理が可能です。例えば、CSS を処理したことで、HTML が変化することはないはずです。 ただ、Javascript は話が違うので、Javascript が読み込まれた時点で HTML のパースを中断して Javascript のパースが開始されます。 また、Javascript が、まだ読み込まれていないスタイルシートの影響を受けそうな特定のスタイルプロパティにアクセスした場合、Javascript はブロックされます。 ② Render tree construction ① の DOM とレンダーオブジェクトから、レンダーツリーを構築します。 DOM とレンダーオブジェクトは、1 対 1 という訳ではなく、例えば head 要素や、","の要素もレンダーツリーに含まれません。 レンダーツリーの更新は、DOM ツリーが更新される度に行われます。 レンダーオブジェクトからスタイルを計算するのですが、ちょっと複雑です。 詳しくは、","を見てください。 ③ Layout of the render tree レンダーツリーから、レイアウト情報を計算していきます。 レイアウト情報とは、位置(x,y)とサイズ(width,height)です。 レンダーツリーのルートから再帰的にレイアウト情報を計算(layout メソッド)していきます。","親レンダラーが自身の幅を決定します。 親が子を確認して、","子レンダラーを配置します（x と y を設定します）。 必要な場合は子の layout メソッドを呼び出します。これにより、子の高さを計算します。","親は子の高さの累積、マージンの高さ、パディングを使用して、自身の高さを設定します。この高さは親レンダラーのさらに親によって使用されます。","※ "," 参考 CSS ボックスモデルの図を参考までに共有しておきます。 ④ Painting the render tree ようやく描画します。 どこに描画するかという配置方法について考えることになります。 大きく分けて、3 つに分かれます。","通常","オブジェクトはドキュメント内の場所に従って配置されます。つまり、レンダーツリー内の場所は DOM ツリー内の場所と同様になり、ボックスの種類や寸法に従ってレイアウトされます。","position:static,relative","フロート","オブジェクトは最初に通常のフローのようにレイアウトされてから、左右のできるだけ遠くに移動されます。","float:right,left","絶対","オブジェクトはレンダーツリー内で DOM ツリーとは異なる場所に配置されます。","position:absolute,fixed","配置方法が分かれば、今度は描画する形について考えます。ブロックボックスとインラインボックスです。 ブロックボックスは、短形の形であり垂直に並びます。 インラインボックスは、独自の形を持たず水平に並びます。 z-index のようなプロパティでは、スタッキングコンテキストという概念を知る必要があります。 詳しくは、"," をご確認ください。","ブラウザを自作してみる","前章では、資料を通してブラウザの動作が理解できました。 読むだけじゃなく、動かして理解してみたいとは思いませんか？ そうです、自作してみましょう。 Rust 製の Servo というブラウザエンジンを開発している人が書いた、次のブラウザ自作に関する記事がとても分かりやすいです。","Toy ブラウザエンジン(mbrubeck) Rust 製","Toy ブラウザエンジン(mbrubeck)のメインフローが、これまでの話ととても似ています。 Style tree は、これまでの話でいうと Render tree だと思います。 Toy ブラウザエンジン(mbrubeck)に、次の HTML と CSS を読み込ませると、下記の画像のようなアウトプットになります。","次のリンクにある自作ブラウザエンジンは、","を参考にして作られたものだそうです。","Toy ブラウザエンジン(askerry) C++製","Toy ブラウザエンジン(askerry)に、次の HTML と CSS を読み込ませると、下記の画像のようなアウトプットになります。 見たら分かると思いますが、とても高機能です。","私としては、こちらの方が興味があるので、まずこちらを知り、それを Rust 版で作り直したいなと思います。","C++ を学ぶ","さて、C++を学ぶために、次のサイトをざっと眺めてみます。","自作ブラウザのソースコード","のメインコード(main.cc)を載せます。","次のとおり、これまで学んできたメインフローと、C++がとても似ていることが分かります。","HTML と CSS をパース","1 の結果から Style tree(Render tree)を構築","2 の結果から Layout tree を構築","3 を paint という描画","Re: ブラウザの仕組み資料を読む","もう一度、"," を読むと、初めて読んだときに比べて、深く理解できるんじゃないかなと思います。","最後に","ブラウザの動作について資料や自作を通して理解を深めました。 ブラウザの動作が分かれば、ブラウザに優しい Web フロントエンド開発ができると思います。 (今度こそ Chromium のリバースエンジニアリングができるかもしれません。)","その他","Chromium のアドベントカレンダーがありました。参考までにざっと見てみると良いでしょう。 "],"t":"ブラウザの仕組みを学ぶ"},{"f":"src/routes/blog/contents/mac_babiniku/index.mdx","c":["Mac で バ美肉 りたい！ (Zoom + Gachikoe + 3Tene or Reality)","import { Image } from \"~/components/image/image\";","きっかけ","みなさん、リモートワーク（テレワーク）してますか？ Hangouts Meet や Zoom といったビデオ会議ツールを使う機会が増えたと思います。 そんな中、次の記事が流行りました。 https://level69.net/archives/26902","バ美肉（バびにく）とは、バーチャル美少女受肉またはバーチャル美少女セルフ受肉の略語","https://ja.wikipedia.org/wiki/バ美肉","これにより、ビデオ会議(例は Zoom)で、次のようなバーチャル美少女 (になりきった私)が参加できるようになります。もちろん、声もボイスチェンジできます。 Windows では、","というアプリで簡単に構築できるみたいです。 これを Mac で構築する方法を紹介しようと思います。 Mac + Bootcamp → Windows10 + Facering でもできると思いますが、動作不安定になる可能性があったため、極力避けようと思い、却下しました。","構成","私は、次のような構成になりました。 音声と動画の 2 つに分かれます。 また、ビデオ会議ツールと連携するため、仮想デバイス(Soundflower, CamTwist)が必要になります。","音声","Voice Changer: Gachikoe","野太い声じゃなくて、かわいい声が聞きたいですよね。そうですよね。はい。","を使いました。 Gachikoe は、次のような設定にしました。 Output を soundflower (2ch)にしています。","仮想マイク","仮想マイクは、Soundflower を使います。 https://github.com/mattingalls/Soundflower/tags 音声出力のルーティングを制御するために、LadioCast も使いました。 https://apps.apple.com/jp/app/ladiocast/id411213048?mt=12 LadioCast は、次のような設定にしました。 Input を soundflower (2ch)とし、Output を soundflower (64ch)としています。","動画","Application for VTuber","Desktop: 3tene","デスクトップで動かす場合は、3tene(ミテネ)を使いました。 https://3tene.com/ 3tene は、特に設定は必要ありません。 撮影前には、Web カメラとリップシンク(口の動きの同期)を起動しておきましょう。","Asserts","肝心のキャラクターですが、3tene は VRM 形式でなければならないそうです。(よくわかっていません) 私は、次のサイトでダウンロードしました。 https://hub.vroid.com/ https://3d.nicovideo.jp/","Mobile: Reality","モバイルで動かす場合は、Reality を使いました。 https://apps.apple.com/jp/app/reality-%E3%83%90%E3%83%BC%E3%83%81%E3%83%A3%E3%83%AB%E3%83%A9%E3%82%A4%E3%83%96%E9%85%8D%E4%BF%A1%E3%82%A2%E3%83%97%E3%83%AA/id1404176564 Reality は、特に設定は必要ありません。 好みのキャラクターをカスタマイズして簡単に作れます。 私は、これです。 iPhone で撮影している画面を Mac に反映する必要があります。 Mac と iPhone を接続し、QuickTime Player へ出力します。こんな感じです。 none は、私の iPhone デバイス名です。","仮想カメラ","CamTwist という仮想カメラを使いました。 http://camtwiststudio.com/download/ CamTwist は、次のような設定にしました。 例では、QuickTime Player のアプリケーションを選択しています。3tene の場合は、3tene の選択肢を選択すれば良いです。","使い方 (Zoom)","今まで説明したものを起動した状態で、Zoom を起動します。 Zoom は、次のような設定にしました。 動画は、CamTwist から取得するようにします。 音声は、Soundeflower (64ch)から取得するようにします。 これで、 Mac で バ美肉 することができました！","終わりに","テレビ会議で、こういった \"リアルな姿を出さず、異なる人物を出す\" のは、実際役立つものなのでしょうか。 テレビ会議ツール、例えば Zoom では、音声や動画を隠せる機能はあります。 \"リアルな姿を隠したい\"要求は、すでに解決できています。 今回のような\"バ美肉\"って、どういうメリットがあるのか、んーってなりました。 ネタ的には『可愛い女の子と会話すると、生産性があがる』なのですが...脳が震える。","参考リンク","https://kumak1.hatenablog.com/entry/2018/09/27/234203 http://kuroyam.hatenablog.com/entry/2020/02/27/204246 https://mzyy94.com/blog/2020/02/25/virtual-bishoujo-meeting/ https://www.excite.co.jp/news/article/MoguraVR_voice-changer-pickup5/ https://www.cg-method.com/entry/gachikoe/#Gachikoe https://vtuberkaibougaku.site/2019/01/31/post-3176/"],"t":"Mac で バ美肉 りたい！ (Zoom + Gachikoe + 3Tene or Reality)"},{"f":"src/routes/blog/contents/left_my_full-time_job/index.mdx","c":["正社員を辞めました","私は 2023 年 6 月末に正社員として勤めていた企業を退職しました。勤務期間は 1 年 6 ヶ月で、私にとっては短い期間でした。業務に対するモチベーションは高く、チームメンバーとも和気あいあいとした良い環境で仕事ができました。しかし、以下の理由により、仕事とプライベートのバランスが崩れてしまい、結果として退職することにしました。この記事は、私の感情を整理するためのものです。","会社の英語化に伴う勉強 諸事情による引っ越しと新幹線通勤 妻との 1 年間の妊活と不妊娠の問題 高齢の祖母への介護と手術の準備","英語化","会社の方針により、英語の勉強を始めることになりました。私の TOEIC のスコアが低く、また、諸事情により英語の研修を始めるのが遅れてしまったため、他の誰よりも一生懸命勉強しなければならないと感じていました。 2023 年 1 月から英語の勉強を開始し、平日の日中はチームとの合意により、チーム全員で 1 時間勉強することになりました。さらに、夜も 1 時間勉強することにしました。土日も 1 時間は勉強しています。","新幹線での通勤","元々、新幹線を利用した通勤を認識した上で、社内の移動制度を活用し、遠い地域の拠点に所属しました。当時、私の居住地は新幹線にアクセスしやすい位置にあったため、出社は楽でした。また、週一出社です。しかし、祖父母が高齢になり、さらに、祖父母が所有していた空き家を私が引き継ぐことになりました。その空き家は新幹線から離れた場所にあるため、通勤時間が往復 6 時間になってしまいました。引っ越しは、2023 年の 4 月頃でした。","妊活","私と妻は子供が大好きです。妻は以前保育士をしており、2023 年 3 月末に退職しました。その理由はいくつかありますが、妊娠を計画していることも一つです。2022 年から、妊活を始めるためにクリニックに通い始めました。当初は、タイミング療法を試しながら、並行して妻の検査と私の検査を行っていました。様々な事情があり、私の検査結果が 2023 年 5 月頃に判明しました。結果は機能不全で、つまり私の問題で子供ができないことがわかり、妻や親族にとても申し訳ない気持ちになってしまいました。それでもなんとか改善できるよう、お薬を飲んだり、ランニングを始めるなど、健康的な生活を心掛けるようになりました。","介護","80 歳を超える祖母が、昔から足が悪く、そろそろ足が動かなくなってしまいそうでした。足を動かさないため、運動しなくなり、血糖値がどんどんと上がってしまいました（HbA1c 9%）。足の手術をするのですが、血糖値が高いと手術ができないようです。足が悪いため、たまに湯船で溺れてしまうことが多く、その度に救助していましたが、とても危険な状態でした。生死に関わるため、祖父も父母も感情的になり、激しく喧嘩するようになりました。私は、祖父母、父母みんなが好きなんですが、喧嘩するたびに悲しい思いになりました。週に一度程度、祖父母の家へ向かい、家事を手伝ったり、祖父母の話（愚痴や不安）を聞いていました。この介護は、2023 年 5 月下旬頃から始まった出来事でした。","結果","これらの要素から、プライベートな精神的な負荷が増大し、英語の勉強によりプライベート時間が削られ、新幹線通勤による身体への負担が増え、私のワークライフバランスが崩れてしまいました。その結果、精神科を受診し、うつ病と診断されました。薬を処方されましたが、効果はあまり感じられませんでした。 私の感情は非常に不安定になっていました。些細なストレスに対して怒りと悲しみが極端に揺れ動き、すぐに怒ったり泣いたりしていました。しかし、仕事に持ち込まないように心がけていました。このままではいけないと感じ、業務委託という提案もしましたが、最終的には退職することに決めました。","反省","私は 2023 年の 4 月頃から、自分の感情が溢れそうだと感じていました。そのままでは問題が起こるかもしれないと思い始めていました。その後の出来事は、英語学習、新幹線通勤、妊活、介護と続きましたが、どうすればこれらの問題を解消できるのか、悩んでいる最中にさまざまな事が起こりました。ストレス解消のために過度な飲食や浪費をしてしまったこともありました。これらの行動は無意識に行っていたのかもしれません。ストレス解消は一時的な対策であり、問題が継続するとストレスは蓄積する一方です。今振り返ると、妻や家族に相談しにくい出来事だったため、問題を抱え込んでしまったのかもしれません。当時の私の認識は、「自分の感情が溢れそうだけど、今を乗り越えれば…」というものでした。 今思うと、","何かを抱え込んだときは何かを手放すことが大切だ","と感じています。例えば、英語学習では、他人に追いつこうとする思いを手放し、自分のペースでゆっくりと学ぶべきだったと思います。また、長時間の新幹線通勤が確定した時点で、出社を諦めるか別の拠点に移動するべきだったかもしれません。妊活で私の機能不全が判明し、落ち込んだときは、妻と向き合い、しっかりと休養を取るべきだったと思います。介護の問題では、祖母の手術が終わるまで、介護休暇を取るか働き方を見直すべきだったと思います。もちろん、これらはすべて後の祭りで、実際に過去に戻ってそれができたかどうかはわかりません。しかし、何かを抱え込んだときには何かを手放すという意識を持つことで、より良い行動をとることができたのではないかと思います。","現在","現在は無職です。職を離れたため、英語の勉強や通勤は必要ありません。また、妊活も一時休止しています。祖母の家には頻繁に訪れています。色々と手放した結果、感情の溢れ具合は回復し、心に余裕が生まれました。そのため、趣味に時間を使う気持ちが湧いてきました。筋トレや遠出、アプリ開発や執筆などをしています。しかし、時々「このままで良いのか」と自問自答することもあります。妻や親族に相談していますが、焦らなくて良いと助言してくれています。 私は空き家を譲り受けているため、家賃やローンの心配はありません。そのため、金銭的に困ることは今のところほとんどありません。しかし、私の収入が一切ないのもどうかと思い、アルバイトを始めようと考えていました。その時、Findy のカスタマーサクセスと話す機会があり、業務委託の提案を頂きました。そして、Findy Freelance を通じて、ありがたいことに案件のオファーを頂きました。働く時間は週に 2、3 回程度ですが、時給が高い（普通のアルバイトに比べて）ので、これは良い機会だと思いました。 明日から、少しずつ働き始める予定です。"],"t":"正社員を辞めました"},{"f":"src/routes/blog/contents/mfe/index.mdx","c":["［覚書］ Micro Frontends","Micro Fronends","(以下、MFE)をご存知でしょうか。説明をざっくりしますと、Microservicesの考え方をフロントエンドまで拡張した考え方です。Microservicesは、バックエンド側で適用される事例をよく耳にしますが、フロントエンドでの適用事例は、あまり聞いたことがありません。 tags: [\"Micro Frontends\"] cover_image: https://micro-frontends-japanese.org/resources/monolith-frontback-microservices.png socialMediaImage: https://micro-frontends-japanese.org/resources/monolith-frontback-microservices.png","import { Image } from \"~/components/image/image\";","Micro Frontends とは?🤔","皆さん、","(以下、MFE)をご存知でしょうか。説明をざっくりしますと、Microservices の考え方をフロントエンドまで拡張した考え方です。Microservices は、バックエンド側で適用される事例をよく耳にしますが、フロントエンドでの適用事例は、あまり聞いたことがありません。 従来、Web サービス開発ではモノリスな構成からスタートします。そこから、規模が拡大するにつれて様々な理由により、フロントエンドとバックエンドの分離、バックエンドの Microservices 化が行われます。 Microservices 化によって、Scalability、Agility、Independency、Availability の大幅な向上が期待できます。しかし、依然フロントエンドはモノリスなままです。そこで、次の画像のように、Microservices と同様にフロントエンドも縦(専門領域)に分割します。 ただし、全ての Web サービスを MFE にする必要はありません。先程の説明にもあった通り、規模が拡大した際に MFE を検討する必要があるため、小・中規模の Web サービスでは時期尚早です。また、次の画像にもある通り、静的ページ(Web サイト,Web ドキュメント)や動的ページ(Web アプリ)の両極端に位置する Web サービスは MFE の適用するのには不向きです(と書いています)。両方の要素が求められる Web サービスに MFE が役立ちます。MFE の適用される Web サービス事例では、EC サイトが挙げられます。 ※ MFE という言葉は、"," の記事より生まれたみたいです。 ※ ","にも記載されていますが、この考え方は Web サービスを対象としており、ネイティブアプリは対象としていません。","導入企業 👨‍💼👩‍💼","実績企業としては、IKEA、DAZN、Spotify などが挙げられます。他の例は、"," にリストアップしていますので、興味がある方はご覧ください。","メリット・デメリット 🔍","MFE を導入することによるメリット・デメリットについて、(プロダクション導入経験無しの私が偏見で)簡単に紹介します。Microservices のメリット・デメリットと似ていると思います。 私が思う最大のメリットは、","Agility","と思います。規模が中・大規模な Web サービスとなると、様々な業務ドメインが詰め込まれます。先程の MFE の例(EC サイト)でいうと、推薦(inspire)、検索(search)、商品(product)、注文(checkout)などにあたります。これらを 1 つのフロントエンドで構築すると、ドメイン設計を適切に分離できたとしても、","開発者の業務ドメイン知識が追いつかず、開発スピードが低下してしまいます","。結果、特定の開発者の属人化が加速し、ボトルネックとなります。 そこで、それぞれ","業務ドメインを分割することで、開発者はそこだけにフォーカスできます。結果、開発スピードは維持できるはずです","。 私が思う最大のデメリットは、","Independency の難しさ","だと思います。例えば、UI/UX の指針となるデザインシステムが Web サービスにあったとして、それをすべてのフロントエンドへ適用しなければいけません。そのため、全体を通した","一貫性のある UI/UX であるかどうか","の品質担保が難しいです。 他には、あるチームのビルドツールを改善したとしても、他のチームではその恩恵を受けれなかったり、アプリケーション設計における全体共通(アクセス履歴、イベント管理、状態管理など)部分を、どうするか考える必要があります。 こちら "," にも簡単にメリット・デメリットを書いていますので、気になる方はご覧ください。","統合パターン 🔮","MFE では、各フロントエンドのフラグメント(HTML)を、どのタイミングで統合するのかが重要です。今回はその統合パターンをざっくり紹介します。 例えば、次の MFE の例で言えば、Team-Product、Team-Checkout、Team-Inspire の 3 つのフロントエンドフラグメント(HTML)があります。これらをどのタイミングで統合するのかがポイントです。 詳しくは、"," をご覧ください。","ビルド時統合パターン","ビルド時統合とは、Web サービスを Publish する前の Build の段階で統合するパターンです。このパターンは、","がよく使われます。 フラグメントを Packaging し、Packaging したライブラリを import させて build(統合)します。あとは、build した静的コンテンツを Publish させるだけになります。","サーバーサイド統合パターン","サーバーサイド統合とは、Web サーバー側の HTML 構築段階で統合するパターンです。このパターンは、SSI や ESI、Podium、Tailor、Ara-Framework などが使われます。 フラグメントを提供するサーバーを準備し、それらからフラグメント情報を収集し、全体のページ HTML を構築します。それを SSR としてユーザーへ提供します。 サーバーサイドのサンプルコードは、次にまとめています。","また、サーバーサイドというより Edge での統合パターンを下記リンクで紹介しています。","※ リッチなインタラクション UI を表現したいなら、サーバーサイドとクライアントの Hydration をする必要があります。","クライアントサイド統合パターン","クライアントサイド統合とは、ブラウザ側レンダリングの段階で統合するパターンです。このパターンは、iframe や WebComponents などが使われます。 iframe を使ったページ(フラグメント)埋め込み、全体のページ HTML を統合させたり、WebComponents のようにカスタムエレメントを定義した HTML タグでページを構成したりします。","終わりに 👨‍💻👩‍💻","MFE のアプローチを実際に導入した企業は、国内だとまだ比較的少なく、どういった場面で役立つのかあまり明確ではありません。また、書籍や知見も多くはないため、未知な領域と思います。 ただ、依然フロントエンドがモノリスな、中・大規模な Web サービスを運用するならば、特に進化が激しいフロントエンド界隈の中で、サービス提供の速度、品質を維持するのは難しいと思います。フロントエンドも Microservices 化する場面が訪れるかもしれません。そんなときに、この記事を思い出して頂ければ幸いです。 ※ 独り言ですが、MFE の構築アプローチとして、Edge Worker + Web Components の組み合わせが最近好みです。","関連リンク 🔗","私が書いた MFE 関連の記事です。もしよければご覧ください。"],"t":"［覚書］ Micro Frontends"},{"f":"src/routes/blog/contents/microfrontends/index.mdx","c":["Micro Frontends を学んだすべて","import { Image } from \"~/components/image/image\"; Micro Frontends という Web フロントエンドアーキテクチャがあります。 このアーキテクチャを知るために、書籍を読み、簡単なサンプル Web アプリを開発しました。 そこから学んだことをすべて議事録として残したいと思います。","モノリシックな Web アプリケーション","マイクロサービスという考え方の多くは、バックエンドへ適用されることが一般的です。 一方で、フロントエンドは依然モノリシックなままの状態です。 EC サイトのような Web アプリケーションでは、様々な専門知識(商品、注文、検索など)を必要とし、フロントエンド開発者の守備範囲がとても広くなってしまいます。 開発者には限界があり、いつしか トラブルシューティングに追われる日々 になってしまいます。 そこで、Micro Frontends というアーキテクチャの出番です。","Micro Frontends とは","それはマイクロサービスの考え方をフロントエンドに拡張したものです。","※ ","要は、バックエンドだけでなく、バックエンドからフロントエンドまでをマイクロサービス化することです。 さらに詳しく知りたい方は、次のページをご参考下さい。とてもわかりやすいです。 https://micro-frontends-japanese.org/ また、次の書籍を読むと、 https://www.manning.com/books/micro-frontends-in-action","Amazon does not talk a lot about its internal development structure. However, there are reports that  the teams who run its e-commerce site have been working like this  for a long time. ...","Micro frontends are indeed quite popular in the e-commerce  sector.  In 2012  the Otto Group, a Germany based mail order company and one of the world’s largest e-commerce players started to split up its monolith. ...","The Swedish furniture company  IKEA and Zalando , one of Europes biggest fashion retailers, moved to this model. ...","But micro frontends are also used in other industries.  Spotify  organizes itself in autonomous end-to-end teams they call Squads. ...","Excerpt From: Michael Geers. “Micro Frontends in Action MEAP V03.” iBooks. という内容があります。 IKEA や Zalando といった EC サイトが Micro Frontends を採用する ケースが多く、公には言っていませんが、Amazon も Micro Frontends で取り組んでいるようです。 EC サイトだけでなく、Spotify のようなサービスにも適用されるケースがあります。","Micro Frontends の良さ","私が思う Micro Frontends から得られる最大の恩恵は、\" 局所化 \" だと思います。 フロントエンドをサービス毎(商品、注文、検索など)に分割することで","サービスの 専門性 向上","ex. 対象サービスのフロントエンドだけに集中できる","サービスの 開発速度 向上","ex. 対象サービスのソースコードだけ読めば良い ex. 対象サービスだけにライブラリアップデートすれば良い ex. フレームワークの切り替えは対象サービスだけすれば良い","少し薄っぺらいかも知れませんが、↑ のように実感しています。 ※ Micro Frontends は Web ベースのアーキテクチャになります。","Micro Frontends の難しさ","ここは、まだちゃんと掘り下げれていませんが、次のようなものがあります。","特定チームが改善しても、チーム全体が改善しない","ex. あるチームが webpack のビルド時間短縮に成功しても、他のチームは影響を受けない ex. 全てのチームが採用しているライブラリのセキュリティパッチは、それぞれのチームが更新しなければならない","チーム全体へ共有する仕組みを考える必要がある","ex. デザインシステム、パフォーマンス、ナレッジ","エッジな技術スタック採用は、チームメンバー移動を困難にする","ex. パラダイムシフトが発生してしまう 技術スタック","Micro Frontends の作る上で考えること","フロントエンドをマイクロサービス化するということは、各サービスで HTML/CSS/JS を作ることになります。 それらの サービスを統合するサービス が重要になってきます。 大きく分けて 2 つの統合パターンがあります。 | 種類 | 解決手段 | メリット | デメリット | | ---------------------- | ----------------------------------- | -------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | | サーバーサイド統合 | SSI, ESI, Tailor, Podium | ・SEO 対策上良い ・ユーザーのネットワークレイテンシーが少ない ・初回ロードパフォーマンスが優れている | ・インタラクションアプローチが不得意 | | クライアントサイド統合 |  Ajax, Iframe,  Web Components | ・Web 標準 ・シャドウ DOM による堅牢な作り | ・サポートブラウザに依存する ・クライアント側の JavaScript が有効であること | また、これら 2 つの選択基準は次のようになります。 | 種類 | 選択基準 | | ---------------------- | --------------------------------------------------------------------------------------------------------------------------- | | サーバーサイド統合 | 良好な読み込みパフォーマンスと検索エンジンのランキングがプロジェクトの優先事項であること | | クライアントサイド統合 | さまざまなチームのユーザーインターフェイスを 1 つの画面に統合する必要があるインタラクティブなアプリケーションを構築すること | 今回、私はサーバーサイド統合(Podium)を選択しました。 ただ、インタラクティブなアプローチも必要だったため、 Hydration を使いました。","Hydration refers to the client-side process during which Vue takes over the static HTML sent by the server and turns it into dynamic DOM that can react to client-side data changes.","Hydration は、サーバーサイドでレンダリングした静的 HTML に、クライアントサイドの動的レンダリングができるようにするようなものです。 ※ クライアントサイド統合(Web Components)でも良かったのですが、私都合により却下となりました。","Micro Frontends サンプル Web アプリ","apple, banana, orange という商品を検索するだけのサンプル Web アプリを作りました。 概要図はこちらです。 サンプルコードは、ここに置いています。 https://github.com/silverbirder/micro-frontends-sample-code","サービス","| サービス | 役割 | JS フレームワーク | | ------------ | -------------------------- | ------------------------------ | | team-search | 商品を検索するサービス | Vue.js | | team-product | 商品を表示するサービス | React.js | | team-page | サービスを統合するサービス | フレームワーク未使用 (Node.js) |","仕組み","Podium というライブラリを採用しました。 https://github.com/podium-lib/ これは、フロントエンドのサービスを簡単に統合できるようなライブラリになっています。 Podium には大きく分けて 3 つの機能があります。","ページフラグメントサーバーを構築する ex. team-search, team-product","Podlet を集めて、ページ全体のレイアウトを構築する ex. team-page","ブラウザベースの機能を提供する MessageBus による Podlet 同士のコミュニケーション ex. team-search, team-product で publish/subscribe","@podium/podlet","Podlet には、manifest.json と呼ばれる値を返却することが必須になっています。 menifest.json には、サービスのエンドポイントや、Asset(JS や CSS)のパスが明記されています。 team-search では","というレスポンス結果になります。","@podium/layout","Layout では、Podlet の manifest.json の定義に従って fetch することになります。 team-page では","のように Podlet を使って、ページ全体を構築します。このようにサーバーサイドで統合しています(SSR)。 しかし、インタラクティブなアクションも必要なため、Podlet から Hydrate するための js を読み込んでいます。 また、team-search の検索結果(x-product-items)を team-product へ渡しているため、商品の検索結果を含めて SSR が実現できます。","@podium/browser","サーバーサイドは、podium/podlet, podium/layout で連携できます。 クライアントサイドは、この @podium/browser の MessageBus で連携できます。 今回のサンプル Web アプリでは、次のようなユースケースに使用しています。","ユーザーが検索ボックスにキーワードを入力する team-search がキーワードから商品を検索する team-search が 2 の結果を publish する team-product が 3 を subscribe し、商品を更新する","このようにすることで、画面更新ではなく部分更新ができました。 インタラクティブな操作も実現可能です。","状態管理, ルーティング","ここは、まだきちんと作っていませんが、次のようなコンセプトで設計するのが良いと思います。","状態管理","各サービスが状態管理する。状態は共有しない。 統合サービスが共通的な状態を管理する。","ルーティング","各サービスが query を設定する。 統合サービスが URL パスを管理する。","その他","各サービスは、fly.io という PaaS へデプロイしています。 https://fly.io/ CDN で SSR が実行できる  Edge Worker を使用しています。 これにより、SSR 結果をキャッシュし、高速にレスポンスを返却できます。 ただ、サンプル Web アプリでは、全くその力を引き出せていないです... ※ 参考記事 https://mizchi.hatenablog.com/entry/2019/02/21/235403","サンプル Web アプリで分かったこと","SSR + CSR (Hydration) が実現可能","サーバーサイド統合であっても、CSR は実現可能 です。 ただし、Hydration には  パフォーマンス面に難有り なため、このあたりは課題として残ります。 また、CSR するための bundle した javascript の size には注意が必要です。 例えば、次のリポジトリにある \"shared_vendor_webpack_dll\" のように、vendor ファイルを共有することで、 javascript の size を減らすといった手段があります。 https://github.com/naltatis/micro-frontends-in-action-code また、次のリポジトリにある zalando tailor は、script load を streaming することで、 全体の script load 完了時間を短縮するツールもあります。 https://github.com/zalando/tailor","サービス内で技術スタックを選択できる","マイクロサービスでは、よくあるメリットとして挙げられるものです。 フロントエンドでも、同様に技術スタックを自由に選択できます。 今回では、React.js と Vue.js を使用しています。 これを Riot.js や Svelte.js にも切り替えることも可能です。 フロントエンド界隈では、JS フレームワークの変化が激しい ので、 このメリットは大切だと思います。 ただし、Podium の manifest.json を返却しなければなりません。 今の所、Podium に対応しているのは Express のみなので、Express を使用する フレームワークのみとなります。","サービス毎のフロントエンドに集中できる","検索サービスだと、検索に特化したフロントエンドのみに集中することができます。 商品サービスだと、商品の表示内容のみに集中することができます。 ただ、どうしても他サービスと連携する要件が出てきます。 これは、マイクロサービスとしての難しさだと思います。 例えば、各サービスがどのタイミングでイベント登録するのかを考える必要があります。","最後に","EC サイトのようなアプリケーションでは『商品を探しやすくする』『買いたくなるような商品を表示する』 『商品を簡単に購入できる』などフロントエンドでやるべきことが多くあります。 そういうサービスにおけるフロントエンドがモノリシックであれば、 統一性が欠けてしまったり、知らぬ間にバグを埋め込んでしまうケースが発生してしまいます。 Micro Frontends は、このような 複雑化するフロントエンドにメスを入れる良いアーキテクチャ だと思います。 ただし、バックエンドにおけるマイクロサービス化による課題があるように、フロントエンドにおける マイクロサービス化にも課題はあるはずです。 日本では、Micro Frontends の導入実績が少なく、まだまだ発展途上だと思います。 この記事が、どこかのサービスへの参考になればと思います。 最後まで読んで頂き、ありがとうございました。","参考リンク","https://github.com/ChristianUlbrich/awesome-microfrontends"],"t":"Micro Frontends を学んだすべて"},{"f":"src/routes/blog/contents/mix_leap_study_45/index.mdx","c":["【増枠】Mix Leap Study #45 - Google I/O、WWDCまとめて報告会！ 2019年6月15日参加レポート","import { Image } from \"~/components/image/image\"; 今回は、ヤフー株式会社主催の下記セミナーに参加してきました。 Google/Apple どちらも大好きで、けど海外カンファレンスにいけなかった私にとって、今回の報告会は 新鮮な内容 ばかりでした。 その内容を記事に書こうと思います。 https://yahoo-osaka.connpass.com/event/132601/ hashtag はこちら ","Google I/O とは？","Google が主催する、開発者向けイベントです。 Google I/O では、WEB や Google が出しているガジェットなど様々な技術情報についてセッションが行われています。 https://events.google.com/io/ ※ ","WWDC（Worldwide Developers Conference）とは？","Apple が毎年開発している、開発者向けイベントです。 WWDC では、apple の新製品の紹介や新しい技術についての発表が行われています。","※ ","ヤフーでは、google I/O と WWDC の両方に約 30 名の社員が参加したそうです。 すごい数ですね。","Google I/O の概要と MLKit のアップデート 加藤 貴晴さん","Google I/O の概要","Google I/O が始まったのは 2008 年からで、毎年開催しているそうです。 今年は 2019 年なので、11 回目になります。 今回は全部で 164 セッションありました。 その内の TOP3 が下記のとおりでした。","Android 64 Web 39 ML/AI 32","Web 好きの私としては TOP2 というのが悔しいですね。(笑) ML/AL が 3 番目とは驚きです。","Deplex on the web","https://www.gizmodo.jp/2019/05/190305.html ウェブベースでも使える GoogleAssistant のことで、レンタカーや映画の予約ができるみたいです。 これのすごいところは、レンタカーを予約するまでのステップを 全て自動入力 してくれるみたいです。 そこまで便利になったのかと驚きました。 ちなみに、日本にはまだ対応していません。","WebAuthn","パスワードレスな生体認証のことを指すそうです。 こちらについてのセッションが下記のようです。 https://developers.google.com/web/updates/2018/05/webauthn ひとまず知ることができてよかったです。","ML","ML Kit の発表があったそうです。 https://developers.google.com/ml-kit/ その中でも、翻訳 API について報告会では熱く話されていました。","ML On-Device Translate API","デバイス上で翻訳することができるようになります。 そのため、外部とのやり取りができない環境でも翻訳できます。 つまり、オフラインでも動作します。 また、59 言語に対応しているというすごい数です。","一部無料で使えるとのことで、こういうスタンスは本当に大好きです。 https://firebase.google.com/docs/ml-kit/android/translate-text ※ 翻訳する際は中間に英語を挟むような作りになっているみたいです。","AutoML Vision Edge","こちらも Edge というデバイス、つまりは Android 端末上で動作するカスタム機械学習モデルを作成できるサービスです。 ここで注目したいのは、またしてもデバイス上(On-Device)で動作する点です。 Google ではこのデバイス上で完結する方針を、これからも進めていくのでしょうか。 On-Device だと、どうしてもデータをデバイス上に保存する必要があります。 そのため、保存すべきデータをいかに軽量にするかという問題があります。 オフライン環境でも動作できるようになれば、災害時や緊急事態には役立ちますよね。 Web 好きなら知っていると思いますが、PWA という技術があります。 こちらにも OfflineMode という機能があり、こういった On-Device の先駆けとなっていたのでしょうか。","Google アシスタントの他プラットフォームへの拡張方法の紹介 一円 真治さん","いろいろとお話されていたのですが、下記の内容が一番衝撃でした。 https://japanese.engadget.com/2019/05/08/google-web-duplex/","Google はまったくあたらしい音声認識と言語理解モデルを開発し、100GB 必要だった学習モデルを 0.5GB 以下まで削減したとしています。これにより、学習モデルをスマートフォン内部に格納できるようになり、AI 機能の動作にネットワーク接続不要に。この結果、ほぼ遅延なくデバイス上で音声認識が行えるようになるとのことです。","またしてもデバイス上ですが、GoogleAssistant を動かすのにモデル作成が必要みたいです。 それにかかる容量が 100GB も必要だったものを 0.5GB まで削減したという衝撃的な発表があります。 また、AI 機能の動作にネットワーク接続が不要とのことなので、必要なデータをダウンロードできていれば、オフライン環境でも動作できます。","What’s WWDC? / Swift UI ’n Siri Recap 田中 達也さん","SwiftUI について","WWDC で発表された SwiftUI は、WWDC を参加していた人みんながめちゃくちゃ盛り上がったそうです。 Swift であんまり開発したことがないので、ほぼ想像で話しますが、 従来の Swift による開発は、ソースコードをビルドして、端末にビルド後のデータを移動させて動作確認する必要がありました。 そこを、SwiftUI はわざわざ端末にデータ移動せず、xcode 上で preview できるという開発者にとって、とてもハッピーな機能がついたようです。","SwiftUI ってどうやって使うの？","https://twitter.com/silverbirder/status/1143475061673717760?s=20 この件について登壇者さんに質問してみました。そのとおりとのことです。 swiftUI を手軽に動かしたい場合は、playground でも試せるそうなので、近い内にやってみようかなと思います。 https://qiita.com/shtnkgm/items/387132cd9633a59e7390","ショートカットアプリ","標準で iphone にインストールされるようになったアプリで、正直あんまり使った覚えはありません。 他アプリとの連携が用意になったらしいので、アプリ開発の幅が広がりますね。 （すみません、SwiftUI のことばかり考えていました（笑））","AR・ML・その他 Apple プラットフォームのアップデート 林 和弘さん","内容的には動画があったほうがわかりやすいのですが、 都合上見せれないものばかりだったため、なんだかモヤっとした内容でした。（笑）","AR について","ARKit→ARKit2→ARKit3 の順で進化してきたのですが、動画がなく、ふ〜んってなってしまいました...。","Authentication","「sign in with apple」という内容に私は惹かれました。 Apple の ID で認証ができるようになります。 特に、JS ライブラリや、REST API の提供もあるそうです。 JS ライブラリ https://developer.apple.com/documentation/signinwithapplejs REST API https://developer.apple.com/documentation/signinwithapplerestapi 良いっすね〜！これで認証の種類が増えました！","最後に","Google I/O や WWDC には参加したいという気持ちがあるのですが、 やはり英語のちからがまだまだ自信がありません。 徐々に聞き取れるように勉強していきます。 https://note.mu/silverbirder/m/mcad08e0f384b 今回の報告会で何度も耳にした「デバイス上で動作、オフライン環境」は、 今後、Google では力を入れていきたいのかなと思いました。 いま私ができることは、無料で使える ML On-Device Translate API を試すぐらいかなと思います。 あとは、今と同じで継続して技術情報に対して、アンテナを貼り続けるぐらいでしょうか。 ヤフーの社員さんたちは、こういった新技術に対してキャッチアップする姿勢が積極的で良いなと思います。 私も負けないように頑張りたいと思います。"],"t":"【増枠】Mix Leap Study #45 - Google I/O、WWDCまとめて報告会！ 2019年6月15日参加レポート"},{"f":"src/routes/blog/contents/my_feedback_after_taking_casual_interviews_with_10_companies_in_2_months/index.mdx","c":["カジュアル面談を10社受けた感想","が、Twitter 上で話題ですね。 その投稿が続いている裏で、私は、転職活動をしていました。 その活動内容や感想について、かんたんにまとめておきたいと思います。","自己紹介","私は、関西に住む 20 代の Web エンジニアです。 2020 年ぐらいから、転職はふんわり考えていて、訳あって 2021 年 8 月末から 10 月末まで本格的に活動していました。","転職活動","スカウト式の転職サイトを使い、カジュアル面談をして頂きました。 転職サイトは、次の 2 つを使いました。","どちらも、企業側から転職希望者へスカウトする転職サイトになります。 転職希望者は、事前に職務経歴書や転職希望条件などを転職サイトに登録しておきます。 当初、(CM でよく目にしていた)ビズリーチで転職先を探していたのですが、次の理由で使うのを諦めました。","登録したい情報に、私の成果物をリンクできない","必要に応じて PDF 化","そこで、Github と連携が可能な Findy というサービスに乗り換えました。 最終的に、ビズリーチで 1 社、Findy で 9 社のカジュアル面談をして頂きました。 ※ ちなみに、SNS の DM で連絡頂く人もいましたが、胡散臭い感じが強くあったので無視しました。","Findy での進め方","次の流れで、カジュアル面談まで進めました。","企業側の担当者が、転職希望者へ\"いいね\"を送る \"いいね\"が、転職希望者へ通知される(メール, Line) 転職希望者が、求人内容を確認する (",") 転職希望者が、企業へ\"いいかも\"を送る 企業側の担当者と転職希望者が、メッセージのやりとりをする 企業側の担当者が、カジュアル面談を設定する","Findy では、","に書いてある通りの企業から、\"いいね\"を頂けることがあります。","エンジニア・プログラマーが最新技術を試せる職場、ホワイト企業、日系大手、外資系、急成長ベンチャー企業など、様々なおすすめテック企業を厳選して掲載しています。ご登録いただくと、これらの企業からスキル偏差値を元にオファーが届くようになります。","メルカリやサイボウズ、(アンジャッシュ児島さんの CM で有名な STORES の)Hey などの企業から\"いいね\"を頂きました。","カジュアル面談での進め方","この世の中、リモートでのカジュアル面談が当たり前になりました。 そのため、平日の勤務後(18 時以降)に、Zoom や Teams を使ってカジュアル面談して頂くことになります。 カジュアル面談の流れは、次のとおりです。","自己紹介 採用ピッチを使った会社紹介 (適宜質問)","ビジネスの話(プロダクト、サービス) 会社の話(ミッション、ビジョン、バリュー、カルチャー) 現場の話(開発、技術、体制)","次のアクション(別メンバーとの MTG、選考進める、保留)","カジュアル面談に来て頂く人は、次の構成が多かったです。","グループリーダークラスの人 人事担当者 グループリーダークラスの人 + 人事担当者","中には、CTO とカジュアル面談して頂いたことがあったのですが、とても緊張して情報交換が一方通行(CTO→ 私)となってしまいました...。","カジュアル面談で思ったこと","カジュアル面談前のメッセージ内容で、モチベーションが変わる","カジュアル面談を進める前に、企業側の担当者から、(ありがたいことに)メッセージを頂けます。 そのメッセージの中には、Github やポートフォリオの感想を書いて頂けることがあります。 そういうメッセージは、『私を見て頂いているんだな』と思い、カジュアル面談への意思表示を強く示すことが多いです。","情報交換を円滑に進めるため、基本的なコミュニケーションを心がけて","まず、次のことは絶対はやめてほしいと思いました。","話す内容が聞こえない","話し方が早口 声が聞こえにくい・小さい","相手の表情が読めない","(Zoom や Teams で)顔を出さない 表情が暗い・笑わない","1 つ目は、情報交換ができていないので、絶対にやめてほしいところです。 2 つ目は、できればやめてほしいのですが、会話するときは、営業スマイルでも良いので、明るい人と話したいです。","カジュアル面談の参加者は、転職希望者と近い距離にある人が良い","カジュアル面談に来て頂く方は、転職希望者と一緒に働くかもしれないぐらいの役職にある人が良いなと思いました。 例えば、私の場合は、グループやチームのリーダーが良いと思いました。 そのような人とカジュアル面談すると、働く現場の話や、課題感についての認識を持ちやすくなるからです。","コロナ禍からフルリモート可になった企業は、避けた","カジュアル面談時に、多くの企業が、フルリモート可能になったという話を聞きました。 関西在住だと、関東の企業で働ける機会が増えて、嬉しい反面、コロナ収束後に、フルリモートがなくなるのではないかと危惧しました。 そのため、そういう企業は避けるようにしました。","求人に悩んだら、オープンポジションを提案する","求人内容が、私に適切なのかどうか分からないときは、\"オープンポジション\"という選考を提案することにしました。 私の場合は、Web フロントエンドの求人に応募することが多いのですが、それとは別の求人も良さそうと悩むことがあります。 そういったときに、\"オープンポジション\"で選考をお願いできないか提案しました。 どちらの求人枠の方が、私の強みを活かせられるのか、企業側に考えて貰いたいのです。","最後に","転職活動らしい活動は、今回がはじめてでした。 転職活動を進めていくと、色々と考えさせられて、良い機会だなと思いました。","企業選びは、何を基準として考えるか","ビジネスへの共感は前提 給与、働き方、技術、etc","私は、どういった人物なのか","強み・弱み 経験・性格 できないこと・できること・やりたいこと","どういったキャリアを積み上げたいのか","数年先は、どういう人物になりたいか マイルストーンをどう刻むのか","良い機会ですが、転職活動すると、色々クタクタになるので、またしたいとは思いませんでした...。"],"t":"カジュアル面談を10社受けた感想"},{"f":"src/routes/blog/contents/nfc_automation_trigger_sesame_api/index.mdx","c":["NFC Automation Trigger + SESAME API による自動解錠と考察","import { Image } from \"~/components/image/image\";","まえおき","これから iOS を前提として話を進めていきますが、話の主題としては iOS かどうかはあまり関係ありません。","NFC Automation Trigger","により NFC Automation Trigger が使えるようになりました。NFC とは、Wiki によると次のようなものです。","狭義には Near field communication（NFC）の訳語。NFC は RFID（Radio Frequency IDentification）と呼ばれる無線通信による個体識別の技術の一種で、近距離の無線通信技術を統一化した世界共通の規格である。IC チップを内蔵した NFC タグを NFC のリーダ・ライタ機能を有する機器で読み取ったり書き込んだりする。","※ https://ja.wikipedia.org/wiki/近距離無線通信 有名なものとしては、","でしょうか。AppleWatch がリーダ機として扱われます。 今回のリリースである NFC Automation Trigger は、この NFC を読み込むと自動的に特定のアクションを Trigger することができます。 例えば、特定の NFC タグを iPhone が読み取ると、3 分のタイマーが起動する！とか。ま、表題の件を用意したんですけどね！（笑）","とりあえずこんなの作ったよ！","登録している NFC タグを iPhone が読み取ると、扉に設置しているスマートロックアイテム SESAME の","を Request して扉の鍵が解錠されます。 ↓ 白色の NFC タグにスマホを近づけると...? NFC Automation Trigger が作動！解錠！ https://twitter.com/silverbirder/status/1187016726363299840 作成方法は、とっても","かんたん","です。iPhone 標準アプリ shortcut と、","があれば誰でも作れます。もちろん、**","**が必要ですけどね（笑)。 shortcut で、次の準備をしました。","を使って","を設定する。","を作成し、用意した NFC タグをスキャンする。 2 のオートメーションを 1 のショートカットと紐付ける。","これの良いところは、次の２点でしょうか。","標準アプリだけで完結 NFC タグ自体には何も加工しないため安全","※ SESAME には、扉に近づいたら解錠する**","や、スマホをノックするだけで解錠する","**というものがあります。本来は、これを使いたかったのですが精度が低いため実用性に欠けると思っています。","考察","この NFC Automation Trigger は、アイデア次第でいくらでも、便利なことができます。 今回の目的は 「","個人利用","で、","ライフハック","できるものを作りたい」というものです。 ただ正直、iPhone をわざわざ NFC に近づける動作は","面倒","だと思います。 私の部屋には","として","があり、スマートリモコンの","や、物理ボタン自動化の","、コンセントのスマート化","などが GoogleHome と連携しています。 手で操作するよりも、声で操作する方が、数ステップですが","遥かに効率的","と感じます。 つまり、VUI で管理できている空間に関しては、NFC の Trigger はあまり役立ちはしないのかなと思っています。逆に、","VUI の管理外の空間","、私の部屋でいうと、トイレとか洗面台、玄関には NFC が役立つかもしれません。また、","声を発するのが躊躇われる環境","においても、NFC の方が役立つと思っています。 外の環境では、どうでしょうか。NFC を貼れる場所なので、限定はされます。 自宅の郵便ポスト、自転車・車、衣類、カバン、傘、学校や職場のマイ机・椅子とかでしょうか。 ","は、シールのように柔らかいので、コップやボールにも貼れると思います。","が一時流行っていましたが、あれと似たようなこともできます。例えば、「トイレットペーパの入れ物に NFC タグを貼っておいて、なくなりそうになったら NFC タグを読み込んで注文する」みたいな。もちろん、注文する処理は、自前で組む必要がありますよ。アマゾン定期注文には向いていないものには、良いかもですね。 さらに、shortcut では","変数を注入できる","ため、例えば、現在地を NFC トリガーに注入することで、 「現在地から自宅までのタクシーを予約する」NFC トリガーもできちゃいます。 個人利用という括りであっても、NFC Automation Trigger の使い道はとても多いと思います。 商用利用と考えると、更にあると思いますが、省略します。","さいごに","NFC Automation Trigger という機能は、珍しくありません。 しかし、とても使いやすくカスタマイズ性が高いため、アイデア次第でいくらでも化けれます。 一攫千金？を目指すのも良いですが、やっぱり","をつきつめたいなと思う私でした。"],"t":"NFC Automation Trigger + SESAME API による自動解錠と考察"},{"f":"src/routes/blog/contents/obsidian-git-muti-device/index.mdx","c":["ObsidianでiPhoneからGit Commitする","import { Image } from \"~/components/image/image\"; WikiWikiWeb というコンセプトが好きで、そのコンセプトが含まれている Obsidian や Scrapbox が好きです。Obsidian には、"," という Git 連携のプラグインがあります。こちらには、デスクトップだけでなく、モバイルからでも Git Commit できます。 そこで、私が持ってる iPhone を使って、Obsidian で Git Commit する手順を紹介します。","手順","iPhone から https://obsidian.md/ にアクセスし、アプリをダウンロード","アプリを開いて、Create new vault をタップ","Vault name に、適当な名前を入力 (後で変更可能)し、Create をタップ","左上のサイドバーアイコン → 設定アイコン → コミュニティプラグイン → コミュニティプラグインを有効化 の順でタップ","コミュニティプラグインを閲覧 → Git を入力 → Obsidian Git をタップし、インストール → 有効化をタップ","オプションをタップ","Github のアカウント、"," (repo の権限があれば良い) を入力. バツボタンをタップし、4 の画面に戻る","下へスクロールして、コマンドパレットを表示 → Clone と入力し、表示された選択肢をタップ","Clone したいリポジトリ URL を入力","Vault Root をタップ","NO をタップ (.obsidian フォルダが repository にあるなら YES)","Clone が成功すると、リポジトリのファイルが閲覧できる","Obisidian Git の Advanced に、Author name と Authr email を入力し、バツをタップ","ファイルを適当に変更する","下へスクロールし、コマンドパレットを開く → Git Open source と入力し、表示された選択肢をタップ","+ボタンで Stage、チェックボタンで Commit、アップロードボタンで Push できる","困ったこと","スマホから、ブランチを作成できるが Push できない スマホから、ローカルには存在しないリモートブランチを(pull しても)Switch できない","基本的には、スマホからの操作は、main ブランチでコミットプッシュするしかできなさそうです。","終わりに","これから、スマホからいろいろメモを書いていこうかなと思います！"],"t":"ObsidianでiPhoneからGit Commitする"},{"f":"src/routes/blog/contents/open_the_techbook7_first_experience/index.mdx","c":["技術書典7で初執筆した経験をすべて公開","import { Image } from \"~/components/image/image\"; 技術書典 7 で初執筆しました。","記事の目的","執筆でどういったことをしたのかの備忘録 執筆を考えている人 の助けになりたい","実際に販売する本は ↓ のものです。 https://techbookfest.org/event/tbf07/circle/5117648689954816","きっかけ","大学時代の友人である castaneai くんが技術書典 6 で初執筆しました。 https://castaneai.hatenablog.com/entry/2019/04/24/093514 castaneai くんの話を聞いていると、得られるメリット(実績、交流)が大きいことと、 製本までのフローがそこまで難しくないことを知りました。 そこから、私も参加しようと思えるようになりました。 castaneai くんは、今回の技術書典 7 も参加するみたいです。興味がある方は是非お立ち寄りください。 https://techbookfest.org/event/tbf07/circle/5182251830607872","何をするのか","大きく分けて３つのステップになります。","文章作成 製本 販売準備","それぞれ説明していきます。","1. 文章作成","本は何よりも文章が必要です。 ただ文章を書くだけでは、本になりません。 書籍化するためのツールを使うと効率よく進みます。","1.1. 書籍化ツール","文章を書き、本っぽい見た目にする必要があります。 Re:VIEW Starter というツールを使うと、学習コストゼロで、良い感じの本が出来上がります。 https://kauplan.org/reviewstarter/ 次のコマンド１つで本の PDF が作られます。","良い感じの本","の PDF が作成されました、最高です。","1.2. 他ツール","他にも次のようなツールがあります。","テキストの表記ゆれを防止","GitOps 的な執筆が可能","テキスト更新後の自動 Preview","執筆当初は、これらのツールを調査していました。 しかし、一人で 100 ページ未満の規模の本を書くなら、特に必要ないかなと思ったので導入しませんでした。 実際、なくても困りませんでした。","1.3. レビュー","文章を書いて終わる訳ではないです。 文章の構成や表現が適切に伝わっているか確認する必要があります。 読んでもらいたい対象読書に近い人を探して、レビューしてもらいます。","1.3.1. 1st レビュー","文章をざっくりと作り終えた時点でレビューしてもらいます。 イメージとしては、各章とそれぞれの第一節ぐらいが書き終えている感じです。 各章の構成がおかしくないかをレビューしてもらいます。構成がおかしいと、読者は困惑してしまいます。 後々になって構成を変更すると、","後戻りコスト","が高く付きます。 ※ 実際は時間の都合上していません。","1.3.2. 2nd レビュー","各章の文章を","とりあえず","書き終えた段階でレビューしてもらいます。 これも、1st レビューと同様の目的です。 2st レビューでは、もう少し細かいレベルで章(節)構成をレビューしてもらいます。","1.3.3. 3rd レビュー","コンテンツの構成に問題なければ、ようやく文章の中身をレビューしてもらいます。 例えば、次のようなものを見てくれました。","口調の統一 (です、ます） 言葉の統一 (本、書籍） 主語の明確化 文章を短くする 内容の誤り修正 誤字脱字 図や表の挿入","また、","GoogleDrive 上で PDF をレビュー","するのが便利です。 直接文章にコメントできるので、オススメです。 https://twitter.com/silverbirder/status/1167314554205786112","1.5. 本のタイトル","本を買ってもらうためには、本のタイトルは重要です。 本の内容を推測しやすく、かつ、注目してもらえるタイトルにしようと考えました。 私は、特定の技術の入門書を書いたので、「特定の技術 + 入門」という組み合わせにしようと思いました。 結果、「はじめての WebComponents 入門」というタイトルにしました。","1.6. イラストの作成","本には、文章だけでなくイラストが必要になります。 例えば、次のようなイラストが必要になります。","表表紙 裏表紙 背表紙 文章中に挿入するイラスト","また、少し内容が異なりますが、次のようなイラストも必要です。","サークル配置図 案内 サークルカット サークルカット（グレートーン）","表紙用のテンプレートがありますので、それを使います。 http://www.nikko-pc.com/offset/template/tonbo.html","背表紙の幅はページ数によって変化します","。 私は、70 ページほど予定していたので 4mm 幅で背表紙を描きました。 幅計算は、テンプレート内に詳細が記載されていますので、ご参考下さい。 （日光企画のお姉さんに指摘頂きました）","1.6.1. 初イラスト","私は Photoshop や Illustrator を使ったことがありません。 まずは、環境準備からです。","iPad Magic Pencil","これらを購入しました。 iPad と Magic Pencil を使うと、紙に書いている感覚で、イラストを書けるようになります。 特に良かったのは「手の小指側の面が iPad に接しても無視される」ので、 手の小指側の面を iPad にひっつけながらイラストがかけます。 iPad, Magic Pencil は買って正解でした。 ソフトウェアは、次のとおりです。","メインのイラストツール iPad","説明用のイラストツール iPad","日本語用の Font iPad","イラストの調整 iMac","お絵かきが苦手だったので、知人に助けてもらい、なんとか作れました。","2. 製本","製本には、技術書典オススメの日光企画さんにお願いしました。 製本する際には、用紙の種類であったり綴じ方であったりと決める必要があります。 私は、あまりこだわりがないので一般的なものを選択しました。 それは、次のようになります。 | 種類 | 選択 | | ---------------------------- | -------------------------------------- | | ご予約のセットまたは仕様は？ | 早割りセット | | 用紙サイズ | A5 | | 表紙込みページ数 | 72 ページ (表表紙+裏表紙+本文(70page)) | | 冊数 | 300 冊 | | 本の閉じ方向 | 左 | | 本の閉じ種類 | 平綴じ | | 表紙用紙 | NP ホワイト 200kg | | 表紙の印刷種類 | 通常 4 色クリア PP | | 本文用紙 | 上質 90kg | | 本文の印刷種類 | データ 150 線印刷 | | 本文はじまりのページ | 1 ページ目 | | 遊び紙 | 有り, 上質 90kg/イエロー/前 | https://jumpei-ikegami.hatenablog.com/entry/2018/10/21/084634 を参考にしました。 本文はじまりのページは、nombre をというものを設定する必要があります。 Re:VIEW Starter は nombre 対応していて、次のコマンドを叩くだけです。","用紙についてこだわりたい方は、次のリンクにあるようにサンプルを手に入れると良いでしょう。 https://natuna.jp/marcket/10282/","3. 販売準備","3.1. 物品購入","技術書典では、会場で本を販売することになります。 販売するサークルブースを目立たせるために、いくつか物品を準備しました。 | 名前 | 店舗 | 用途 | サイズ | イラスト | | ---------------------------------------------------------------------------------------------------------------------------------------- | ---------- | -------------------- | ------------ | -------- | | "," | ダイソー | 値札 | 110mm×60mm | 必 | | "," | ダイソー | 見本誌カバー | A5 | 不 | | "," | ダイソー | 公式後払い QR コード | 90mm×128mm | 必 | | "," | ダイソー | PixivPayQR コード | 90mm×128mm | 必 | | "," | ダイソー | 商品紹介 | 90mm×128mm | 必 | | "," | ダイソー | TwitterQR コード | 90mm×128mm | 必 | | "," | 公式サイト | テーブル作業 | - | 不 | | テーブルクロス前用紙 | 印刷業社 | 宣伝 | 900mm×600mm | 必 | | "," | ダイソー | あの布を隠す | 900mm×1200mm | 不 | | "," | ダイソー | あの布 | - | 不 | | "," | ダイソー | 見本誌 | - | 不 | | タペストリー用紙 | 印刷業社 | 宣伝 | 728mm×1030mm | 必 | | "," | ダイソー | 宣伝 | - | 不 | | "," | ダイソー | 著者、売り子 | 50mm×25mm | 必 | | "," | ダイソー | お品書き | A5 | 不 | | 複写式 領収書 | ダイソー | お客さん | - | 不 | | メモ、ふせん | ダイソー | 作業 | - | 不 | | 養生テープ | ダイソー | 作業 | - | 不 | | スケッチブック | ダイソー | 作業 | - | 不 | | ダンボールカッター | ダイソー | 作業 | - | 不 | https://note.mu/mochikoastech/n/nf484f114855c https://blog.vtryo.me/entry/techbookfest5-preparation-of-journey#登壇ブログなどによる宣伝活動 https://note.mu/yagitch/n/nc796a0c2c796 印刷する手段は 3 つあります。","細かいもの","1 つのファイルにして","大きなもの","で印刷","大量印刷","3.2. 電子書籍の準備","ピクシブ社のサービスである Booth を利用しました。 https://booth.pm/ja 特に専門的な知識が必要なことがなく、本の PDF を登録するだけです。 せっかく足を運んで会場に来て頂いた方のために、電子書籍と物理本の違いを出そうと考えました。 （中身のデータは同じです） そこで、物理本を購入して頂いた方には、","無料で電子書籍をプレゼント","することにしました。 技術書典ではよくある方法だそうです。 また、サンプルの本をアップロードし、無料でダウンロードできるようにすることで、 事前に本の中身を確認できるようにしました。","ただ、ダウンロード数を見る限り、あまり数は多くありませんでした。 Google Analytics (","初登録",")と Booth が連携できるので、流入数を見れるのですが、 離脱率が 86%という悲しい結果を知りました。ここは改善の余地がありそうです。 pixiv ID 登録しないとダウンロードできないので、ここが駄目ならサービスを使わない方が良いかもしれません。 見本誌に限っては、GoogleDrive で渡すようにするとかですかね。","3.3. 支払い手段の準備","技術書典では、次のような支払手段を用意しました。","公式かんたん後払い pixiv pay","前者は、技術書典で口座情報を登録すると利用できます。 後者は、アプリをダウンロードして商品を登録するだけです。無料です。 他にも PayPay や Kyash といった手段も用意しようか迷ったのですが、やめました。 理由は、支払い方法が若干複雑そうでしたからです。","4. その他","宣伝","この本のことを広く知ってもらうためには、宣伝が必要です。 私が取った宣伝手段は次の通りです。","Twitter で \"#技術書典 7\" タグ LINE の OpenChat や TL FaceBook 本の内容と関係する勉強会ハッシュタグ 会社 友人","あとは、勉強会に参加して宣伝する手段もあります。","「興味を持ってくれそうな人」が「多い」場","を探す必要があります。 例えば、4 番は事前に ","で関係がありそうな勉強会を調べて、 該当ハッシュタグで宣伝したりしました。 また、積極的に 1 番を実施していると、他のサークル参加さんがリツイートしてくれるため、とても助かりました。 Twitter で宣伝するために、16:9 の画像を用意したりもしました。 https://twitter.com/silverbirder/status/1172097536510676994","被チェック数と販売冊数","被チェック数は、お客さんが気になる本をチェックした数になります。 この数字から、印刷する冊数を決める大きな要因になります。 https://note.mu/yagitch/n/n2b5576363f4e 恥ずかしい話になりますが、私は毎朝この数字を見ていました。（笑） https://github.com/silverbirder/get-checked-number-for-techbook 被チェック数を定期的に取得する API をサクッと作って、CloudFunction で稼働させています。 https://twitter.com/silverbirder/status/1171178281380405248 このようにどの時間やどの曜日にチェックされるのかがわかるようになります。 今回、300 冊を印刷することにしました。間違いなく残ってしまうと思うので、 とらのあなさんへ委託しようと考えています。 https://news.toranoana.jp/107460 残ってしまったいくつかの本は、お家に保存用として持ち帰ろうと考えています。（笑）","公式ツイッター","公式ツイッターアカウントをフォローしておくと、なにかと便利です。 https://twitter.com/techbookfest","Google カレンダー 登録","技術書典のスケジュールが登録されている Google カレンダーを、ご自身のカレンダーにも登録することをオススメします。 https://twitter.com/techbookfest/status/1083948257095503872 いつまでに何をしないといけないのか逆算できるので、知っておいたほうが良いです。","スケジュールと実績","公式予定と私の実績は次のとおりです。 | 日付 | 公式予定 | 筆者実績 | | ----- | -------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | | 07/10 | 当落通知日 | 当選 | | 07/12 | - | "," | | 07/17 | 入金締切日 | 入金済 | | 07/20 | - | 計画立てる＆テーマ確定 | | 07/23 | - | castaneai くんに 印刷用サークルカット書いてもらう | | 07/27 | - | "," | | 07/31 | 印刷用サークルカット締切日 | 登録済 | | 08/02 | - | 原稿作成開始 | | 08/15 | - | レビュー対応中(2st) | | 08/21 | サークル配置発表日 | "," | | 08/26 | 一般参加者向け正式サイトオープン | "," | | 08/29 | - | "," | | 08/30 | - | 原稿作成完了, ",", "," | | 08/31 | - | ","(あの布は事前購入) | | 09/06 | - | 現在 | | 09/07 | サークル通行証の割当日 | - | | 09/19 | 見本誌の提出締切 | - | | 09/22 | イベント当日 | - | ※ レビュー依頼中に、表紙等のイラスト作成を並行して進めていました。 サンプルコードを書いている時は、「こんな話も増やそうかな？」と楽しい気分になれました。 原稿を書いている時は、「予定日の ◯◯ 日には終わらせないと…」という苦しい時期がありました。 なんだかんだで、日光企画さんの早割チケットを手に入れれたので満足です。","終わりに","私が今回、初執筆した経験を包み隠さずすべて公開しました。 執筆をやってみてよかったことは次のとおりです。","本を作るのは、意外と簡単","ツールやサービスが十分整っている","書いた本が好きになる","いろんな人にみてほしい 書いた知識は、とても定着する","コミュニティが楽しい","\"#技術書典\"のハッシュタグはオススメ フォロワーが増えた","逆につらかったことは次のとおりです。","ずーっと本のことを考える 締め切りに追われる 文章力のなさを痛感する","初執筆しようと考えている人にとって、何かの助けになれば幸いです。 あとは、技術書典 7 当日を楽しむだけ！！"],"t":"技術書典7で初執筆した経験をすべて公開"},{"f":"src/routes/blog/contents/part_time_jobs_I_had_as_a_student/index.mdx","c":["学生時代に経験したアルバイト","学生時代、いろいろアルバイトをしました。忘れないように記録として残そうと思います。","郵便局 年末年始のはがき","種別","短期","時期","高校生","頻度","年末年始(2 週間程度)","概要","年末年始のはがきの仕分け","感想","はじめてのアルバイト。有名人が近くに住んでいるのを知り驚いた。アルバイトの人によっては社員からお小遣いを貰っていた。仕分けが楽しかった。","ヤマザキパン工場","大学生","1 ヶ月(週 2,3 日)","レールに乗っているパンの調整","時間の経過がめっちゃ長く感じて、暇だった。パンがレールにずーっと流れていて、定型作業をずっとする。お昼ごはんに菓子パンとお味噌汁が無料。4 食パンやスナックパン、ランチパックのちょっとした手伝いをした。","市民プール 監視員","市民プールの監視員","街の小さな市民プールでの監視員。定期的にプールに入れるのが楽しかった。眠くて監視台で寝そうになって怒られたのを覚えている。","東急ハンズ","東急ハンズの品出し、接客","倉庫から東急ハンズの商品を運んで、お店に陳列。お客さんに商品の案内。はじめて、お客さんと接する仕事をして、緊張した。","個人書店","1 ヶ月(週 4,5 日)","私立中学向けの教材セットの準備・運搬","めちゃくちゃコスパがよかった仕事。1 日 3~5 時間ぐらいで 8000 円ぐらい貰えて、アルバイト最終日にボーナス(数万)も貰えた。する仕事は、新学期の中学生向けの教材(国語,数学,etc)をセットで準備。中学校へ運搬。めっちゃ楽だった。","美術館のモノ運搬","2 週間(週 3,4 日)","ある美術館への美術品の運搬","重めの美術品を車から美術館へ運ぶ。純粋に力仕事でしんどかった。","ミスタードーナツ","長期","6 ヶ月(週 3 日)","接客・レジ","姉がミスタードーナツで働いて、廃棄のドーナッツをダースの箱で持って帰ってくれたのをきっかけに、自分もアルバイトをはじめた。レジで、ドーナツを見てレジ打ちするのが、難しかった。ダース箱にうまくドーナツを入れるのも難しい。コーヒーのおかわりで、店内をウロウロするのが楽しい。女性のアルバイトが多く、肩身が狭かった。","ミドリ電化","商品陳列・レジ","クレジットカードでの支払いが多く、慎重にレジ打ちしていた。薄い記憶だが、店内の商品(乾電池)を、子供に盗ませて、お母さんが捕まったのが印象に残っている。","塾講師","4 年(週 4,5 日)","小中高校生の塾講師","大学生の頃、一番働いた。複数の塾を掛け持ち。古参になった。1 対 2,3 人の形式が多く、1 コマの時間配分を工夫したり、生徒に合わせた伝え方を考えた。"],"t":"学生時代に経験したアルバイト"},{"f":"src/routes/blog/contents/publish_my_web_components/index.mdx","c":["OEmbedとOGPのWebComponentsを作ったので、自分のブログサイトに使う","次の 2 つの Web Components を作成しました。","https://www.webcomponents.org/element/Silver-birder/o-embed https://www.webcomponents.org/element/Silver-birder/ogp-me","背景","自分のブログで、埋め込みコンテンツを表示したいな〜って思ってました。 iframely というサービスを使っていましたが、自前で作りたいなというモチベーションが生まれました。 そこで、OEmbed と OGP の表示ができるように、Web Components を独自に作成しました。","o-embed","https://twitter.com/silverbirder/status/1475262255818473473","ogp-me","https://silverbirder.github.io/blog/contents/intro_rocket/","終わりに","自分のブログは、Rocket という Markdown と Web Components をシームレスに使える SSG を使っています。 そのため、めっちゃ楽に、自分で公開した Web Components を組み込むことができました。 いや〜、満足です。"],"t":"OEmbedとOGPのWebComponentsを作ったので、自分のブログサイトに使う"},{"f":"src/routes/blog/contents/prompt_phrases_useful_in_Midjourney_StableDiffusion/index.mdx","c":["Midjourney, StableDiffusion で役立つPrompt フレーズ集","Midjourney や StableDiffusion を使っていると、どういうフレーズを使えばよいかわからなくなります。 そこで、フレーズ集を作って、Prompt で役立てたいなと思っています。","練習場","どこで Prompt の練習したら良いか悩むので、まとめておきました。 お勧めは、DreamStudio.ai です。","Midjourney","課金制(無料枠あり)","StableDiffusion","OSS","Google Colab","WebApp","DreamStudio.ai","API","Docker","Prompt で入力する文章構成","Prompt で入力する文章は、AI に理解しやすい構成である方が良いです。その方が欲しい画像を手に入れやすくなります。 文章の構成は、次のフォーマットです。","※ "," より引用 各項目についてのフレーズを、まとめておきました。","全体のフォーマット","Ancient of Beautiful concept art of Cartoon of Concept art of Detailed illustration of Detailed water painting of Detailing oil painting of Futuristic of Illustration of Image of nightmare of Logo about Pencil sketch of Photo of Pop art of Portrait of Scene of graphic novel that Scene of the movie that Screenshot of UE5 of Side profile of Sketch of","作者","by 作者 in 作者 style","全体の補足","8k art beautiful shadow collection sheet comic golden hour grid kawaii manga perfect lighting pixiv realistic photo unreal engine","フレーバー","11mm by Canon EOS 5D Mark4 and SIGMA Art Lens 35mm F1.4 DG HSM, F1.4, ISO 200 Shutter Speed 2000 EF11-24mm F4L USM no background taken with Canon 5D Mk4 white background","終わりに","フレーズをまとめておくと、Prompt で文章入力するときに、参考にできて便利でした。 別件で、そもそも私には英語力がないため、Deepl を欠かせないことに気づきました...。 では、よい お絵かきライフを!"],"t":"Midjourney, StableDiffusion で役立つPrompt フレーズ集"},{"f":"src/routes/blog/contents/puppeteer_account_manager/index.mdx","c":["アカウント画像一括更新ツールを作ったので、紹介と学びについて","Google や Github など、様々なサービスのプロフィール情報(画像, etc)を一括更新するツール、puppeteer-account-manager を開発しました。 開発の目的や、開発から得た知見を紹介します。 リポジトリは、こちらです。 https://github.com/silverbirder/puppeteer-account-manager","なんで作ったの？","Github や Twitter、Facebook など、Web サービスにはプロフィール画像を登録することができます。 私の性格上、どのサービスでも、同じ画像で登録したいと考えています。 そのため、いい感じのプロフィール写真を手に入れたら、全サービスのプロフィール画像を再登録しないと気がすまなくなり、とても面倒です。 そこで、今回、その面倒さを解決したく、このツールを作りました。","それ、Gravatar で良くない？","今回の面倒さは、Gravatar という Web サービスで解決できるかもしれません。 http://gravatar.com/ このサービスは、グローバルなプロフィール画像を提供するサービスです。 API 経由で、プロフィール画像を取得できます。 しかし、次の問題があったので、却下となりました。","gravatar が提供するプロフィール画像サイズは 80px × 80px","サービスによっては、小さすぎる","画像サイズを拡大することができるが、画質がよくない","gravatar が提供するプロフィール項目が固定","画像だけではなく、プロフィール項目も一括登録したかった","サービスによっては、プロフィール項目がマッチしない","そこで、Contentful という API ベースの CMS を使うことにしました。 https://www.contentful.com/ Contentful では、自由に項目を決めることができます。 独自に作った項目 (画像や紹介文)を、API 経由で取得できるため、とても便利です。","どうやって作ったの？","愚直なやり方です。 Puppeteer と呼ばれる Chrome ブラウザを自動操作できるライブラリを使いました。 Chrome ブラウザから、\"各サービスへログインし、写真をアップロードする\"処理を自動化しただけです。 https://github.com/puppeteer/puppeteer","プロフィール画像を更新する API は、なかったの？","サービスによってはあります。例えば、Twitter には、次のようなプロフィール画像を更新する API があります。 https://developer.twitter.com/en/docs/accounts-and-users/manage-account-settings/api-reference/post-account-update_profile_image ただ、全てのサービスには、そのような API はありません。 API を使って更新するのが正しい姿ですが、全サービスの実装方法の足並みを揃えるために、 Puppeteer で自動操作することにしました。","パスワードって大丈夫？","Puppeteer を動かす node アプリケーションと、Chrome ブラウザを同一マシン内で動作するようにしました。 そのため、node アプリケーション実行中に、パスワードを傍受されることはありません。 また、パスワードの設定は環境変数から注入するようにしています。 Docker コンテナで動作できるようにしているので、ローカルでも、コンテナサービスでも動かすことができます。 今後、パスワードの管理は、Keepass や Lastpass のようなサービスと連携したいと思っています。 https://github.com/keeweb/kdbxweb","どのサービスが対応している？","これは、私が楽になりたいために作ったため、使い方が、限定的になっています。 | サービス名 | 認証手段 | | ---------- | ------------ | | Hatena | Google 認証 | | Qiita | Google 認証 | | Medium | Google 認証 | | Note | Twitter 認証 | | devTo | Github 認証 | | Twitter | 通常認証 | | Github | 通常認証 | | Google | 通常認証 | | Facebook | 通常認証 | | LinkedIn | 通常認証 | 詳しくは、"," をご確認下さい。","どんな学びがあった？","結構色々とハマりました。","極力 セレクタ指定したコードを書かない","Web サービスが返す HTML は、いつもずっと変わらないことはありません。 ある id や class の html タグがずっと残り続けるとは限りません。 そこで、できる限り、セレクタを指定せずにブラウザ操作をするようにしました。 例えば、","ボタンやリンクをクリックしてページ遷移するのではなく、目的のページへ最短で直接遷移する"," とか。","submit ボタンをクリックするのではなく、エンターキーを入力する","です。こうすることで、安定した自動化ができました。","XPath が意外と使える","Google や Medium では、id や class がランダム値になっています。 そのため、単純な id や class を指定して進めることができません。 そこで、『○○』のテキストが含まれているセレクタの指定することが、XPath でできます。 これは、助かりました。","ログインが難しいものは、無理せず諦める","Amazon のログインは、2 段階認証が発生します。 テキストメッセージや、音声電話によるログインが求められ、Puppeteer 単体ではどうしようもありません。 この 2 段階認証の機能を解除することもできますが、セキュリティ上よろしくないので、ここは無理せず諦めることにしました。","並列処理をガンガン実行する","処理速度向上のため、全サービスを Promise.all で並列処理しました。それぞれが、シークレットウィンドウで開くことで、独立して処理するようにもしました。 しかし、たまに Puppeteer が落ちてしまうことがあります。原因は、実行しているマシンのスペック(Core 数)にも影響しますが、サービス側からの影響も受けたりします。 そのため、落ちても大丈夫のようにエラーハンドリングし、リトライするようにしました。 また、失敗したらどういった画面なのか知りたいので、スクリーンショットを撮るようにもしました。","Docker で実行可能に","Puppeteer に必要なモジュールを Docker に詰め込み、ログイン情報等を環境変数から外注することで、 環境非依存の実行環境ができました。そのため、Pub/Sub と Container Engine 等を組み合わせれば、 Contentful の Webfook 経由で、アカウント情報を更新することができます。","終わりに","私の性格がもっと大雑把であれば、このツールを作らなかったのですが、どうしても気になって仕方がなく... (笑) 最後まで読んでいただき、ありがとうございました。"],"t":"アカウント画像一括更新ツールを作ったので、紹介と学びについて"},{"f":"src/routes/blog/contents/recovery_and_starting_job-hunting/index.mdx","c":["復調と面接質問の回答","2023 年 6 月末、私は体調を崩してしまいましたが、幸い 2 ヶ月の休養を経て、回復することができました。この回復を受けて、新たな職場での活動を再開することを決意し、正社員としての転職活動を始めることとしました。この記事では、これまでの 2 ヶ月間の出来事と、面接での質問への予想される回答をまとめています。 ※体調を崩した背景や詳細については、","で詳しく触れていますので、ご興味があればご参照ください。","体調の経緯","2023 年 7 月より、元気を取り戻すため以下の取り組みを実施しました。","毎朝の散歩やジム通い 友人や親戚との交流 好きな食べ物や甘味の摂取 旅行への出掛け 趣味であるプログラミング","これらの活動により、笑顔が増え、良い睡眠がとれるようになりました。(ありがとう、ポケモンスリープ) 多くの方々から、明るくなったとの声を頂きました。特に薬を頼りにすることなく自然に体調が良くなったのは、大変嬉しく思います。 また、祖母の手術が無事成功し、そのことも安堵しています。 以前は、介護の負担で心に余裕がなかったのですが、祖母の体調の回復により、その点でも安心しています。","業務委託の経緯","2023 年 7 月中旬から、業務委託の形式で、週 3 日のフルリモートワークを行っています。業務委託を選んだ最初の動機は、体調に配慮しながら柔軟に仕事をこなすためでした。しかしながら、今では勤務のリズムをつかみ、ON/OFF を明確に分けることができるようになりました。機能開発の中で、お客様からのありがたいフィードバックやチームメンバーからの「丁寧なテキストコミュニケーション」という賛辞を得ることができ、自分の能力に再び自信を持つことができました。その結果、仕事に全力で取り組めるように戻りました。","業務委託か正社員か","体調が安定したタイミングで、業務委託の道を続けるか正社員を目指すか、妻と相談しました。 業務委託は、収入の柔軟性や仕事の量を自由に決められる利点があります。一方で、確定申告や保険料の手続きの煩雑さ、補償面の薄さなど、自分自身で調査・対応する手間が生じます。正社員としての働き方は、業務委託とは対照的に自由度は制約されますが、安定した収入や充実した福利厚生を享受できるメリットがあります。 妻との話し合いの結果、私たちは","安定した収入や福利厚生","、特に子育てや健康面でのサポートを重視することに決定しました。そこから、正社員としてのキャリアを追求する方向で転職活動を開始することとなりました。業務委託としての経験は、正社員としての働き方の価値を新たに感じさせてくれました。 現在お世話になっている企業には心から感謝しています。もちろん、こちらの企業での正社員のポジションも考慮していますが、他の企業も含めて、正社員としてのポジションを探しています。","面接での質問への回答","面接時に頻繁に聞かれる質問への回答を整理します。これは、自らの考えを明確にし、面接担当者にも適切に伝えるためのものです。","興味のある業界について","EdTech 分野に興味を持っています。学生時代、私は 4 年間塾講師として働いており、その経験を通じて、子どもたちの成長を間近で感じることができました。生徒一人一人の性格やニーズに合わせて、教え方やコミュニケーションの方法を調整する面白さがありました。彼らのスキルや理解が徐々に増し喜んでくれると、私にとっても嬉しい気持ちになりました。そのため、","人の成長をサポートする仕事","に、携わりたいと考えています。 ちなみに、学生時代に自ら制作した初めてのアプリ、Gif 掲示板を実際に生徒たちに利用してもらい、「面白い！」というフィードバックを受け取ったことが、IT 業界に興味を持つきっかけとなりました。","職種について","これまでのキャリアで、Web エンジニアとしての経験が豊富です。そのため、Web アプリケーション開発に関連するポジションを希望しています。私はフロントエンド、バックエンド、インフラ、データ基盤の各領域での経験を有していますが、特に ","Web フロントエンドの開発"," が得意です。また、性格的には守り重視であり、品質や運用に特化して力を発揮したいと考えています。","キャリアビジョンについて","これまで","2016 年に Sler 企業に新卒として入社し、Web アプリケーションの開発・保守・運用を手掛けました。上流工程から下流工程までの幅広い業務を経験しました。その後 2018 年に大手 EC 企業へ移籍。こちらではフルスタックエンジニアとして、データ、インフラ、アプリケーションなど様々なことを経験できました。また大規模アプリケーションのリアーキテクチャやモダナイゼーションに貢献しました。2022 年には FinTech 企業 に転職し、フロントエンドエンジニアとして既存プロダクトの改修や新規プロダクトの開発に取り組みました。 詳しい情報は、業務履歴書をご用意しています。ご興味があれば","にお伝えください。","これから","ビジョン","Web 技術を通して、ユーザーの生活をより豊かにすることができる"," 、そんな Web エンジニアを目指しています。","キャリア","フロントエンドの専門家として、テストや DevOps のノウハウを活用し、フロントエンドの品質を向上させつつ、機能開発に貢献したいと思っています。 ユーザーや開発者の視点を大切にし、両者の体験を向上させる方針で業務を進めたいと考えています。フロントエンドだけでなく、フルスタックの観点や保守運用の経験からプロジェクトを俯瞰し、設計開発したいと思います。 管理職の経験はありません。エンジニアとしての専門性を深め、スキルを積み上げていく方向でキャリアを進めていきたいと思っています。管理職は絶対 NG という訳ではなく、少数のチーム等であれば、検討したいと考えています。","自身の長所と短所","ハードスキルの長所","私の長所は、多角的な観点からの設計能力です。フロントエンドだけでなく、バックエンドやインフラといったレイヤーを経験しています。また、0→1 フェーズのプロダクト開発から、10→100 フェーズの保守・運用に至るまでの経験があります。これには自社開発の経験も含まれるとともに、Sler での請負開発の経験もあります。","ソフトスキルの長所","学んだことや伝えたいことをテキストとしてアウトプットする能力","が私の強みです。自分の考えや学びを文章にすることで、それを整理し、他者と共有することが得意です。技術記事や技術書の執筆経験もあります。非同期のコミュニケーションやフルリモートワークの環境下では、このスキルが特に重要となります。また、文章としてのログや証拠を残すことで、進捗や問題点を効率的に共有することが可能です。","ハードスキルの短所","幅広い経験は持っていますが、特定の分野に特化した経験は少ないです。これを補うため、フロントエンド分野にて、自分の経験値を生かして深めていきたいと考えています。例えば、テスティングや DevOps、アーキテクチャなどです。","ソフトスキルの短所","私は人見知りの傾向","があります。特にリモートワーク環境下では、対話相手の背景情報が限られるため、名前以外の詳細を知らない相手に話しかけるのは苦手です。しかし、一度コミュニケーションを取ることができれば、その後の関係構築には問題はありません。","一緒に働きたい人・働きたくない人","一緒に働きたい人","知見をオープンにする人","私は、","学び取った知識や経験を積極的にオープン","し、他者と共有し合うことで、新たな気づきやフィードバックを得ることができると思います。自己成長につながりますし、チームや組織、コミニティの成長にもつながると思います。","一緒にいて楽しい人","仕事は、結局は人との関わりの中で成し遂げるもの。そのため、一緒に働く人との関係性が非常に大切です。私は、一緒にいると楽しさを感じるような人と共に仕事をしていきたいです。","一緒に働きたくない人","黙々と仕事を進める人","情報を閉じ込めて仕事を進めると、進捗の共有や問題点の議論、助け合いの機会が減少します。結果のみが伝えられても、その過程が不明瞭であると、適切なサポートが難しくなります。過程は、たとえ簡潔なログであれ、共有されることが望ましいと考えています。","人によって態度を変える人","私は、相手によって態度を変える人とは働くことが難しいと感じています。彼らは時にチームの雰囲気を悪化させる原因となり得ます。","これまでの大きな成果・苦労したこと","コロナ禍における EC サイトの検索改善","コロナ禍に伴い、突如として急激に売れた商品（例：トイレットペーパーやマスク）の在庫情報をリアルタイムで表示する機能の設計からリリース、そして運用までを担当しました。企画は古くから在籍する経験豊富な先輩によって立案され、その指示のもと設計を進めました。システムの要件を明確にするための Design Doc を作成し、そのドキュメントに基づきフロントエンドでの実装を行いました。この機能により、売り切れることの多い商品の在庫情報は CMS 上から更新することが可能となりました。このプロジェクトは","スピード勝負","で、わずか 2 週間ほどでのリリースとなりました。最大の課題は、当時の私にとって複雑であった検索ロジックの理解でしたが、先輩のサポートのもと乗り越えることができました。","大規模フロントエンドのリアーキテクチャ","Python の EOL 対応を目的とした大規模なフロントエンドのリアーキテクチャを 1 年以上の期間をかけて行いました。特に重要な検索ページの担当を務めました。既存のページの機能理解や、新しいクリーンアーキテクチャの導入は非常に時間がかかりましたが、チームメンバーとの継続的なコミュニケーションにより、成功に至りました。","請負契約における炎上プロジェクト","新卒時代に参加したプロジェクトで、開発初期から最後までのフェーズを経験しました。お客様からの要件変更が頻発し、納期に迫る中での開発は多くの困難を伴いました。しかし、仲間たちとの連携や他部署からの協力もあり、プロジェクトを進行させることができました。その結果、アプリは完成することができました。納品の詳細は不明です。","チーム開発の経験","私はアジャイル開発におけるチームでの経験を持っています。ペアプログラミングでの認識統一やスキル共有があり、これが後のレビューの効率化に繋がっています。PullRequest レビューでは、フィードバックの質を意識し、以下のようなポイントを大切にしています。","レビューイ","詳細な Description 1 つのタスクに絞った小さな変更 コメントの活用","レビュワー","賞賛と楽しさを取り入れる（giphy や emoji など） フィードバックの温度感（IMO/FYI/MUST） 機械的なチェックを CI へ移譲 可視化情報の提供（動画、写真、コード）","情報をチームへオープンにするため、様々なドキュメントを作成することを心がけています。","議事録 運用書: Run Book 設計書: Design Doc 設計変更履歴: Architectural Decision Records","コミュニケーション","私はテキストとビデオ、2 つの方法でコミュニケーションを行っています。","テキストコミュニケーション","非同期伝達で効率的。 強調、リスト、リンク、絵文字を用いて明瞭な伝達を心がける。 必要な返答とその緊急性を伝達する。","ビデオコミュニケーション","言語化が困難、または対面での話し合いが望ましい場合に適用。 テキストでのやり取りが繰り返される場合はビデオに切り替える。 共同編集のメモを使用し、感謝の気持ちを伝える。","コミュニケーションにおいては、","和やかな雰囲気作り","を重視し、アイスブレイクやユーモアを取り入れたい人です。また、フラットな関係性で議論ができることを望んでいます。","趣味や興味","私の趣味の一つは、個人開発です。興味を持った技術やツールを用いて、常に新しい学びを追求しています。フロントエンドの技術だけでなく、DevOps に関連するライブラリやツール、さまざまな SaaS にも手を広げてきました。その中で、","学んだことをアウトプットすることを欠かしません","。これは自分の考えを整理するため、コミュニティに貢献するため、そして自己成長の一環として重要だと感じています。 また、私は温泉や散歩が大好きです。自分を穏やかな性格の持ち主だと自覚しており、温泉や散歩は心身のリフレッシュになるだけでなく、思考を整理する良い機会としても利用しています。 さらなる軽いトピックとして、私のお気に入りのビアードパパのシュークリームは ","「パイコロネ」","。そして、ミスタードーナツでの一番のお気に入りは ","「エンゼルフレンチ」"," です。","収入や福利厚生","収入に関しては、できれば前職と同等以上を希望いたします。また、福利厚生としては、育児休暇、介護休暇、時短勤務などの制度が充実している企業を望んでいます。これは、今後家族の介護や育児の責任が生じる可能性があるためです。","働き方","関西地域を拠点として出社が可能です。 フルリモートや週に一度の出社を伴うリモートワークも可能です。","その他","体調不良の経験を踏まえ、","余裕(余白)を持って働きたい"," と思います。","最後に","最後までお読みいただき、ありがとうございました。何かありましたら"],"t":"復調と面接質問の回答"},{"f":"src/routes/blog/contents/replace_my_portfolio/index.mdx","c":["silverbirderのポートフォリオページ刷新","import { Image } from \"~/components/image/image\"; この度、私のポートフォリオページを刷新致しました。本記事では、 刷新することになった動機から、刷新内容、今後について紹介したいと思います。","動機","元々、私のポートフォリオページは、静的ページジェネレーターである Hugo を使って 構築していました。 http://kohki.hatenablog.jp/entry/hugo-portfolio こちらの記事を参考にして、Hugo でポートフォリオページを作りました。 その当時、なぜポートフォリオを作ったのかというと、確か次の 3 つの思いがありました。","私がどういった人かを知ってもらいたい 自分のサイトを持ちたい 静的ページジェネレーターを使ってみたい","Hugo で記事を管理する対象は、Markdown であるため、エンジニアにとって書きやすいです。 また、デザインテーマは、公開されているテーマがあるので、好きなものを選びます。 導入当初は、とても快適でした。手軽にオシャレなポートフォリオサイトを公開できて満足でした。 しかし、ずっと使っていると、かゆいところに手が届ないもどかしさを感じるようになりました。 これは、便利さとのトレードオフだと思いますが、下記のようなデメリットがあると認識し始めました。","Javascript で技術的な挑戦が難しい デザインテーマのカスタマイズが難しい SEO のチューニングが難しい","便利さというメリットよりも、デメリットの方が大きいように思い始めました。 そのため、独自にポートフィリオサイトを作成することにしました。","やったこと","AMP を存分に使ったポートフォリオサイトを作成しました。全体像は、下記のとおりです。","を中心とした構成です。 ソースコードは、下記のリポジトリにあります。 https://github.com/silverbirder/silverbirder.github.io","技術選択","今回のポートフォリオサイトに、必要以上の機能を持つ Web フレームワーク(e.g. Next.js)を使うのは、メンテナンスコストが高くなるので、却下としました。 また、静的ページジェネレーター(e.g. Gatsby)も、動機の理由より却下としました。 そのため、必要最小限な構成を目指しました。結果、次のような流れとなりました。","コンテンツを用意する(Markdown,HTML,JSON) 1 をインプットとして","で AMP 化する","これらの順序を制御するタスクランナーとして、"," を採用しました。 ","は、NPM でインストールするので、Node.js と相性が良いタスクランナーを求めました。 その選択肢として、Grunt や Gulp があったのですが、","ので、Gulp を選択しました。 大きな技術選択としては、これくらいです。他の細かい所は、下記のとおりです。","highlightjs","プログラムコードのハイライト機能","jsdom","html の各処理","h1~h6 タグの Anchor 設定(anchorJS 風) HTML のテンプレートとメインコンテンツの Mix ...etc","ampcssframework","Dark Theme や Grid 機能が欲しかった","Cloudinary","画像管理 SaaS。OGP などに利用","SEO 向け","Google search console Google analytics","ポートフォリオコンテンツ","ポートフォリオサイトにどういったコンテンツを用意しようか悩みました。 ","があります。 これは、HTML だけではなく、Markdown も(HTML 経由で)AMP 化することができるようです。 そのため、ブログのようなコンテンツもポートフォリオページに加えることができそうと気づきました。 また、これまで私が書いたブログコンテンツは、Markdown で管理していたので、ちょうど使えそうでした。 結果、次のようなコンテンツを用意しました。","自己紹介 ブログ 持っている本検索 買ったものリスト","アマゾンで買ったもの、サブスク","ウォッチ","チェックしてる RSS","プロジェクト","作ったものの紹介","ウォッチページで、RSS の WebPush 機能を追加しようとしましたが、Push する側である Server が必要となり、開発が伸びそうだったのでやめておきました。","刷新してどうだったか","想定通り、Hugo ではできなかったような様々なポートフォリオサイトの機能拡張ができるようになりました。","Javascript で技術的な挑戦が難しい","AMP や、Web Worker(amp-script)を試せた","デザインテーマのカスタマイズが難しい","CSS フレームワークや、CSS のチューニングができた","SEO のチューニングが難しい","SearchConsole や GoogleAnalytics が使えた sitemap や meta タグのチューニングができた","想定通りにできなかったのは、AMP の制約なのですが、WebComponents のような amp-script 上で動かせない技術もあるということでした。 また、WebWorker(amp-script)上で、ES Module(",")を Import しようとしても、Safari が未対応だったりで、断念したりもしました。 ただ、最終的な感想としては、HTML を柔軟に処理できるようになったので、AMP 上でできることは何でもできるようになり、刷新してよかったと思います。","学んだこと","経験学習モデルより、簡単に振り返ります。(はじめて) | 経験 | 省察 | 概念化 | 試行 | | ---------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- | ------------------------------------------------------ | -------------------------------------------- | | AMP を初めて使ってみた | AMP 使ったことなかったけど、思っていたより課題は少なかった。 しかし、想定していなかった課題もあった。 | 使ったことがない技術要素の課題は、想定していても未知数 | 未知数な技術は、軽く試してみる | | Next.js や Gatsby など、フレームワークを使わなかった | シンプルな構成にしたかった。 必要以上に機能が多いフレームワークを入れたくなかった。 | 保守性を担保するため、最小限の機能で構成 | 大掛かりな技術の選択は、保守性と天秤にかける |","終わりに","ポートフォリオ刷新をしました。これまで 1 から Web サイトを作ったことがなかったので、 sitemap や JSON-LD など全て手作りで開発したので、良い勉強になりました。 まだまだポートフォリオの課題は山積みですが、少しずつ改善していきたいと思います。"],"t":"silverbirderのポートフォリオページ刷新"},{"f":"src/routes/blog/contents/review_of_micro-frontends_book/index.mdx","c":["「マイクロフロントエンド」を読みました","DAZN の Luca Mezzalira さんが書かれた"," を読みました。簡単な書籍レビューを残しておこうかなと思います。","なぜマイクロフロントエンドを使うのか","従来のモノリスなフロントエンドから、マイクロフロントエンドに置き換えることで、どういう価値があるのでしょうか。 書籍に書いてある内容と、自身の意見を混ぜて以下に列挙します。","機能開発のイテレーションが短くなる","1 チームに 1 サブドメインという小さなスコープのため","チーム内の意思決定がしやすい","コードベースが小さいため","リリース速度が早い","各チームが独立しているため","1 チームが小さなサブドメインで独立することで、開発やコミュニケーション、リリースなどのコスト低減ができます。 これは、最終的には顧客への価値提供するサイクルを短くすることに繋がります。","マイクロフロントエンドの導入方法","書籍には、実例として既存アプリケーションをマイクロフロントエンドへマイグレーションする話があります。 マイクロフロントエンドは、その設計の性質上、基本的に(既存アプリケーションからの)マイグレーションとセットです。 そのため、マイグレーションをどのように進めるかというのは、とても重要です。 その中で、マイクロフロントエンドへの最初の一歩として、次のパターンがあります。","共有コンポーネントをマイクロフロントエンドとしてリリース 新機能をマイクロフロントエンドとしてリリース 既存アプリケーションの一部をマイクロフロントエンドとしてリリース","要は、段階的に導入しましょうという話です。 また、マイクロフロントエンドには、従来の SPA 開発に似ている垂直分割という方法が最初の一歩としてお勧めのようです。","垂直分割","1 つの画面に 1 つのマイクロフロントエンド","水平分割","1 つの画面に複数のマイクロフロンエンド","ちなみに、マイクロフロントエンドとコンポーネントは、次のように区別します。","マイクロフロントエンド","サブドメインのビジネス表現 ロジックをカプセル化し、イベント通信","コンポーネント","再利用性の目的で使用される技術的ソリューション 拡張しやすく複数のプロパティを公開","書籍にあった好きな言葉","付録にある、New Relic で働かれている Erik Grijzen さんのインタビュー記事にて、","Q. マイクロフロントエンドを 3 語で表現すると A. Scaling UI Development","という回答がありました。 "," という言葉、めちゃくちゃ好きになりました。 マイクロフロントエンドは、フロントエンドをサブドメインで分割し、小さく独立した開発が可能となります。 大規模な 1 つのアプリケーション開発や、1 つのブランド内の様々なプロダクトを提供するアプリケーション開発に対しては、マイクロフロントエンドは効果的だと思っています。","終わりに","余談ですが、ブログを書く時間が、徐々に減っています。 1 年前とかは、1 日とか使っていたんですが、今日は 30 分とかです。 効率化できている訳じゃなく、単純に時間がなくなってきたなと思います。"],"t":"「マイクロフロントエンド」を読みました"},{"f":"src/routes/blog/contents/self-hosted_cache_server_with_turborepo-remote-cache/index.mdx","c":["turborepo-remote-cache でキャッシュサーバをセルフホストした","vercel 製の turborepo という ビルドシステムが爆速なモノレポツールがあります。 爆速にする機能の 1 つに、リモートキャッシュというものがあります。 この機能は vercel のキャッシュサーバを使うのですが、キャッシュサーバをセルフホストする方法もあります。 今回は、それを紹介します。","なぜ、セルフホストしたいのか","vercel のキャッシュサーバを使う場合、vercel のアカウントが必要です。 ","を見ると、個人利用(Hobby)では無料ですが、会社(Pro)で使うとすると、"," という価格になります。費用対効果に見合うならそれで良いかもしれませんが、まだそれがわからない段階でコストをかけられない場面もあると思います。そこで、","とおり、キャッシュサーバをセルフホストする方法があります。","ローカルで、やってみた","実際に試してみました。ソースコードは、次のリンクにあります。","https://github.com/silverbirder/turborepo-with-selfhost-remote-cache","手元に Git clone して、README に従って動作確認できると思います。必要なソフトウェアは、Docker と Yarn です。","キャッシュサーバの準備","セルフホストする場合、キャッシュサーバを建てる必要があります。 キャッシュサーバは、https://github.com/fox1t/turborepo-remote-cache を使うと良いです。 Docker イメージが公開されているので、それを使っても良いですし、自前で "," しても良いです。","Docker イメージ","https://hub.docker.com/r/fox1t/turborepo-remote-cache","キャッシュサーバには、最低でも次の 2 つを環境変数を設定する必要があります。","TURBO_TOKEN","turborepo と api を繋げるための TOKEN","STORAGE_PATH","キャッシュオブジェクトを保存するパス STORAGE_PROVIDER が "," を指定する場合は、バケット名","簡単にするため、次の.env ファイルを用意しました。","あとは、キャッシュサーバを起動するために、docker-compose を書きます。","次のコマンドで、キャッシュサーバを起動しましょう。","これで、キャッシュサーバは PORT:3000 番 で起動します。","turbo build","では、実際に turborepo からつながるか、試してみます。 turborepo は、"," で作成できます。 作成後、作成したフォルダで次のコマンドを実行します。","turbo コマンドのオプションで、3 つ指定します。","team","キャッシュを保存するときの名前空間の役割","token","先程定義した環境変数","api","キャッシュサーバの URL","実行すると次のログが表示されるはずです。"," で、リモートキャッシュが有効となりました。 初回の場合、cache miss となります。ハッシュ値は、"," と "," になります。 キャッシュがローカルに保存されるため、削除します。","ではもう一度、turbo build してみましょう。","どうでしょうか、"," と表示されています。手元にキャッシュがないのにも関わらず、リモートのキャッシュサーバにキャッシュがあるため、"," となります！","キャッシュオブジェクト","キャッシュのオブジェクトは、ハッシュ値名で、アウトプット(file やログ)のバイナリになります。 Docker コンテナ内で見ると、次のようなファイルが置かれています。","--team オプションで指定した名前で、フォルダが作成されています。 そのため、team 毎にキャッシュが作成されます。","キャッシュとは","turborepo のキャッシュについては、"," を読むと良いでしょう。 ざっくりいうと、次の流れで cache miss,cache hit になります。","turbo build を実行 turbo.json の","タスクの inputs(ソースコードなど)や環境変数をハッシュ化 キャッシュが既にローカルまたはリモートに存在していなければ、cache miss turbo.json の","タスクの outputs(dist フォルダ、標準出力など)をバイナリ化し、ハッシュ名で保存","3 の手順で、キャッシュが存在していれば、"," となり、outputs が復元します。","クラウドで、やってみた","キャッシュサーバは、AWS や GCP などのクラウドベンダーにあるコンピューティングリソースへデプロイしましょう。 Docker イメージがあるので、AppRunner や CloudRun が楽にできそうです。 キャッシュストレージは、いまのところ AWS S3 のみ対応とのことです。 AWS S3 のクライアントは、","です。まあ README に従うなら、S3 に配置するのがベターでしょう。コンピューティングリソースを動かす IAM は、ストレージリソースへの READ/WRITE 権限を足しましょう。","おわりに","セルフホストして、リモートキャッシュが使えるようになりました。 まだ運用したことがないので、課題を実感していません。引き続き、利用してみようと思います。"],"t":"turborepo-remote-cache でキャッシュサーバをセルフホストした"},{"f":"src/routes/blog/contents/rminc/index.mdx","c":["GMailをGCalendarに登録するサービス rMinc を作ってみた","import { Image } from \"~/components/image/image\";","ターゲットユーザー","GMail と GCalendar を使っている人","メールを開くって面倒じゃないですか？","例えば、次のようなメールを受信していたとします。","アマゾンで商品を購入した際、お届け予定日が記載されたメール 映画館(TOHO シネマ)でネット予約した際、上映日が記載されたメール ホテルをネット予約した際、宿泊日が記載されたメール","『いつ商品が届くのかな？メールを確認しよう』が、","面倒と感じませんか","？私は面倒と思います。 Google は気を利かせて、次のような予定を勝手に登録してくれることがあります。（良い悪いがありますが...） この気を利かせるかどうかは、Google の判断によるため未知数です。 先程あげた例のメールも、同様のことが勝手にしてくれたら良いな〜と思っていました。 そこで、rMinc というツールを作りました。 ※ 昔、","という、アマゾンからのお届け予定日が記載しているメールを GCalendar に登録するツールを作りましたが、 アマゾンのメールに特化しすぎてしまい、汎用性がないものとなりました。","rMinc is 何？","https://www.npmjs.com/package/@silverbirder/rminc","rMinc is the Google Apps Script Library that register Mail in Calendar.","以下サービスからの GMail が届いたときに、その内容を抽出して GCalendar に登録します。","発送のお知らせ (お届け予定日)","チケット購入完了のお知らせ (上映日)","配送手配のお知らせ (お届け予定日)","また、これ以外にも対応したいサービスがあると思うので、カスタマイズして使えるようにしました。 詳しくは、","をご確認下さい。 概要はこんな感じです。","特定キーワードでメールを抽出","アマゾンなら、","メールの下記を抽出","タイトル 本文","イベント開始日&終了日 (予定日とか） 場所 (配達先とか)","メールのリンク","抽出した内容を GCalendar に登録","実際に使ってみるとこんな感じになります。 小さくて見えないと思いますが、お届け予定日、タイトル、配達先、メールリンクが登録されています。 このツール(",")を GAS 上で 定期的に動かしておくだけ で、自動的に GCalendar へ予定登録されます。当たり前ですが、無料です。 ※ RMinc は、","にある APP ID を登録する必要あり","最後に","Google Apps Script は、エンジニアにとって、とても強力な武器です。特に、G Suite を積極的に使っている人にとっては、欠かせないものです。 こういった  かゆいところに手が届く  ことができるのは、Google Apps Script の魅力的なところです。 ぜひぜひ、積極的に活用していきたいですね！"],"t":"GMailをGCalendarに登録するサービス rMinc を作ってみた"},{"f":"src/routes/blog/contents/smart_home_for_enginner/index.mdx","c":["エンジニアのためのスマートホーム化","エンジニアの皆さん、IoT 使っていますか？ スマートホームに欠かせない IoT 商品を使うことで、生活体験はより良くなります。 この記事では、ご自宅をスマートホーム化するための IoT 商品をリストアップします。 『そんなのあるの？』という気づきがあれば、幸いです。","スマートプロダクト リスト","スマートリモコン","Nature Remo","https://nature.global/jp/nature-remo","スマートスピーカー","Google Home","https://store.google.com/product/google_home","スマートロック","SESAME","https://jp.candyhouse.co/","スマートトラッカー","Tile","https://thetileapp.jp/","スマートタグ","Qrio Smart Tag","https://qrio.me/smarttag/","スマートスイッチ","Switch Bot","https://www.switchbot.jp/","スマートトースター","Toasteroid","https://www.kickstarter.com/projects/258723592/toasteroid-first-app-controlled-smart-image-toaste","スマートボタン","Qmote S","スマート加湿器","SwitchBot 加湿器","スマートプラグ","TP‐Link HS105","https://www.tp-link.com/jp/home-networking/smart-plug/hs105/","スマートスケール","Withings Body +","https://www.withings.com/jp/ja/body","スマートスリープ","Withings Sleep","https://www.withings.com/jp/ja/sleep","スマートライト","Light Strip Plus","スマートカメラ","Arlo Ultra","https://www.arlo.com/jp/products/arlo-ultra/default.aspx","スマート歯ブラシ","Philips Sonicare","https://www.philips.co.jp/c-m-pe/electric-toothbrushes","スマートカーテン","Mornin' Plus","https://mornin.jp/","SwitchBot Curtain","https://www.rakunew.com/items/82929","スマートエアモニター","Awair","https://jp.getawair.com/","スマートクリーナー","iRobot","https://www.irobot-jp.com","スマートテレビ","Chromecast","https://store.google.com/jp/product/chromecast","スマートウォッチ","Apple Watch","https://www.apple.com/jp/watch/","スマートケトル","iKettle","https://www.smarter.am/ikettle","スマートグラス","Focals","スマートエナジーハブ","Nature Remo E","https://nature.global/jp/nature-remo-e","Other","食宅便","https://shokutakubin.com/shop/default.aspx"],"t":"エンジニアのためのスマートホーム化"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_02/index.mdx","c":["一足遅れて Kubernetes を学び始める - 02. Docker For Mac -","import { Image } from \"~/components/image/image\";","ストーリー","前回","にて、Kubernetes を学ぶ環境を考えてみました。いきなり GKE を使うんじゃなくて、お手軽に試せる DockerForMac を使おうとなりました。","Docker For Mac を試す","環境","実践","さっそく、使ってみます。 (","参考)","Kubernetes では、MasterNode と WorkerNode の 2 種類の Node が存在しており、 そのうちの MasterNode にあるコンポーネントの一覧が上記よりわかります。詳細については、","にあります。 要は、"," とすると","etcd にマニュフェスト(nginx.yaml)を登録 controller-manager が etcd にあるマニュフェストと既存 pod を比べて pod が少ないことを検知 scheduler が適切な数の pod に調整","という理解になりました。また、全てのやり取りは、api-server を経由しているそうです。 私なりの理解をアウトプットしたものが下記になります。 (ほとんど真似した感じです。しかし、アウトプットするだけで理解が深まるため実施。 ","アウトプット大事！"," )","使い始めたばかりだと、pod が１つもない状態ですね。 また、DockerForMac では、もちろん動かしているマシンは一台（VM とか使えば増やせますが）なので、 MasterNode と WorkerNode が同一になっているはずです。試してみます。","WorkerNode に Pod が作られていますね。んー、これだとある程度の学習には繋がりそう（Pod の動き）ですが、 後の学ぶ ReplicaSet や Daemonset など Node 横断した機能を経験したい場合には不向きのようですね。 まあ、簡単に使えるので良いっちゃ良いのですが... 次は、いくつかのコマンド(cp,exec, port-forward)を試してみます。","ローカルと Pod との双方向コピー、仮想的なターミナルを体験していました。 「ふ〜ん、で？」ってなっちゃいました。(笑)","お片付け","ものたりない","やっぱり Node 増やしたい！！ ","を見て、これをやるっきゃない！ すごく今更だけど、試してみようと思います。 次回は","です。"],"t":"一足遅れて Kubernetes を学び始める - 02. Docker For Mac -"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_03/index.mdx","c":["一足遅れて Kubernetes を学び始める - 03. Raspberry Pi -","import { Image } from \"~/components/image/image\";","ストーリー","前回","では、Mac で Kubernetes を軽く動かしてみました。DockerForMac では、Node が Master のみだったため、Kubernetes を学習するには、ものたりない感がありました。そこで、RaspberryPi を使っておうち Kubernetes を構築することになりました。 参考サイト","レシピ","| 商品名 | 個数 | 用途 | | ---------------------------------------------------------------------------------------------------------- | ---- | -------------------------------------- | | "," | 3 つ | MasterNode1 台 WorkerNode2 台 | | "," | 3 枚 | RaspberryPi の image 書き込み先 | | "," | 1 本 | RaspberryPi とネットワーク接続 | | "," | 1 台 | RaspberryPi の電源 | | "," | 4 本 | RaspberryPi と USB 充電器をつなげる | | ","  ヒートシンク付 | 1 台 | 4 段 (3:RaspberryPi,1:USB 充電器) | RaspberryPi は世代 3 の ModelB なら WiFi 接続できるので、自宅の WiFi につなげることにしました。自宅では SoftbankAir を使っています。 （ただし、初回のみ LAN ケーブルでネットワーク接続します) また、私の環境は下記のとおりです。","構築（物理）","で十分な情報があります。こちらを参考にして組み立てします。 できたものがこちらです。 WiFi を使うために、LAN ケーブルや WiFi 親機などがなくなり、スッキリしました。 電源を確保できるところであれば、家の中なら、どこでも持ち運びできます。 ✨","構築（論理）","をダウンロードしておきます。 Step の 1 から 3 までの手順を","RaspberryPi 一台ずつ"," 、下記の手続きを踏んでいきます。","1. 初期設定","microSD カードを Mac につなげた後に、下記を実施します。","イメージを書き込み際、","r"," をつける (rdisk3)と、高速になるそうです。","2. RaspberryPi に接続","MicroSD カードを RaspbeeryPi に挿入し、電源をつけたら、下記を実施します。 LAN ケーブルは、自宅の WiFi に直接つなげます。(私の場合は SoftBankAir) hostname は、お好みの名前にします。（私は、","としました。)","※ 2 回目以降は、","をしましょう。 電源を落として、LAN ケーブルを外します。再度電源をつけて数分待ってから、下記を実施します。","接続できたら成功です。","3. 各種インストール","おまじないをします。","Docker をインストールします。","Kubernetes をインストールします。","4. MasterNode の設定","MasterNode にする RaspberryPi に対して下記を実施します。","出力される join メッセージをメモしておき、WorkerNode の構築時に使います。","に従い下記を実行します。","5. WorkerNode の設定","MasterNode から出力された join コマンドを実施します。","6. MasterNode から確認","Node が増えているか確認します。","7. ブラウザから確認","試しにデプロイ → サービス公開 → ブラウザ確認までを、さっと通してみます。","サービス公開までしたので、アクセスしてみます。 内部","外部","にアクセス OK!","お片付け","完成","すんなりと構築することができました。これは先人たちの記事がたくさんあるので、 サクサクと進めることができました。これで、Kubernetes を使いまくります!! 💪💪 次回は","です。"],"t":"一足遅れて Kubernetes を学び始める - 03. Raspberry Pi -"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_01/index.mdx","c":["一足遅れて Kubernetes を学び始める - 01. 環境選択編 -","import { Image } from \"~/components/image/image\";","ストーリー","経緯","Kubernetes を使えるようになりたいな〜（定義不明） けど、他にやりたいこと（アプリ開発）あるから後回しにしちゃえ〜！！ と、今までずっと、ちゃんと学ばなかった Kubernetes を、本腰入れて使ってみようと思います。✨","環境","私の知識レベルは、 「Kubernetes はコンテナオーケストレーションしてくれるやつでしょ」というざっくり認識で、関連用語は耳にしたことがあるだけで、よく理解できていません。","最初、何から始めよう？","マネージドサービスの GKE 使ったほうが、最初は楽で簡単だから、そっちを使ったほうが良いみたいです。 😍","GKE SetUp","ノードってのは、ポッド（コンテナ）を入れるマシンなんだっけな。 (",") まあ、デフォルトで良いよね 🤔 単語がどれも分からなさすぎる...(Istio?自動プロビジョニング?垂直ポッド自動スケーリング？) 🤔🤔🤔","Mac で Kubernetes 試せるから、そっちで学んでいこう...","ちょっと意味がわからない状態で、GKE 動かしたらお金がかかる上に、何してるのか分からないから、もったいない。 Docker For Mac に Kubernetes 使えるみたいだから、まずはそっちを使って学んでいこうかな。。。 💪 次回は","です。"],"t":"一足遅れて Kubernetes を学び始める - 01. 環境選択編 -"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_04/index.mdx","c":["一足遅れて Kubernetes を学び始める - 04. kubectl -","ストーリー","前回","では、RaspberryPi の環境に Kubernetes を導入しました。無事、動作確認ができたので、さっそく学習していきたいです。","参考","「","」を読んで進めてみます。ソースコードは","。 以前の投稿では、","を参考にしていましたが、Kubernetes 完全ガイドの方が網羅的に学べて良かったで、そちらを使いました。","kubectl","Kubectl is a command line interface for running commands against Kubernetes clusters","※ https://kubernetes.io/docs/reference/kubectl/overview/ kubernetes を操作するための CLI です。 よく使うものを私なりに整理し、入門時に最小限覚えておけば良いものをまとめました。","1. apply","Kubernetes では、基本的にはマニフェストファイルを作成し、","で適用するのが一般的のようです。それは、新規作成だけでなく、更新や削除も同様です。","や",",","といった CLI もありますが、","でも同様の操作ができるため、使い分ける必要はあまりありません。","で登録したマニュフェストファイルは履歴として保存されています。 ※ ","2. set, get","kubectl では、どのリソース種類（",",etc）で、どのリソース名なのかを教えてあげる必要があります。 また、フィルタリングする機能として","があります。","更に詳細の情報が必要な場合は、","を使います。","※ ","という直接編集する方法もありますが、一時的な対応のみに利用するべきとのことです。 せっかくの宣言的ファイルが意味ないですよね。 余談ですが、","を","という風に省略できたりします。 ※ ","3. debug","どれも","に対する操作なためリソース種類の指定はありません。どれも開発時に必要が迫られれば使う感じですね。","99. top","こちら、どうしても動作できませんでした... 😥😥 今はそこまで必要としないので、一旦見送ります。 ","だか","とかが関係しているっぽいのですが、理解が浅いため未解決です。","お片付け","複数の pod を扱っているなら、","よりも","の方が良いですが、今回は単体 pod なので、直接","しました。","おわりに","入門当初は、どれほど覚えなくてはいけないのかと不安になっていたのですが、 蓋を開けてみると、そこまで多くはありませんでした。（まだ知らないものは多いと思いますが） 規則性として、 リソース種類とリソース名を指定する習慣にも徐々に慣れてきました。 面倒なときは、","で全部出すという荒業も覚えました。（笑） 次回は","です。"],"t":"一足遅れて Kubernetes を学び始める - 04. kubectl -"},{"f":"src/routes/blog/contents/replace_my_portfolio_v2/index.mdx","c":["silverbirderのポートフォリオページ刷新(v3)","import { Image } from \"~/components/image/image\"; この度、私のポートフォリオページを刷新致しました。本記事では、 刷新することになった動機から、刷新内容、今後について紹介したいと思います。","動機","元々、私のポートフォリオページは、静的ページジェネレーターである Hugo を使って 構築していました。 http://kohki.hatenablog.jp/entry/hugo-portfolio こちらの記事を参考にして、Hugo でポートフォリオページを作りました。 その当時、なぜポートフォリオを作ったのかというと、確か次の 3 つの思いがありました。","私がどういった人かを知ってもらいたい 自分のサイトを持ちたい 静的ページジェネレーターを使ってみたい","Hugo で記事を管理する対象は、Markdown であるため、エンジニアにとって書きやすいです。 また、デザインテーマは、公開されているテーマがあるので、好きなものを選びます。 導入当初は、とても快適でした。手軽にオシャレなポートフォリオサイトを公開できて満足でした。 しかし、ずっと使っていると、かゆいところに手が届ないもどかしさを感じるようになりました。 これは、便利さとのトレードオフだと思いますが、下記のようなデメリットがあると認識し始めました。","Javascript で技術的な挑戦が難しい デザインテーマのカスタマイズが難しい SEO のチューニングが難しい","便利さというメリットよりも、デメリットの方が大きいように思い始めました。 そのため、独自にポートフィリオサイトを作成することにしました。","やったこと","AMP を存分に使ったポートフォリオサイトを作成しました。全体像は、下記のとおりです。","を中心とした構成です。 ソースコードは、下記のリポジトリにあります。 https://github.com/silverbirder/silverbirder.github.io","技術選択","今回のポートフォリオサイトに、必要以上の機能を持つ Web フレームワーク(e.g. Next.js)を使うのは、メンテナンスコストが高くなるので、却下としました。 また、静的ページジェネレーター(e.g. Gatsby)も、動機の理由より却下としました。 そのため、必要最小限な構成を目指しました。結果、次のような流れとなりました。","コンテンツを用意する(Markdown,HTML,JSON) 1 をインプットとして","で AMP 化する","これらの順序を制御するタスクランナーとして、"," を採用しました。 ","は、NPM でインストールするので、Node.js と相性が良いタスクランナーを求めました。 その選択肢として、Grunt や Gulp があったのですが、","ので、Gulp を選択しました。 大きな技術選択としては、これくらいです。他の細かい所は、下記のとおりです。","highlightjs","プログラムコードのハイライト機能","jsdom","html の各処理","h1~h6 タグの Anchor 設定(anchorJS 風) HTML のテンプレートとメインコンテンツの Mix ...etc","ampcssframework","Dark Theme や Grid 機能が欲しかった","Cloudinary","画像管理 SaaS。OGP などに利用","SEO 向け","Google search console Google analytics","ポートフォリオコンテンツ","ポートフォリオサイトにどういったコンテンツを用意しようか悩みました。 ","があります。 これは、HTML だけではなく、Markdown も(HTML 経由で)AMP 化することができるようです。 そのため、ブログのようなコンテンツもポートフォリオページに加えることができそうと気づきました。 また、これまで私が書いたブログコンテンツは、Markdown で管理していたので、ちょうど使えそうでした。 結果、次のようなコンテンツを用意しました。","自己紹介 ブログ 持っている本検索 買ったものリスト","アマゾンで買ったもの、サブスク","ウォッチ","チェックしてる RSS","プロジェクト","作ったものの紹介","ウォッチページで、RSS の WebPush 機能を追加しようとしましたが、Push する側である Server が必要となり、開発が伸びそうだったのでやめておきました。","刷新してどうだったか","想定通り、Hugo ではできなかったような様々なポートフォリオサイトの機能拡張ができるようになりました。","Javascript で技術的な挑戦が難しい","AMP や、Web Worker(amp-script)を試せた","デザインテーマのカスタマイズが難しい","CSS フレームワークや、CSS のチューニングができた","SEO のチューニングが難しい","SearchConsole や GoogleAnalytics が使えた sitemap や meta タグのチューニングができた","想定通りにできなかったのは、AMP の制約なのですが、WebComponents のような amp-script 上で動かせない技術もあるということでした。 また、WebWorker(amp-script)上で、ES Module(",")を Import しようとしても、Safari が未対応だったりで、断念したりもしました。 ただ、最終的な感想としては、HTML を柔軟に処理できるようになったので、AMP 上でできることは何でもできるようになり、刷新してよかったと思います。","学んだこと","経験学習モデルより、簡単に振り返ります。(はじめて) | 経験 | 省察 | 概念化 | 試行 | | ---------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- | ------------------------------------------------------ | -------------------------------------------- | | AMP を初めて使ってみた | AMP 使ったことなかったけど、思っていたより課題は少なかった。 しかし、想定していなかった課題もあった。 | 使ったことがない技術要素の課題は、想定していても未知数 | 未知数な技術は、軽く試してみる | | Next.js や Gatsby など、フレームワークを使わなかった | シンプルな構成にしたかった。 必要以上に機能が多いフレームワークを入れたくなかった。 | 保守性を担保するため、最小限の機能で構成 | 大掛かりな技術の選択は、保守性と天秤にかける |","終わりに","ポートフォリオ刷新をしました。これまで 1 から Web サイトを作ったことがなかったので、 sitemap や JSON-LD など全て手作りで開発したので、良い勉強になりました。 まだまだポートフォリオの課題は山積みですが、少しずつ改善していきたいと思います。"],"t":"silverbirderのポートフォリオページ刷新(v3)"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_06/index.mdx","c":["一足遅れて Kubernetes を学び始める - 06. workloads その2 -","ストーリー","前回","では、Pod,ReplicaSet,Deployment の３つを学習しました。今回は DaemonSet,StatefulSet(一部)を学びます。","DaemonSet","ReplicaSet とほぼ同じ機能のリソース。 ReplicaSet との違いは、各ノードに 1 つずつ配置するのが DaemonSet,バラバラなのが ReplicaSet。 用途として、モニタリングツールやログ収集の Pod に使うそうです。 さっそく、試してみます。","ReplicaSet と大きく違いはありません。 また、各ノードに対して pod が作られていることがわかります。 Deployment と似ているアップデート戦略があり、OnDelete と RollingUpdate(デフォルト)があります。前者は、pod を明示的に削除した(",")際に更新する戦略です。DaemonSet は、死活監視やログ収集に使うので、手動でのタイミングが効く OnDelete が好まれます。後者は、Deployment と同じ動きで、即時更新していく戦略です。 ReplicaSet と似ているようで、機能的には Deployment に近い感じですね。ReplicaSet は pod が削除されたら複製されますけど、アップデートされません。DaemonSet は pod が削除されたら複製するし、アップデートもされます。試してみます。","nginx のバージョンを 1.12 から 1.13 に変更しました。","apply してみると、一台ずつ update されています(containerCreating)。Deployment と違うのは、最大 pod 数が１のために、一時的に pod が機能しなくなるタイミングが生まれます(超過分の設定不可)。","pod を削除しても、セルフヒーリングで復活します。","StatefulSet","ステートレスな pod ではなく、DB のようなステートフルな pod 向けのリソース。 pod を削除しても、データを永続的に保存する仕組みが存在。 動作自体は、replicaSet と似ています。 さっそく、試してみます。","mountPath で指定したマウントしたいパスを、volumeClaimTemplates でマウントしてくれます。 どこに？ Storage に関しては別で学習することにします。 ひとまず、apply します。","おや、Pending になってしまいました。 ","PersistentVolume と PersistentVolumeClaims","PersistentVolume(永続的ボリューム)は、名前の通りで、データを永続的に保存しておく場所のリソースです。 マネージドサービスを利用すると、デフォルトで PresistentVolume が用意されているそうです。 私の環境は、マネージドサービスではなく、自作環境であるので、PresistentVolume を用意する必要があります。 PersistentVolumeClaims(永続的ボリューム要求)は、これも名前の通りで、「PresistentVolume を使わせて」というリソースです。 このリソースで、PresistentVolume の name を指定し、apply することで、初めてマウントができます。 例えば、Pod から PersistentVolumeClaims の名前を指定してあげると、その Pod は Claim した PersistentVolume をマウントすることができます。 volumeClaimTemplates というのは、「わざわざ PersistentVolumeClaims を定義しなくてもテンプレートに沿って書けば Claims できるよ」というものです。","で、何が問題だったの？","のとおりで、「PersistentVolume の要求をしたけど、Volume 割当できなかったよ」とのことです。 PersistentVolume(pv)があるのか確認してみます。","たしかにないです。PersistentVolume を用意しないといけないのですが、どうしましょう。 解決手段として考えたのは 3 点です。","GCP や AWS,Azure のサービスを使う LocalVolume を使う NFS を使う","※ ","1 は、書いておいてなんですが、却下です。理由は、せっかく raspberryPi で構築したのでクラウドサービスを利用したくないからです。 2 は、","の参考にして","試した","のですが、 記事にも書いてあるとおり「Local Volume は他の Pod から共有で利用することができない」ため、statefulset が","でなければ動きません。それはそれで動くので学習になり良いのですが、せっかくなら replica の制限なしにしたいです(ReadWriteMany にしたい)。 3 は、もう一台 raspberryPi を用意して、それを NFS と見立てて PersistentVolume にしてみる方法です。 3 を進めようと思います。","NFS 導入","サーバ設定","NFS 用の新たな raspberryPi を用意します。設定手順は","を参考にしました。 その後の続きは下記です。 NFS のホスト名は","とします。","意味としては、「指定範囲の IP アドレスからのマウントを許可する」。オプションは、","を参照。 | host | ip | | ---------------- | ------------ | | iMac | 192.168.3.3 | | raspi001(master) | 192.168.3.32 | | raspi002(worker) | 192.168.3.33 | | raspi003(worker) | 192.168.3.34 | | nfspi(NFS) | 192.168.3.35 |","正しく設定されたか、iMac から確認してみます。","OK","クライアント設定","各ノードに対して下記を実行します。","nfs-client 導入","raspberryPi 環境では、真っ白な状態なので、一から PersistentVolume を用意する必要があります。それには Volume となる Storage の型を用意する必要もあるのですが、","を見る限り、NFS 用の型は標準で存在しません。そこで、","を使って NFS 用の StorageClass を作成します。","rbac.yaml にある namespace を現在動かしている環境の namespace に置換して、apply しています。","deployment-arm.yaml では、NFS サーバの IP アドレス(192.168.3.35)とマウントパス(/home/data)を設定しました。 class.yaml が、今回欲していた NFS の storageClass(managed-nfs-storage)になります。 ※ raspberryPi のイメージは Raspbian を使っているので、arm 用の deployment-arm.yaml を使います。"," これに随分とハマってしまいました...","試しにマウント先にファイルが作成できているのかテストしています。確認します。 nfspi に移動","あれば成功です。あれば、下記で片付けます。","statefulset をリトライ","以上で、StorageClass を用意できました。よって後は、PersistentVolume 作って、PersistentVolumeClaim 作って...となる予定でした。 しかし、","には、","dynamic provisioning","という機能が備わっており、PersistentVolume を作らなくても、PersistentVolumeClaim するだけで良くなります。この件については、storage を学習する際に書きます。 raspi001 に移動して、sample-statefulset.yaml をもう一度 apply します。 (storageClassName: managed-nfs-storage を追加, ReadWriteOnce→ReadWriteMany に変更)","nfapi に移動して、あるか確認。","ありました！ マウントできています！","お片付け","でも良いのですが、下記のほうが使いやすかったです。","と","を試して頂き、今回作ったリソースがありましたら削除お願いします。","おわりに","StatefulSet を使える状態にするまでに記事が大きくなってしまいました。次回に詳しく学んでいこうと思います。笑 あと、","を見て思ったのが、kubernetes のパッケージマネージャである helm を導入した方が、遥かに便利だと思いつつ、手動設定しました。。。 次回は、","です。"],"t":"一足遅れて Kubernetes を学び始める - 06. workloads その2 -"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_05/index.mdx","c":["一足遅れて Kubernetes を学び始める - 05. workloads その1 -","ストーリー","前回","では、kubenetes の CLI ツール kubectl を学習しました。 今回は、目玉機能である workloads について学習します。","workloads","Kubernetes には、下記のようにリソースの種類が存在します。 今回は、Workloads を学習します。 | リソースの分類 | 内容 | | :----------------------- | :----------------------------------------------------------- | | Workloads リソース | コンテナの実行に関するリソース | | Discovery＆LB リソース | コンテナを外部公開するようなエンドポイントを提供するリソース | | Config＆Storage リソース | 設定・機密情報・永続化ボリュームなどに関するリソース | | Cluster リソース | セキュリティやクォータなどに関するリソース | | Metadata リソース | リソースを操作する系統のリソース | ※ ","Workloads には、下記 8 つの種類があります。","Pod ReplicationController ReplicaSet Deployment DaemonSet StatefulSet Job CronJob","Pod,ReplicationController,ReplicaSet,Deployment までを見ていきます。","Pod","コンテナを 1 つ以上含めた最小単位のリソース。 Pod 毎に IP アドレスが振られる。ボリュームは共有。 基本的に、Pod にコンテナを詰め込めるのではなく、「分離できるなら、分離する」方針がマイクロサービスとして良いそうです。 さっそく、動かしてみます。 ※ ","期待通り複数のコンテナが動いていますね。(READY 2/2) exec で中に入る場合、どうなるのでしょうか。","なるほど、デフォルトのコンテナ（spec.containers の先頭)に入るみたいです。 redis-container に入る場合は、","でコンテナを指定するだけみたいです。 他にも説明したいことがありますが、長くなりそうなので切り上げます。","ReplicaSet, ReplicationController","レプリカという名前だけあって、Pod を複製するリソース。 過去の経緯から ReplicationController から ReplicaSet へ名前変更があったため、ReplicaSet を使うことが推奨 さっそく、動かしてみます。","確かに、replica3 つ(sample-rs)で、それぞれコンテナが２つ(READY 2/2)作れていますね。 書いて気になるのは、 pod の apiVersion は、","に対して、replicaSet の apiVersion は、 ","というのが気になりましたので、調べてみたところ、","という記事を見つけました。 Core となる機能は、","で良いみたいです。 Kubernetes の目玉機能であるオーケストレーションの機能であるセルフヒーリングを試してみます。","おー、ContainerCreating されています。良いですね〜。 ちなみに、気になったのは node 自体が故障してダウンした場合は、どうなるのでしょうか。試してみます。","raspi003 の電源を落としてみます。 worker(raspi003)に移動","master(raspi001)に移動","ん？ raspi003 で動いている？ 数十秒後...","おー、期待通り raspi003 にある pod が消えて、raspi002 に作り直されました。sample-rs-nsc5b は node が落ちちゃっているので、消すこともできず残り続けます。","少し待ち時間が長いような？","という記事によれば、kube-controller-manager が検知して、kube-scheduler が正しい数に揃えているみたいです。","数十秒待たされた","のは、検知の間隔のせいでしょうか。","のオプションで、","とあります。","1 分間隔","なのですかね。","Pod を特定の Node で動かさないようにしたい","みたいな要望を叶えれるのでしょうか。","によると、nodeSelector フィールドでアサインされる node を指定できるそうです。（除外ではなく、指定） ただし、","によると、それは replicaSet ではなく、deployment で行うべきとのことです。replicaSet で動くかどうか、念の為試してみます。 まず、先程落とした raspi003 を電源を入れ直して起動させます。 その後、master(raspi001)に移動。","node にラベルを貼って、nodeSelector しやすいようにしました。 sample-rs は、全て raspi002 で動いているので、下記を試してみます。","sample-rs は raspi002 でのみ動くよう設定 raspi002 をシャットダウン","その結果、「sample-rs は raspi002 が動いていないので、セルフヒーリングしない」ことを期待します。","nodeSelector を追加しました。 今回は単純な指定なのでこれで良いですが、より柔軟に指定したい場合は nodeAffinity を使うそうです。 worker(raspi002)に移動","数十秒待つ... 結果は...! master(raspi001)に移動","期待通りでした。つまり、sample-rs は raspi002 以外で作り直せないので、Pending,Terminating 状態です。 また、単純な pod である sample-2pod は replicaSet ではないので、セルフヒーリングされずに Terminating になっています。 面白いですね。これ。","Deployment","複数の ReplicaSet を管理。 ReplicaSet にない「ローリングアップデート、ロールバック」機能が存在。 Pod や ReplicaSet ではなく、Deployment が最も推奨されるリソース種類。 ReplicaSet では、指定したコンテナイメージを更新した場合(アップデート)、どうなるのでしょうか。すべて更新されるのか、一部だけなのでしょうか。試してみます。 sample-2pod-replica.yaml の nginx イメージを 1.12 から 1.13 に更新しました。","replicaset のマニュフェストは更新されました。","pod は変化なしのようです。 では、Deployment を使ってみます。","をつけることで、履歴を保持することができます。ロールバックに使います。","sample-deployment が、deployment,replicaset,pod を作成しました。 では、sample-deployment の nginx コンテナを 1.12 から 1.13 に更新してみます。","おー、deployment の pod が作り変わっていっています。これが","ローリングアップデート","です。 ローリングアップデートは、spec.template 以下が更新されると変化したとみなすそうです。 また、ロールバックは、rollout コマンドで実施できますし、revision 指定で戻すこともできます。 しかし、基本的にはマニュフェストを戻して apply すべきです。 アップデート戦略というものがあり、デフォルトは RollingUpdate です。過不足分の Pod 考慮した更新戦略になります。 アップデート中に許容される不足分と超過分を設定できます。(maxUnavailable, maxSurge) 他の戦略として、Recreate 戦略があります。こちらは、全て同時に作り直しになります。ですので、一時的にアクセス不可になってしまいます。 １つ不安に感じたものとして、「フロントエンドのバージョンを１から２にアップデートしたら、バージョン 1 のコンテナにアクセスしたユーザがバージョン 2 のコンテナに遷移したら大丈夫なのかな」と思いました。しかし、これはローリングアップデートに限った話ではないので、それは考えないこととしました。ちゃんと設計すれば良い話ですね。 ちなみに、マニュフェストを書かずに deployment ができます。","です。お試しなら、便利ですね。","お片付け","試しに、prune で削除しています。","んー、こうすると消せるのですが、どうしても１ファイル残してしまいます...。 すべて org にすると、","が失敗しますし...。","結局、こうしました...。","おわりに","思った以上に、ReplicaSet にハマってしまいました。 次は、残りの workloads を試します。 次回は","です。"],"t":"一足遅れて Kubernetes を学び始める - 05. workloads その1 -"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_07/index.mdx","c":["一足遅れて Kubernetes を学び始める - 07. workloads その3 -","ストーリー","前回","にて、DaemonSet と StatefulSet(一部）を学習しました。今回は、StatefulSet の続きと Job,CronJob を学習します。","StatefulSet","永続的にデータが保存されるかどうか確認します。","sample.html というファイルを作りました。こちらが消えるかどうか確認します。","pod を消してセルフヒーリングで復活した後、確認すると、sample.html 残っています。","こちらも残っていますね。OK です。","スケーリング","StatefulSet では、スケールアウトするときは、インデックスが小さいものから増えていきます。 逆にスケールインするときは、インデックスが大きいものから削除されていきます。 また、１つずつ増減します。そのため、一番始めに作られる Pod は、一番最後に削除されることになります。 試してみます。","期待通りですね。１つずつではなく、並列して作成したい場合は、spec.podManagementPolicy を parallel にすれば実現できます。","アップデート戦略","戦略は２通りあり、OnDelete と RollingUpdate があります。前者は、削除された（マニュフェスト更新ではなく、delete）タイミングに更新され、後者は、即時更新します。StatefulSet の更新では、アップデート中の過不足分の調整(maxUnavailable, maxSurge)は一切できません。また、partition というフィールドのによって、どのインデックス以降を更新するかを調整することもできます。これは、ステートフルならではの機能です。 Deployment では試してませんでしたが、こちらで試してみようと思います。 デフォルトの戦略は RollingUpdate です。これは何度も動作して確認できているので、OnDelete を試そうと思います。(partition は置いとく）","アップデート戦略を OnDelete にし、nginx イメージを 1.12 から 1.13 に更新しました。","期待通りですね。明示的に pod を削除すれば nginx が更新されました。","Job","一度限りの処理を実行させるリソース。 replicaSet のように複製ができる。 バッチ処理に向いている。 10 秒 sleep するだけの job を実行してみます。","job の実行が終わると、pod が消えていますね。そして、job の COMPLETIONS が 1/1 になっているので正常終了したみたいです。逆に正常終了しなかった場合、restartPolicy に沿って再実行することになります。種類として Never と OnFailure があります。Never は、新規に Pod を作って再実行、OnFailure は、既存 Pod を使って再実行するそうです。ただし、データ自体は消失することになるので、ご注意下さい。 completions は目標成功数で、parallelism は並列数、backoffLimit は失敗許容値です。 目的に合う設定にすれば良いですね。 また、completions を未指定にすると job を止めるまでずっと動き続けます。backoffLimit を未指定にすると 6 回までとなります。 んー、特に興味が惹かれることもなく、終わります。笑","CronJob","Job をスケジュールされた時間で実行するリソース。 Deployment と ReplicaSet の関係と似ていて、Cronjob が job を管理する。 1 分毎に 50%の確率で成功する job を用意して、試してみます。","時間がくるまで、job,pod は作成されないようです。 数分待ってみました。","名前の命名ルールがあるので、どう関連しているのか一目瞭然ですね。 Pod が残っているのは、failedJobsHistoryLimit と successfulJobsHistoryLimit の値の影響ですね。 log で確認できるように残しておくそうですが、ログ収集基盤に集約した方が良いとも言われています。 途中で止めたいときは、spec.suspend を true にすることで実現可能になります。 同時実行する制限として、concurrencyPolicy があり、Allow,Forbid,Replace があります。 Allow は、特に制限しない。 Forbid は、前の job が終わらない限り実行しない。 Replace は、前の job を削除し、job を実行する。 遅延がどのぐらい許容できるかは、startingDeadlineSeconds で指定します。 こちらも、特に何事もなく終わりました。笑","お片付け","終わりに","ようやく、workloads が終わりました。最後はざっくり進めてしまった感がありました。 次回は","です。"],"t":"一足遅れて Kubernetes を学び始める - 07. workloads その3 -"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_09/index.mdx","c":["一足遅れて Kubernetes を学び始める - 09. discovery&LB その2 -","import { Image } from \"~/components/image/image\";","ストーリー","前回","で Service についての概要を学びました。今回は下記を一気に学びます。","ExternalIP NodePort LocadBalancer Headless ExternalName None-Selector Ingress","※ ClusterIP を飛ばしたのは、前回使った内容で十分だと思ったため。","ExternalIP","こちらは、外向けの IP アドレスを割り振ります。","私の Node 情報では、下記の状態です。 | host | ip | | :--------------- | :----------- | | raspi001(master) | 192.168.3.32 | | raspi002(worker) | 192.168.3.33 | | raspi003(worker) | 192.168.3.34 | | nfspi(NFS) | 192.168.3.35 | ここで、spec. externalIPs に、公開したい IP アドレスを上記 Node の IP アドレスより設定します。 今回は、１つだけ(raspi002:193.168.3.33)にしました。","前回同様のファイルを用意します。","externalIP が設定されました。","どこの pod かどうかわかりやすいくするため、index.html を書き換えます。 では、ブラウザからアクセスしてみます。 raspi002 を公開したので、その Node に存在する Pod がランダムに出力されている、つまりロードバランサが動作していることがわかります。","NodePort","ExternalIP のような特定 Node を公開するのと違って、NodePort は、","全て","の Node を公開します。 試してみます。","内向けには、10.96.173.243:8080 でアクセスでき、外向けには、各 Node の IP アドレス:30080 にアクセスします。 どちらも正常にアクセスできています。もちろん、アクセス先の Pod は、ロードバランシングされます。ロードバランシングさせるのが嫌な場合にも対応できます。アクセスされた Node の先は、その Node 内にある Pod のみにアクセスさせる「spec.externalTrafficPolicy：Local」に設定すれば大丈夫です。 注意点として、nodePort は、30000~32767 の範囲と決まっています。","LoadBalancer","ExternalIP や NodePort の場合、ロードバランシングするのはクラスタ内の Node になります。そのため、アクセスが集中することで、Node 単一障害が発生しやすいそうです。そこで、LoadBalancer を使うことで、","クラスタ外","にロードバランサを作成します。 ただ、クラスタ外にロードバランサを作成する際は、プラットフォームによって対応しているか確認が必要です。私のような raspberryPi 環境では、もちろんそういった機能がないため、準備する必要があります。 master(raspi001)に移動","metallb と呼ばれるロードバランサを適用します。 https://metallb.universe.tf","MetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols.","これで、raspberryPi 環境でも loadBalancer が使えます。さっそくつかってみましょう。","お、192.168.3.100:8080 にアクセス可能みたいです。 iMac に移動","OK","Headless","今までのロードバランスと違い、公開する IP アドレスは提供されません。 DNS ラウンドロビンによる転送先の Pod の IP アドレスを取得できます。 つまり、Headless のサービスへ問い合わせすると、spec.selector で登録した Pod の IP アドレスが手に入ります。 Pod の IP アドレスがほしいときには便利です。（Envoy とか?）","spec.type が ClusterIP であり、spec.clusterIP が None、そして、metadata.name が statefulset の spec.serviceName と同じことで、Headless Service と呼ぶそうです。","たしかに、headless のサービスに問い合わせると、IP アドレスが返ってきました。","ExternalName","外部のドメイン宛の CNAME を返すサービスです。 例えば、Pod から外部の","へアクセスする場合、下記のように設定します。","確かに、","と問い合わせすることで、外部の","への CNAME を取得できます。また、外部のサイトを切り替えたいときは、問い合わせ先は","変わらず","に、sample-externalname.yaml の spec.externalName を変更するだけで済みます。これは切り替えが楽ですね。","None-Selector","外部のサービスに対してロードバランシングします。","172.217.31.164 と 172.217.31.165 は、どちらも","を指します。","ClusterIP なので、内部で公開されていますね。","少し結果が不自然だったのですが、確かに google.com へアクセスしました。 外部サービスへのロードバランシングも容易に実現できます。 ※ ","へアクセスすると、リダイレクトがかかります。","Ingress","今までのロードバランサは、l4 レイヤーのロードバランサです。(IP アドレスとポート番号による負荷分散) Ingress では、l7 レイヤーのロードバランサを提供します。(URL や HTTP ヘッダーで負荷分散が可能) Ingress を置く場所は、クラスタ内、外の２つあります。 クラスタ外の場合は、使うプラットフォームによります。 クラスタ内の場合は、Nginx Ingress を使うことができます。 raspberryPi 環境では、Ingress-Nginx-Controller を使うことで、Ingress を使えるそうです。 "," を参考にして進めたのですが、arm64 環境では動きませんでした。 そこで、下記の yaml を発見し、試してみると動作します。ぜひ、お試しあれ。 ","※ namespace を削除できない場合は、","を参考下さい。","お片付け","最後に","Service について学びました。 様々な用途に応じて、エンドポイントを公開する手段を学びました。 手を動かして確認してみると、理解が深まりました。 本番で k8s を使った経験はありませんが、今後必要に迫られた際に、こちらの記事を思い返そうと思います。 次回は","です。"],"t":"一足遅れて Kubernetes を学び始める - 09. discovery&LB その2 -"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_08/index.mdx","c":["一足遅れて Kubernetes を学び始める - 08. discovery&LB その1 -","import { Image } from \"~/components/image/image\";","ストーリー","前回","でようやく workloads が終了しました。今回は、discovery&LB を進めようと思います。","discovery&LB","Kubernetes には、下記のようにリソースの種類が存在します。 今回は、discovery&LB を学習します。 | リソースの分類 | 内容 | | :----------------------- | :----------------------------------------------------------- | | Workloads リソース | コンテナの実行に関するリソース | | Discovery＆LB リソース | コンテナを外部公開するようなエンドポイントを提供するリソース | | Config＆Storage リソース | 設定・機密情報・永続化ボリュームなどに関するリソース | | Cluster リソース | セキュリティやクォータなどに関するリソース | | Metadata リソース | リソースを操作する系統のリソース | ※ ","discovery&LB をには、下記 8 つの種類があります。","Service","ClusterIP ExternalIP NodePort LoadBalancer Headless (None) ExternalName None-Selector","Ingress","Service の概要について学びます。","Kubernetes とネットワーク","Kubernetes では、Pod 毎に IP アドレスが割り振られています。 そのため、異なる Pod 間で通信する際は、Pod の IP アドレスが必要になります。逆に同一の Pod 内なら localhost で通信できます。 説明するために、準備します。","このような状況下で、","を中心に見ていきます。 ※ nginx は 80 ポートで開放されています。","前準備","curl がないのでインストールします。","同一 Node,同一 Pod 内のコンテナへ通信","OK","同一 Node,異なる Pod のコンテナへ通信","異なる Node,異なる Pod のコンテナへ通信","MasterNode から各 Pod へ通信","OK ここから分かるように、Pod 内部の通信、Pod 間の通信、さらに Node 間の通信までも、Kubernetes によってネットワークが構築されています。","Service は、下記の２つの大きな機能が存在します。","pod 宛トラフィックのロードバランシング サービスディスカバリとクラスタ内 DNS","pod 宛トラフィックのロードバランシング","先程の例で、Pod 間を通信することは可能です。しかし、pod を作り直すたびに IP アドレスが変わってしまうため、 自作すると、少し大変です。そこで、Service の出番です。 サービスは、複数存在する Pod に対して","自動的にロードバランスしてくれる","のと、合わせて","外向けの IP アドレス","(ExternalIP)や、","内向けの IP アドレス","(ClusterIP)も提供してくれます。 さっそく、試してみます。","これは、","にマッチする Pod に対してロードバランスしてくれます。外から 8080 ポートで待ち受けて、80 ポートでコンテナへ通信します。 spec.type が ClusterIP なので、内向けの IP アドレスが提供されています。","内向けに 10.111.197.69 の IP アドレスが振られました。また、ロードバランスする対象 Pod は、先にあげた Pod の IP アドレスです。 Endpints に","とあるように、port 毎にサービス(clusterIP)を作ることもできます。(service の spec.ports は配列指定） アクセスできるのか、試します。 せっかくなので、pod 毎に index.html の内容を変化させましょう。","確かに、ロードバランシングによって pod に適度なランダム具合でアクセスできています。 もちろん、外からはアクセスできません。 iMac へ移動","サービスディスカバリとクラスタ内 DNS","サービスディスカバリとは、「問題においての解決策」を指しています。 Kubernetes における問題とは、動的にサービスが生成され続けていることによるサービスを特定することが難しくなる問題です。 そのサービスディスカバリが、Service にあります。 その方法について下記があります。","環境変数を利用したサービスディスカバリ","Pod に IP アドレスや port,protocol が設定されている。","DNS A レコードを利用したサービスディスカバリ","Kubernetes 内のクラスタ内 DNS によって、ドメイン名によるアクセスができる。(ドメイン名の命名規則に従う)","DNS SRV レコードを利用したサービスディスカバリ","IP アドレスからドメイン名を取得する逆引きもできる。","dnsPolicy による明示的な設定がない限り、Pod 生成時にクラスタ内 DNS へレコード追加されます。 クラスタ内 DNS で名前解決できなかった場合は、クラスタ外 DNS に問い合わせします。","お片付け","最後に","今回は、Service についての概要を学びました。Kubernetes の世界では、自動的にネットワーク構築されているため、特段意識することはありませんでした。 もう少し理解が進めれば、ネットワークがどのように構築されているのか、クラスタ内 DNS がどのように動いているのか知りたいと思います。 次回は、","です。 ※ お絵かきしてアウトプットすると、理解が深まるのでおすすめです。"],"t":"一足遅れて Kubernetes を学び始める - 08. discovery&LB その1 -"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_11/index.mdx","c":["一足遅れて Kubernetes を学び始める - 11. config&storage その2 -","ストーリー","前回","では、config について学習しました。 今回は、storage を学びます。","Volume と PresistentVolume","Volume は、あらかじめ決められた利用可能なボリュームを指します。こちらは、ボリュームの削除や新規作成ができません。 PresistentVolume は、外部にある永続ボリュームを指します。こちらは、ボリュームの削除や新規作成ができます。 DB のようなステートフルなものは PresistentVolume を使います。 一時的なものなら、Volume を使うのですかね？ ※ PresistentVolumeClaim は、PresistentVolume をアサインするためのリソース。","Volume の種類","書籍（kubernetes 完全ガイド）で紹介されていた Volume の種類は、下記のとおりです。","emptyDir","一時的なディスク領域を利用 pod 削除されると、emptyDir も削除 マウント先を指定できない","hostPath","emptyDir のマウント先を指定できる版","downwardAPI","Pod の情報をファイルとして配置したファイルをマウント","projected","secret/configMap/downwardAPI/serviceAccountToken を１つにまとめたディレクトを作成し、マウント","※ ","Volume を残すことができないので、Pod を削除する際は気をつけないとダメですね。 ログをファイルとして保存するなら、一時的に Volume が良いのですかね。 ただ、定期的に外部ストレージに移さないといけないですので、手間です。 （そもそも、ログはストリームにして外部サービスに流すのがベスト） プロダクトとしては、あんまり使い道ない...?","PresistentVolume の種類","外部の永続ボリュームを利用します。例えば、下記の種類があります。","GCE Persistent Disk AWS Elastic Block Store NFS iSCSI Ceph OpenStack Cinder GlusterFS","では、NFS を使いましたね。 PersistentVolume の作成方法は、外部の永続ボリュームによって違うのですが、共通して言えるところもあるみたいなので、 そこを書いてみます。","ラベル","PersistentVolume をラベリングすることで、指定しやすくする","容量","Volume で要求する容量。最も小さい容量からアサインされる。","アクセスモード","ReadWriteOnce","単一ノードから Read/Write が可能","ReadOnlyMany","複数ノードから Read が可能","ReadWriteMany","複数ノードから Read/Write が可能","Reclaim Policy","Volume を使い終わったあと、破棄するか再利用するかのポリシー","Delete","PersistentVolume の実体を削除","Retain","PersistentVolume の実体を残さず保持 再度マウントされない","Recycle","PersistentVolume のデータを削除し、再利用可能にする 再度マウントされる （廃止予定で、DynamicProvisioning を利用すること)","StorageClass","各プロバイザーが提供するストレージの型","基本的に自動作成されている","PersistentVolumeClaim","実際に、PresistentVolume を使うためには、PresistentVolumeClaim で要求を出す必要があります。 必要な項目は、下記です。","ラベルセレクタ","ラベルでフィルタリング","求めている容量","PresistentVolume のアクセスモードを参照","PresistentVolume の StorageClass を参照","要求を満たした Volume が RetainPolicy だった場合、Claim を削除した時点で「Released」になります。","最後に","今回は、書籍をそのまま書いた感じになりました。 実際に試したのは、","です。 まあ、あんまり深くはハマらない方が良いのではと思いました。 次回は、","です。"],"t":"一足遅れて Kubernetes を学び始める - 11. config&storage その2 -"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_12/index.mdx","c":["一足遅れて Kubernetes を学び始める - 12. リソース制限 -","ストーリー","前回","では、storage について学習しました。 今回は、リソース制限について学習します。 ※ リソースの種類から、次は「Metadata」だったのですが、kubernetes 完全ガイドによると直接説明するのではなく、内容ベースで説明されていましたので、それに準拠します。","リソース制限","kubernetes で管理するコンテナに対して、リソース制限をかけることができます。主に CPU やメモリに対して制限をかけることができますが、Device Plugins を使うことで GPU にも制限をかけることもできます。 ※ CPU の指定方法は、1vCPU を 1000millicores(m)とする単位となります。","requests と limits","request は、使用するリソースの下限値です。 limits は、使用するリソースの上限値です。 request は、空きノードに指定するリソースがなければスケジューリングされませんが、limits は、関係なくスケジューリングされます。 とにもかくにも、試してみましょう。 まず、現状確認です。","jsonpath の使い方は、","にあります。 allocatable が Pod に配置できるリソース量で、capacity は Node 全体での配置できるリソース量です。 これだけだと、現在使っているリソース量が不明なので個別に調べます。","現状のリソース状況を表にすると下記のとおりです。 | node | allocatable (memory/cpu) | capacity (memory/cpu) | used (memory/cpu) | remain (memory/cpu) | | -------- | ----------------------------- | -------------------------- | ---------------------- | ------------------------ | | raspi002 | 847,048Ki/4000m | 949,448Ki/4000m | 150,000Ki/200m | 697,048Ki/3800m | | raspi003 | 847,048Ki/4000m | 949,448Ki/4000m | 320,000Ki/400m | 527,048Ki/3600m | では、リソース制限を試してみましょう。","apply する pod で要求する memory の下限合計は 384Mi(128Mi×3),cpu は 900m(300m×3)です。 これだと、pod が run するはずです。","期待通りですね。 今度はリソース制限になる状態を試してみます。 全 WorkerNode の memory 下限合計は 1,224Mi(697,048Ki+527,048Ki)です。 これを超えるように先程のマニフェストを更新します。 replica 数を 3 にしましたが、10 にすれば良いですね（1,280Mi) 期待動作として、9 個(128Mi*9=1,152Mi)は Running で、1 個(128Mi)は Pending になるはずです。 sample-resource.yaml の replica を 10 に変更したあと ↓","あれ、2 つ Pending になっていますね。もしかして、Node の空きリソースが中途半端にないからですかね。 確認してみましょう。","raspi002 は、847Mi 中 790Mi 使っています。１つの Pod を追加するためのリソース（128Mi）はないですね。 raspi003 は、847Mi 中 704Mi 使っています。こちらは空いている気がするのですが、なぜでしょうか。 ここで、memory の","というところに着目すると、100%だった場合は 828Mi ということになります。 確かに、それだと 704Mi+128Mi=832Mi でオーバーしています。 では、allocatable で表示されていた 847Mi との違いは何でしょうか。 allocatable というのは、全ての namespace にある pod も込みのリソース配置可能量だからです。 default だけでなく、kube-system など他の namespace にある pod も、もちろんリソースを消費しています。 828Mi というのは、default で使えるリソース配置可能量ではないでしょうか。（現在の namespace は default) ちなみに、Limits は、100%を超えていますね...。ひぇ〜...。","Cluster Autoscaler","需要に応じて Kubernetes Node を自動的に追加されていく機能です。 これが動作するタイミングは、Pod が Pending になったときに動作します。 つまり、先程の例であったように、requests の下限によってスケールします。 そのため、requests が高すぎるために、実際はロードアベレージが低くてもスケールしてしまったり、 requests が低すぎるために、実際は高負荷でもスケールしなくなったりします。 requests は、パフォーマンステストをしつつ最適化していきましょう。","LimitRange","さっきの例でもあったように、それぞれに対して requests,limit を設定しても良いのですが、 もっと便利なものがあります。それが LimitRange です。 これは、Namespace に対して CPU やメモリのリソースの最小値や最大値を設定できます。 設定可能な制限項目として、下記があります。","default","デフォルトの Limits","defaultRequest","デフォルトの Requests","max","最大リソース","min","最小リソース","maxLimitRequestRatio","Limits/Requests の割合","また、制限する対象は、Container,Pod,PersistentVolumeClaim があります。 実運用する際は、きちんと定義しておきましょう。（プロバイダーによってはデフォルトで設定されているものもあるそうです）","ResourceQuota","ResourceQuota を使うことで、Namespace ごとに「作成可能なリソース数の制限」と「リソース使用量の制限」ができます。 「作成可能なリソース数の制限」を試そうと思います。","こうすると、pods が 5 個までしか作成できないので、sample-resource.yaml を適用しても 5 個までしか作成されません。 replica のような場合は、特に警告がなく単純に作られませんでした。 configmap を 5 個までに制限して、1 つずつ configmap を apply すると、警告がでるそうです。","HorizontalPodAutoscaler(HPA)","HPA は、Deployment,ReplicaSet で管理する Pod の CPU 負荷などに応じて自動的にスケールするリソースです。 30 秒に１回の頻度でスケールするかチェックしています。 必要なレプリカ数は、下記の数式で表します。","必要なレプリカ数 = ceil(sum(Pod の現在の CPU 使用率)/targetAverageUtilization)","で、","auto scaling は target value に近づくように pod 数が調整されるということ。","という文がわかりやすかったです。つまり、targetAverageUtilization が 50 なら、全体の CPU 使用率が 50%になるよう調整されます。 今回、試そうと考えたのですが、metrics-server を install していないため、動作確認できませんでした。 また今度 install して試してみようと思います。","VerticalPodAutoscaler(VPA)","VPA は、コンテナに割り当てる CPU やメモリのリソース割当をスケールさせるリソースです。 これは、HPA のスケールアウトではなく、Pod のスケールアップを行うものです。","お片付け","最後に","今回は、Requests や Limits を操作してリソース制限をしてみました。 どれがいくらリソースを消費しているのか確認する術を学び、 ついでに jsonpath の使い方も知りました。 次回は、","です。"],"t":"一足遅れて Kubernetes を学び始める - 12. リソース制限 -"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_13/index.mdx","c":["一足遅れて Kubernetes を学び始める - 13. ヘルスチェックとコンテナライフサイクル -","ストーリー","前回","では、requests や limit などのリソース制限について学習しました。今回は、ヘルスチェックとコンテナライフサイクルについて学習します。","ヘルスチェック","Kubernetes では、Pod の正常生判断のためのヘルスチェックが 2 種類用意されています。","Liveness Probe","Pod が正常か判断。異常だった場合、Pod を再起動。","Readiness Probe","Pod がサービスインする準備ができているか判断。準備できていなかったら、トラフィックを流さない。","たとえば、Pod 内でメモリリークが発生し応答しなくなった場合に有効です。 LoadBalancer のサービスは、ICMP による簡易ヘルスチェックがデフォルトで用意されています。 また、Liveness、Readiness どちらにも３つの方式があります。","exec","コマンドを実行し、終了コードが 0 でなければ失敗","httpGet","HTTP GET リクエストを実行し、statusCode が 200~399 でなければ失敗","tcpSocket","TCP セッションが確立できなければ失敗","では、試してみましょう。","timeoutSeconds","タイムアウトまでの秒数","successThreshold","成功と判断するまでのチェック回数","failureThreshold","失敗と判断するまでのチェック回数","initialDelaySeconds","初回ヘルスチェック開始までの遅延","periodSeconds","ヘルスチェックの間隔","設定どおりに動作していますね。では、失敗させましょう。 liveness を失敗させるには index.html を削除すれば良いですね。","一度削除されて、再起動しましたね。 今度は、readiness を失敗させましょう。こちらは 50x.html を削除すれば良いですね。","期待通り、50x.html を削除すると、READY から外れて、追加すると READY に戻りました。","コンテナの再起動","コンテナのプロセスが停止、またはヘルスチェックの失敗によってコンテナを再起動するかどうかは、spec.restartPolicy によって決まります。 種類は下記３つです。","Always","常に Pod を再起動させる","OnFailure","終了コード 0 以外の予期せぬ停止の場合、Pod を再起動させる","Never","再起動させない","試してみましょう。","成功、失敗どちらも再起動していることがわかります。","成功時は、Completed の終了していますね。CrashLoopBackOff していません。失敗時は、Error となり、CrashLoopBackOff しています。 期待通りですね。","initContainers","Pod のメインとなるコンテナを起動する前に別のコンテナを起動させるための機能です。 spec.containers がもともとありますが、こちらは同時並列で起動するので、順序が必要な場合には向いていません。 initContainers は、spec.initContainers で設定でき、複数指定できます。複数の場合は上から順に起動します。 試してみましょう。","確かに、initContainers が順序通り起動できています。ふむふむ。","起動時と終了時のコマンド実行(postStart,preStop)","コンテナ起動後に実行するコマンドを postStart, コンテナ終了前に実行するコマンドを preStop という機能で実現できます。","たしかに、postStart, preStop が動いています。 注意しないといけないのが、postStart は、spec.containers[].command の実行とほぼ同じだそうです。（非同期)","Pod の安全な停止とタイミング","terminationGracePeriodSeconds に指定した秒数は、pod が削除開始時からの猶予です。 デフォルトで 30 秒となっています。30 秒の間に preStop+SIGTERM の処理が終わらなければ、 強制的に SIGKILL されて停止されます。ただし、preStop が終わっていなくて 30 秒たった場合、 SIGTERM 処理を 2 秒だけ実施できます。 terminationGracePeriodSeconds の値は、prePost を必ず終える秒数に設定しましょう。","Node をスケジューリング対象から外す","Node を kubernetes のスケジューリング対象から外す cordon というコマンドがあります。 Node の状態には、SchedulingEnabled と SchedulingDisabled があり、後者の状態になると、 kubernetes からのスケジューリング対象外となり、たとえば ReplicaSet の更新などが機能しなくなります。 cordon コマンドを使うと、指定する Node が SchedulingDisabled になります。(uncordon は逆) ただし、現在動作している Pod はスケジューリング対象になったままで、新たに追加するものが スケジューリング対象外になります。現在動作しているものも対象にしたい場合は、drain コマンド を使います。 実際に試してみます。","drain すると、ReplicaSet のように管理した Pod であれば、別 Node に作成されるので良いのですが、 単体 Pod など管理されていないものは、削除されてしまいます。上記の警告は、DaemonSet で管理されている Pod は、 削除するしかないけど、良いですか？というものです。 そのため、drain をすると、いくつか警告されます。警告内容に従って適宜操作する必要があります。","お片付け","最後に","今回は、ヘルスチェックの動作と、コンテナを停止するまでのステップを学習しました。 わざわざヘルスチェックの処理をアプリケーションに用意せずとも、kubernetes に機能として 存在することに、驚きました。次回は、","です。"],"t":"一足遅れて Kubernetes を学び始める - 13. ヘルスチェックとコンテナライフサイクル -"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_14/index.mdx","c":["一足遅れて Kubernetes を学び始める - 14. スケジューリング -","ストーリー","前回","では、requests や limit といったヘルスチェックの仕方を学びました。今回は、Affinity などによるスケジューリングについて学習します。","スケジューリング","これから学ぶスケジューリングでは、大きく分けて２つに分類します。","Pod のスケジューリング時に特定の Node を選択する方法","Affinity Anti-Affinity","Node に対して汚れをつけて、それを許容できる Pod のみスケジューリングを許可する方法","汚れ = Taints 許容 = Tolerations","Node のラベル確認","デフォルトで設定されている Node のラベルを見てみます。","arch や os はデフォルトで設定されているみたいです。 次以降の学習のため、ラベルをはります。","NodeSelector","最も簡単な NodeAffinity の設定です。 指定するラベルに属する Node に Pod を割り当てるようスケジューリングします。簡易なので、equality-base のみしか指定できません。 では、disksize が 300 の Node(raspi003)に Pod を配置しましょう。","期待通りですね。OK です。 nodeSelector はイコールでしか表現できないので、柔軟性に欠けます。","Affinity","Affinity は、NodeSelector よりも柔軟に設定できます。つまり、set-based の指定方法です。 詳しくは","を参照下さい。今回は In オペレータを使います。","NodeAffinity では、required と preferred の 2 つ設定できます。","required","必須スケジューリングポリシー","preferred","優先的に考慮されるスケジューリングポリシー","必須条件が「cputype=low である Node(raspi002,raspi003)」で、優先条件が「hostname=raspi002 である Node」です。 適用してみましょう。","確かに raspi002 に配置されました。では、raspi002 をスケジューリングできなくするとどうなるのでしょうか。","今度は、raspi002 を cordon したので、raspi003 に移りました。優先なので、満たされなくても良いのですね。必須条件が満たされなかったら、Pending になります。 元に戻します。","AND と OR","nodeSelectorTerms や matchExpressions は配列なので複数指定できます。","上記の場合は、 (A and B) OR (C and D)という条件になります。","Anti-Affinity","Anti-Affinity は、Affinity の逆です。つまり、特定 Node 以外の Node に割り当てるよう スケジューリングします。特別な指定はなく、単に Affinity の否定形式にするだけです。言葉だけですね。","Inter-Pod Affinity","特定の Pod が実行されているドメイン上へ Pod をスケジューリングするポリシーです。 Pod 間を近づけることができるので、レイテンシを下げることができます。 まず、特定の Pod は、先程の NodeSelector で使ったものとします。","これだと、sample-app がある Node で kubernetes.io/hostname(=raspi003)と同じ Node に Pod を割り振ります。つまり、raspi003 にできるはずです。","期待通り raspi003 にできています。 また、required だけでなく、preferred も設定できます。","必須条件としては、下記のとおりです。","「label が app=sample-app である Pod が動いている Node(raspi003)で、kubernetes.io/arch が同じ Node(arm)」","これは raspi002(arm),raspi003(arm)どちらにも当てはまります。 そして、優先条件として、下記のとおりです。","「label が app=sample-app である Pod が動いている Node(raspi003)で、kubernetes.io/hostname が同じ Node(raspi003)」","これにより、raspi003 が選ばれるはずです。","期待通り raspi003 で動いていますね。","Inter-Pod Anti-Affinity","Inter-Pod Affinity の否定形です。以上。 今まで紹介した Affinity、AntiAffinity,Inter-Pod Affinity, Inter-Pod AntiAffinity は、組み合わせることができます。","Taints","Node に対して汚れをつけていきます。汚れた Node に対して、許容する Pod のみがスケジューリングできるようになります。 汚れの種類(Effect)は、３つあります。","PreferNoSchedule","可能な限りスケジューリングしない","NoSchedule","スケジューリングしない（既にスケジューリングされている Pod はそのまま）","NoExecute","実行を許可しない（既にスケジューリングされている Pod は停止される）","それでは、まず Node を汚しましょう。","これで raspi003 に Pod をスケジューリングできなくなりました。","Tolerations","さきほど汚した Node に対して、許容(Tolerations)できる Pod を作成しましょう。 key と value(env=prd)と Effect(NoSchedule)が設定された Pod のみ許容されます。作ってみます。","※ nodeSelector で汚れた Node である raspi003 を指定するようにしています。 operator には、2 種類あります。","Equal","key と value が等しい","Exists","key が存在する","では、適用してみます。","汚れた Node に許容される Pod が適用されましたね。 許容を一部変えてみる(env=stg)と、もちろん Pending になりました。 もとに戻しておきます。","お片付け","最後に","Pod をどの Node にスケジューリングするのか学習しました。 汚れ(taint)と許容(tolerations)という考えは面白いなと思いました。 ただ、あまり使いすぎると複雑に陥りやすそうなので、注意が必要ですね。 次回は、","です。"],"t":"一足遅れて Kubernetes を学び始める - 14. スケジューリング -"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_10/index.mdx","c":["一足遅れて Kubernetes を学び始める - 10. config&storage その1 -","ストーリー","前回","では、様々な service を学習しました。 今回は、config&storage の config を学びます。","config&storage","Kubernetes には、下記のようにリソースの種類が存在します。 | リソースの分類 | 内容 | | :----------------------- | :----------------------------------------------------------- | | Workloads リソース | コンテナの実行に関するリソース | | Discovery＆LB リソース | コンテナを外部公開するようなエンドポイントを提供するリソース | | Config＆Storage リソース | 設定・機密情報・永続化ボリュームなどに関するリソース | | Cluster リソース | セキュリティやクォータなどに関するリソース | | Metadata リソース | リソースを操作する系統のリソース | ※ ","環境変数","静的設定や、Pod やコンテナの情報を設定、シークレットでの設定があるみたいです。","MAX_CONNECTION は、静的に設定できています。 Pod やコンテナの設定は、POD_IP,KIMITS_CPU で設定できています。 Pod やコンテナの情報は、","で得ることができます。ふむふむ。","Secret","パスワードなどの機密情報を Secret で暗号化してくれます。 手段の種類が下記のようにいくつかあります。","Generic TLS Docker Repository Service Account","Generic の場合は、スキーマレスなため、汎用性の高い指定が可能になります。それを使ってみようと思います。(TLS の場合は、tls.crt,tls.key が必要） 使い方して、ファイル参照、envfile 参照、直接指定、マニュフェスト指定の４パターンです。それぞれ試してみます。","ファイル参照","envfile 参照","直接指定","マニュフェスト指定","どれも、正しく動きましたね。プロダクトとしては使わないと思いますが、お試しで確認するには Generic は扱いやすくて良いですね。 では、設定した値を使ってみましょう。","Secret の利用","手段として、環境変数か Volume かの 2 つです。","環境変数から Secret を使う","環境変数から使う場合、値が固定されてしまいます。（静的）","Volume から Secret を使う","こちらは、動的に書き換えることができるそうです。逐次 Volume を見ているんでしょうね。（環境変数の場合、コンテン起動した時点で固定される）","動的に書き換わっていますね。OK! ※ admin の a が文字化けしていた...","ConfigMap","設定情報を Key-Value 形式で登録することができます。 こちらも手段としては、ファイル参照、直接参照、マニフェスト参照があります。 さっきと同じなので、ファイル参照のみ試してみます。","secret と同じ感じですね。これって、どんなファイルでも(1MB まで)保存できちゃうそうです。 secret と同様で、設定したデータは環境変数、Volume の２つから参照可能です。","お片付け","最後に","環境変数の設定方法について、学びました。 個人開発で、外部サービスをアプリケーションに組み込む際、 API_KEY を環境変数として登録して開発しています。 今回は、Generic で Secret を保存しましたが、プロダクトでは、 service_account を使うのが一般的なのでしょうか？ 次回は、Storage について学習します。 ","です。"],"t":"一足遅れて Kubernetes を学び始める - 10. config&storage その1 -"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_15/index.mdx","c":["一足遅れて Kubernetes を学び始める - 15. セキュリティ -","ストーリー","前回","では、Affinity などで Pod のスケジューリングについて学習しました。今回は、セキュリティについて学習します。","サービスアカウント","Pod で実行するためのプロセスを制御するために割り振られるアカウントのことをサービスアカウントというそうです。","これを apply してみます。","サービスアカウントが作成されました。また、imagePullSecrets の内容が secrets に登録されました。(sample-serviceaccount-token-4xhgm) imagePullSecrets は private な docker レジストリに使います。","認証に必要な token が登録されていますね。","RBAC (Role Based Access Control)","RBAC は、さきほど作成したサービスアカウントと、どういった操作を許可するのかを定めた Role を紐付け(RoleBinding)して、権限管理をします。1 つの Role に対して複数のサービスアカウントを RoleBinding できます。 RBAC は、２つのレベルがあり、１つは Namespace レベルで、もう一つはクラスタレベルがあります。 設定範囲がクラスタの方が大きい感じです。(namespace 横断して設定する場合はクラスタレベルにする)","Role と ClusterRole RoleBinding と ClusterRoleBinding","操作の種類ですが、どの Deployment や DaemonSet のようなリソースに対して下記のものがあります。 | * | 全ての処理 | | :----- | :--------- | | create | 作成 | | delete | 削除 | | get | 取得 | | list | 一覧取得 | | update | 更新 | | patch | 一部変更 | | watch | 変更の追従 | ※ ","今回は","を参考に進めます。 新しく作ったサービスアカウントでコンテキストを作成し、そのアカウントから Pod 情報を取得できるか試してみます。 そのためには、サービスアカウントの認証情報を通しておく必要があります。 ※ Role と ClusterRole に大きな違いはないため、Role を試します。","では、コンテキスト(sample-sa-context)を作成して、それを使用します。","新たに作成したサービスアカウントで Pod の情報が取得できるか試してみます。","Error になりました。sample-serviceaccount は何も Role をバインドしていないからですね。 では、RoleBinding していきます。 元に戻ります。","では、もう一度試してみます。","おお、取得できました！ もとに戻しておきます。","SecurityContext","コンテナに対してセキュリティ設定をすることができます。 例えば、Capabilities の追加・削除、実行するユーザ、グループの変更、ファイルの ReadOnly 化などができるそうです。","apply し、中身を確認してみます。","cap_sys_admin が増えてますね。audit_write は見つかりません。 そもそも、どんな種類があるのか分からなかったので、","を参考にしました。","PodSecurityContext","Pod(全てのコンテナ)に対してセキュリティ設定をすることができます。 例えば、実行するユーザやグループの制御、root 実行を拒否したり、カーネルパラメータを上書きすることもできます。","では、apply しています。","実行したユーザが nobody(99)に変更されていますね。また、supplementalGroups で、プライマリ GID に指定の GID を追加することができます。","そのほか","PodSecurityPolicy や、NetworkPolicy、そして認証、認可の AdmissionControl というものもあるそうです。","お片付け","最後に","主に RBAC について学習しました。 複数人で開発する際は、コンテキストを分けて開発を進めるのが良いみたいですね。 今回で取り組んだように、誰がどの権限を持っているかを RBAC で管理できるので、 必要以上の権限を与えられて事故るようなことは少なくなりますね。 （といっても、まだ個人でしか使ってないので分かりませんが...） 次回は、最後で","です。"],"t":"一足遅れて Kubernetes を学び始める - 15. セキュリティ -"},{"f":"src/routes/blog/contents/start_the_learning_kubernetes_16/index.mdx","c":["一足遅れて Kubernetes を学び始める - 16. コンポーネント -","import { Image } from \"~/components/image/image\";","ストーリー","前回","では、RBAC による権限について学習しました。今回は最後に Kubernetes のコンポーネントについて学習します。","コンポーネント","Kubernetes では、下記のような構成になっています。 ※ ","それぞれのコンポーネントについて学習します。","現状確認","下記は、MasterNode で動いています。","etcd-raspi001 kube-apiserver-raspi001 kube-controller-manager-raspi001 kube-scheduler-raspi001","下記は、全 Node で動いています。","kube-flannel-ds kube-proxy","coredns は、Master1 台と Worker1 台で動いています。 ※ ","に設定しました。","etcd","MasterNode に存在するコンポーネントです。 分散 Key-ValueStore である etcd は、Kubernetes のクラスタにある全情報が保存されています。そのため、単一障害にならないようクラスタを組むことが推奨されているみたいです。ここのデータにアクセスするのは kube-apiserver から経由しなければなりません。 直接確認したい場合は、etcdctl を使ってみると良いです。","kube-apiserver","MasterNode に存在するコンポーネントです。 KubernetesAPI を提供するコンポーネントです。kube-scheduler や kube-controller-manager,kubelet から呼ばれます。 etcd に対してリソースを管理するだけで、Pod の起動はしません。","kube-scheduler","MasterNode に存在するコンポーネントです。 Node 情報が未割り当ての Pod を検知して、その Pod に Node を割り当てるリクエストを kube-apiserver に送ります。 割り当てるだけであって、Pod を起動させません。Node を割り当てる際、NodeAffinity や Taints などを考慮に入れます。","kubelet","各 Node 上で動作するコンポーネントです。未割り当てだった Node が割り当てられたことを検知し、 実際に Pod を起動します。","kube-controller-manager","MasterNode に存在するコンポーネントです。 様々なコントローラを実行するコンポーネントです。Deployment や ControllerReplicaSetController では、 状態を監視し、期待する Pod 数と現在の Pod 数を見ます。kube-apiserver に対して、過不足分の Pod を調整するよう要求します。 その後は、さきほどの kube-scheduler,kubelet の一連の流れになります。","kube-proxy","各 Node 上で動作するコンポーネントです。NodePort や ClusterIP 宛のトラフィックを転送します。","kube-dns","kubernetes クラスタ内の名前解決やサービスディスカバリに利用される DNS サーバです。 私の環境では、CoreDNS を使っていました。","そのほか","CustomResourceDefinition(CRD)と Operator","CRD は独自のリソースを定義できるリソースです。このような拡張性をもたせることで、様々な開発が進められます。 CRD は、単なる Kubernetes オブジェクトなだけなので、Operator というカスタムコントローラをセットで作る必要があります。 Operator Framework と呼ばれるもので簡単に作成できるそうです。","最後に","ようやく Kubernetes 完全ガイドの内容を読み切ることができました。 当初は、ここまで記事にアウトプットし続けるつもりはなかったです。 実際に kubernetes を raspberryPi 上で動かしてみると、 いろいろな発見があってのめり込んでしまいました。 ただ、","ぐらいから、いろいろとあって、 書籍の内容を、ほぼそのまま使わさせてもらいました。（笑) これからは、実際に GKE を使ってアプリケーション開発をしてみようと思います。"],"t":"一足遅れて Kubernetes を学び始める - 16. コンポーネント -"},{"f":"src/routes/blog/contents/started_using_ruby_on_rails_at_work./index.mdx","c":["Ruby on Railsを業務で使って思ったこと","2022 年 1 月より新しい職場で、はじめて Ruby on Rails(以下、Rails)を使うようになりました。 これまで、PHP の CakePHP、Node.js の Express、Python の Flask の経験がある私ですが、 Rails に、なかなか慣れない苦労がありました。ゆるく、言語化しようと思います。 ちなみに、Ruby もはじめて使いました。","Rails Doctrine","Rails という新しく Web フレームワークを学ぶので、どういう思想で作られたのか知りたいなと思いました。 そこで、下記のページをざっくり読んでみました。","https://rubyonrails.org/doctrine","PoLS や CoC といった原則により、プログラマは余計な悩み事が減って良いな〜と思います。 DB のテーブルのカラム名で、post_id なのか postId なのかなんてどうだって良いのは、とても同感です。 ディレクトリ構成も、CoC で決まっているので、論争が生まれることが少ないのかなと思います。","慣れないところ","まず、前提として、私が Ruby、Rails どちらも知らないというのがあります。","引数無し関数の括弧省略と、変数の見分けがつかない","Ruby の話で、よくある話と聞いています。 これは、コード読んで、悩みました。","Ruby の関数か、Rails の関数か、独自の関数か、分からない","業務で Rails のコードを読んでいると、これは Ruby の関数なのか、 Rails の関数なのか、それとも、独自の関数なのか、いつもわからなくなります。","マイグレーションって、移行という言葉として思ってた","Rails の文脈だと、マイグレーションって DB 管理の Ruby コードを指すみたいです。 DB マイグレーションって言わないのね〜って、違和感がありました。","最後に","rails って、色々よしなにやってくれて便利〜！と思うこともあれば、 どう動いているんだっけ？と分からなくなることもあるなと思っています。 トレードオフと思うんですが、まだまだ慣れない今日このごろでした。"],"t":"Ruby on Railsを業務で使って思ったこと"},{"f":"src/routes/blog/contents/tailor/index.mdx","c":["Zalando tailor で Micro Frontends with ( LitElement & etcetera)","import { Image } from \"~/components/image/image\"; Zalando 社が開発した Tailor を使って、サンプル Web アプリを Micro Frontends で構築してみました。Tailor はサーバーサイドで統合するアーキテクチャです。クライアントサイドは、Web Components で作られている Lit Element を使って統合しました。どういった内容か、ここに投稿しようと思います。 作ったリポジトリは、下記に残しています。 https://github.com/silverbirder/micro-frontends-sample-code-4","全体構成","ざっくり説明すると、HTML から Tailor に対してフラグメント(コンポーネント)を取得・返却するようにします。各フラグメントは、LitElement で WebComponents を定義させた Javascript を指します。フラグメントを読み込むだけで、カスタムエレメントを使えるようになります。","Tailor","https://github.com/zalando/tailor","A streaming layout service for front-end microservices","tailor は、ストリーミングレイアウトサービスというだけあって、fragment の load をストリーミングするそうです。(こちらのライブラリは、Facebook の"," に影響されたそう) まず、tailor.js の HTML テンプレートは次のとおりです。 templates/index.html","これらの fragment の取得は、tailor.js を経由します。 tailor.js","x-request-uri は、後ろのフラグメントに URL を引き継ぐためのようです。 そして、フラグメントサーバーは、次のとおりです。 fragments.js","fragments.js は、Response Header に Link ヘッダを追加するようにします。Tailor は、このヘッダの Javascript を読み込むことになります。 さらに、fragments.js は、Link ヘッダで指定されたリクエストを "," でストリームのパイプを返すそうです。","Lerna","それぞれのフラグメントを Lerna で管理するようにします。 私は、下記のような packages 分けをしました。","common","共通する変数・ライブラリ","fragment","LitElement のカスタムエレメント定義","function","フラグメントと連携する関数 (ヒストリーやイベントなど)","具体的に言うと、次のようなものを用意しました。 | directoy name | package name | | ------------------------------------ | --------------------------------------- | | packages/common-module | @type/common-module | | packages/common-variable | @type/common-variable | | packages/fragment-auth-components | @auth/fragment-auth-components | | packages/fragment-product-item | @product/fragment-product-item | | packages/fragment-search-box | @search/fragment-search-box | | packages/function-event-hub | @controller/function-event-hub | | packages/function-history-navigation | @controller/function-history-navigation | | packages/function-renderer-proxy | @controller/function-renderer-proxy | | packages/function-search-api | @search/function-search-api | | packages/function-service-worker | @type/function-service-worker | どの名前も、その時の気分で雑に設定したので、気にしないでください。（笑） 伝えたいのは、@XXX が 1 チームで管理する領域みたいなことをしたかっただけです。 package を使いたい場合は、次のような依存を設定します。 package.json","LitElement","https://lit-element.polymer-project.org/","LitElement A simple base class for creating fast, lightweight web components","純粋な WebComponents だけを使えばよかったのですが、次のような理由で LitElement を使いました。","Typescript が書ける レンダリングパフォーマンスの良い lit-html が使える プロパティ変化によるレンダリング更新ができる","まあ、特にこだわりはないです。 書き方は、次のとおりです。","LitElement + Typescript では、open-testing を使ってテストすることができます。 https://github.com/PolymerLabs/lit-element-starter-ts/blob/master/src/test/my-element_test.ts また、jest でもテストができるようです。 https://www.ninkovic.dev/blog/2020/testing-web-components-with-jest-and-lit-element","DynamicRendering","このサンプルでは、カスタムエレメントを使って、ブラウザ側でレンダリングする 所謂 SPA の動きで構築しています。 『SEO ガー！』と SSR しなきゃと思う訳ですが、正直 SSR を考えたくないです。(ハイドレーションなんて無駄なロードをブラウザにさせたくない） 次の記事のように、ボットのアクセスのみに、ダイナミックレンダリングした結果（SPA のレンダリング結果 HTML）を返すようにしたいです。 https://developers.google.com/search/docs/guides/dynamic-rendering?hl=ja 技術的には、次のようなものを使えば良いです。 https://github.com/GoogleChrome/rendertron function-renderer-proxy/src/renderer.ts","要は、Puppeteer で実際にレンダリングさせた結果を Bot に返却しているだけです。","EventHub","フラグメント同士は、CustomEvent を通して連携します。 https://developer.mozilla.org/ja/docs/Web/Guide/Events/Creating_and_triggering_events 全て、この CustomEvent と AddEventListener を管理する EventHub(packages 名)を経由するようにします。(理想)","History","ページ全体のヒストリーは、HistoryNavigation(packages 名)で管理したいと考えています。(理想) https://developer.mozilla.org/en-US/docs/Web/API/History_API また、ルーティングを制御する Web Components 向けライブラリ vaadin/router も便利そうだったので導入してみました。 https://vaadin.com/router","ShareModule","LitElement のようなどこでも使っているライブラリは、共通化してバンドルサイズを縮めたいです。 Webpack のようなバンドルツールには、External や DLLPlugin、ModuleFederation などの共通化機能があります。 https://webpack.js.org/concepts/module-federation/ 今回は、external を使っています。 common-module/common.js","common-module/webpack.config.js","共通化したライブラリは、次の Tailor の index.html で読み込みます。 templates/index.html","そうすると、例えば searchBox の webpack では、次のようなことが使えます。 fragment-search-box/webpack.config.js","その他","その時の気分で導入したものを紹介します。(or 導入しようと考えたもの)","GraphQL","API は、雑に GraphQL を採用しました。特に理由はありません。","SkeltonUI","Skelton UI も使ってみたいなと思っていました。 https://material-ui.com/components/skeleton/ React を使わなくても、CSS の@keyframes を使えば良いでしょう。が、まあ使っていません。(笑) https://developer.mozilla.org/ja/docs/Web/CSS/@keyframes","Rxjs","typescript の処理をリアクティブな雰囲気でコーディングしたかったので導入してみました。 (リアクティブに詳しい人には、怒られそうな理由ですね...笑) https://rxjs.dev/","所感","これまで、Podium、Ara-Framework, そして Tailor といった Micro Frontends に関わるサーバーサイド統合ライブラリを使ってみました。 https://silverbirder.github.io/blog/contents/microfrontends https://silverbirder.github.io/blog/contents/ara-framework これらは、どれも考え方が良いなと思っています。 Podium のフラグメントのインターフェース設計、Ara-Framework の Render とデータ取得の明確な分離、そして Tailor のストリーム統合です。 しかし、これらは良いライブラリではありますが、プロダクションとしてはあんまり採用したくない(依存したくない)と思っています。 むしろ、もっと昔から使われていた Edge Side Include や Server Side Include などを使ったサーバーサイド統合の方が魅力的です。 例えば、Edge Worker とか良さそうです。(HTTP2 や HTTP3 も気になります) まあ、まだ納得いく Micro Frontends の設計が発見できていないので、これからも検証し続けようと思います。"],"t":"Zalando tailor で Micro Frontends with ( LitElement & etcetera)"},{"f":"src/routes/blog/contents/tech_antena/index.mdx","c":["技術におけるアンテナの張り方 (巨人の肩に乗れ!)","エンジニアは、普段様々なところから技術をキャッチアップすると思います。それは、SNS であったり、ブログであったり、動画であったりです。 そこで、私の技術をキャッチアップするためのアンテナの張り方について、紹介しようと思います。","RSS","RSS は、良いです。不定期にお気に入りの企業やサービスのテクニカルな情報を知ることができます。 特に、大手の企業のブログや、自分自身の関心を持っているサービスの RSS がオススメです。","Tech Blog","Web","CDN","BBS","Book","News","Twitter","Twitter の公式アカウントをフォローするのも良いです。 また、特定分野に長けているエンジニアの方であったり、コミュリティアカウントもフォローすると良いでしょう。 例えば、私の場合は、次のアカウントをフォローしていたりします。","Micro Frontends in Action の著者","日本における勉強会のスライド収集 Bot","Web Incubator のコミュニティ","UseCotlin","私の独自ツールです。TwitterAPI を使って、Speakerdeck や GoogleSlides などの技術スライドを Spreadsheet に収集しています。 勉強会やカンファレンスが開催されると、Twitter で宣伝されることが多いため、こちらのツールで自動的にキャッチアップできます。 また、日本だけではなく、世界各国のプレゼンテーションを知ることができます。","Medium","Medium の有料会員です。Medium は、Tech 系の質の高い投稿が多く充実できます。 無料会員だと、一日の閲覧回数に制限があるため、ちょっとストレスに感じ、有料会員になりました。","Hatena Blog","今日のホットエントリを見て、何が流行りなのか、どういう分野に関心があるのかざっとななめ読みしています。","また、テクノロジだけではなく、総合分野を見ていたりします。","最後に","このネタを思いついたので、勢いで投稿してみました。 もしどなたかの参考になれば、幸いです。"],"t":"技術におけるアンテナの張り方 (巨人の肩に乗れ!)"},{"f":"src/routes/blog/contents/testing_pattern/index.mdx","c":["Webアプリのテスト観点を調べてまとめてみた (25選)","import { Image } from \"~/components/image/image\"; 最近、Property Based Test という言葉を知りました。 他にどういうテストの種類があるのか気になったので、調べてみました。 本記事は、テストの種類を列挙します。 ※ 使用する技術は、私の都合上、node.js で選んでいます。","テスト観点一覧","Cache Test","Web アプリでは、様々な Cache が使われます。 例えば、ブラウザ Cache、CDN Cache、プロキシ Cache、バックエンド Cache などなどです。 Cache は、便利な反面、使いすぎると、どこがどう Cache しているのか迷子になってしまいます。 Web アプリでも、Cache をテストする必要がありそうです。 https://github.com/http-tests/cache-tests","Code Size Test","大きなサイズの JS ライブラリを読み込むと、レスポンスタイムが悪化してしまいます。そこで、常にコードサイズを計測する必要があります。 https://github.com/ai/size-limit","Complexity Test","循環的複雑度(Cyclomatic complexity)は、制御文(if や for)の複雑さを計測します。 複雑なコードは、バグの温床になりがちなので、極力シンプルなコードを心がけたいところです。 https://eslint.org/docs/rules/complexity","Copy&Paste Test","Copy&Paste は、DRY の原則に反するため、特別な理由がない限りは、してはいけません。Copy&Paste を検出するツールがあるみたいです。 https://github.com/kucherenko/jscpd","Cross Browser/Platform Test","サポートするブラウザや、プラットフォーム(iOS,Android,Desktop など)の動作検証が必要です。 そのため、サポートするブラウザやプラットフォームの環境を準備しなければなりません。 そういう環境を手軽に使えるサービスがあったりします。 https://github.com/browserstack","E2E Test","Web アプリを、端から端まで (End To End: E2E)を検証します。 例えば、ユーザーが Web アプリを訪れて、クリックや入力するなど、使ってみることです。 このテストは、不安定なテスト(よく失敗する)になりがちなので、安定稼働できるような取り組みが必要です。 例えば、操作する処理の抽象化や、データ固定などです。 https://github.com/cypress-io/cypress","Exception Test","正常系、準正常系、異常系などのテストが必要です。 準正常系は、システムが意図的にエラーとしているものです。例えば、フォーム入力値エラーとかです。 異常系は、システムが意図せずエラーとなるものです。例えば、Timeout エラーとかです。 また、Java が得意な人なら知っているであろう、検査例外や非検査例外という例外の扱い方があります。 基本的には検査例外はエラーハンドリングし、非検査例外はエラーハンドリングしない方針が良いです。","Flaky Test","不安定なテストのことを指します。これに対するアプローチ方法の１つに、Google 社の資料があります。","日本人がまとめて頂いたものが、次の資料です。 https://speakerdeck.com/nihonbuson/flakytests","Integration Test","Integration Test は、Unit Test のような単一機能を統合した検証になります。 定義によりますが、私は『Unit Test では発見できないようなもの』かなと思います。 Unit Test でカバーできていなくても、他のテストで検証できていれば、Integration Test は不要になります。","Logging Test","ログ出力が適切なレベルで出力されているか検証する必要があります。 INFO, WARN, ERROR などがルールに基づいて使い分けされているか気になります。 ログを出すことができるかどうかは、ログライブラリの検証になりますので、必要ないかもしれませんが、 意図したタイミングで、意図したログレベルで、意図したメッセージが出力されるかは、テストしても良いと思います。","Monkey Test","お猿さんがランダムにテストするような、モンキーテストです。 テストのパターン網羅が難しい場合や、パターン網羅できているけどダメ押しで、このテストをします。 https://github.com/marmelab/gremlins.js/","Multi Tenanct Test","マルチテナントは、企業者（利用者）毎に区別した、同一のシステムを提供する方式です。 これは、企業毎にサブドメインを分けたりするため、その環境毎のテストが必要になります。","Mutation Test","テストを検証するため、突然変異テストというものがあります。 プロダクトコードを破壊することで、テストも壊れるかどうかを検証します。 もし、プロダクトコードを壊しても、テストが成功してしまうと、それは正しくテストできていません。 https://github.com/stryker-mutator/stryker","Chaos Test","障害を注入した際に、どういった動きになるのかを検証するテストです。 https://github.com/goldbergyoni/node-chaos-monkey","Performance Test","パフォーマンスと言っても、 CPU 使用率、メモリ使用率、レスポンスタイム、RPS など様々な指標があります。 これらを計測し、SLO などの基準値を満たせているかを検証しておく必要があります。 https://github.com/bestiejs/benchmark.js/","Property Based Test","データを半自動生成し、テストをする手法です。 https://github.com/dubzzz/fast-check","Regression Test","Regression Test は、修正した内容が意図せず他の箇所に影響を及ぼしていないか(デグレーション)を確認するテストです。 このテストは幅広い意味を持つので、ここに内容されるテスト種類は多いと思います。","Robustness Test","Web アプリは、ロバストであるべきです。 何かしら Web アプリ内で障害が発生したとしても、最低限のサービスだけでも提供するのが好まれます。 もちろん、その際の HTTP ステータスを 200 にせず、障害にあったステータスを返しましょう。","Security Test","セキュリティのテストは、どんな Web アプリでも必須になります。 セキュリティの専門家ではないので、どういうテストが必要なのかは、ここでは割愛します。 依存するパッケージ脆弱性検査には、下記のコマンドが有効です。","SEO Test","Web アプリへ流入数を改善するためには、SEO は不可欠です。 lighthouse というツールで SEO スコアを見ることができるみたいです。 https://github.com/GoogleChrome/lighthouse-ci","Smoke Test","Smoke Test は、Web アプリが最低限動作するために必要なケースを確保する検証です。 例えば、トップページへリクエストしたら、レスポンスが HTTP 200 で返却されるとかです。 この最低限の動作保証がなければ、これ以上の詳細なテストができません。 個人的には、Smoke Test → E2E Test の順で進むのかなと思っています。","Snapshot Test","Web アプリへリクエストし、そのレスポンスである HTML(スナップショット)を保存します。 この HTML が、変更前と比較して変化がないかの検証をするのが、Snapshot test です。 リファクタリングなど、変化がない修正に対して有効です。 https://jestjs.io/docs/ja/snapshot-testing","Static Test","Static Test は、Web アプリを動かさなくても検証できるテストです。 よくあるのが、Linter です。","HTML https://github.com/htmlhint/HTMLHint","CSS","https://github.com/CSSLint/csslint","JS","https://github.com/eslint/eslint","SVG","https://github.com/birjolaxew/svglint","Commit","https://github.com/conventional-changelog/commitlint","Docker","https://github.com/RedCoolBeans/dockerlint/ これらは、プルリクエストで機械的に指摘する Danger との相性が良いです。 https://github.com/danger/danger","Unit Test","単一機能をテストする Unit Test があります。この Unit Test が全て PASS したら、 他のテストを進めるのが一般的かなと思います。 https://github.com/facebook/jest","Code Coverage","Unit テストで、どこをテストできたかのカバレッジを見ることができます。 感覚としては、全体の 8 割を満たしていれば良いかなと思います。","実際に動作している JS や CSS のカバレッジを収集することもできます。 https://speakerdeck.com/pirosikick/puppeteerdeiranaicsswoxiao-su https://gist.github.com/silverbirder/71135913192fbca51a7e26924bd36b8b","Visual Regression Test","見た目の変化を監視する必要があります。例えば、リンク切れとかがあれば、検出するべきです。 https://github.com/garris/BackstopJS","最後に","どういうテストの観点があるのか、調べたり、経験則よりざっと書いてみました。 全てをテストする必要はなく、『どういう動作の品質を担保したいか』を意識して、 取捨選択するのが良いと思います。 最後まで読んでいただき、ありがとございます。"],"t":"Webアプリのテスト観点を調べてまとめてみた (25選)"},{"f":"src/routes/blog/contents/the_goodness_of_web_components./index.mdx","c":["Web Componentsの良さ","Web Components を人にお勧めしたいんです。メリット・デメリットをかんたんにまとめたいと思います。","Web Components is 何","https://www.webcomponents.org/specs より引用します。","Web components is a meta-specification made possible by four other specifications: The Custom Elements specification The shadow DOM specification The HTML Template specification The ES Module specification","WebComponents は、ブラウザの 4 つの API で実現可能となるメタ仕様です。","These four specifications can be used on their own but combined allow developers to define their own tags (custom element), whose styles are encapsulated and isolated (shadow dom), that can be restamped many times (template), and have a consistent way of being integrated into applications (es module).","div や span といった HTML タグがブラウザ標準にありますが、WebComponents は、独自のタグを定義することができます。","メリット","環境適合性","フレームワークに依存しないので、Rails など Web フレームワークへの適用が容易です。","軽量","ブラウザ API のみ使用するので、ロードするアセットはありません。","シンプル","HTML タグを使う要領で、WebComponents を使います。","サンドボックス","Shadow DOM のおかげで、CSS のスタイルが独自のタグの外に漏れません。","デメリット","パフォーマンス","React のような描画最適化は、ありません。","機能性","ブラウザ API の影響を、受けます。","SEO","Web Components は Javascript 実行が必須です。そのため、SSR は実現できません。"],"t":"Web Componentsの良さ"},{"f":"src/routes/blog/contents/thing_the_gas_testing_code/index.mdx","c":["Google Apps Script でも テスト がしたい！ (Clasp + Typescript + Jest)"," という技術スタックを選択しました。その開発体験について共有しようと思います。特段変わったことはしていません。 tags: [\"Google Apps Script\", \"Testing\", \"Clasp\", \"Typescript\", \"Jest\"] cover_image: https://res.cloudinary.com/silverbirder/image/upload/v1614431246/silver-birder.github.io/blog/google_apps_script_and_typescript_and_jest.png socialMediaImage: https://res.cloudinary.com/silverbirder/image/upload/v1614431246/silver-birder.github.io/blog/google_apps_script_and_typescript_and_jest.png","import { Image } from \"~/components/image/image\"; Google Apps Script(以下,GAS)でライブラリを公開しました。ライブラリを開発する際、","テストのフィードバックサイクルを短くする","ため、"," という技術スタックを選択しました。 その開発体験について共有しようと思います。特段変わったことはしていません。","Google Apps Script のテストってどうしてますか？","にアクセスしてデバッグ実行って、しんどくないですか？","ネットワーク越しでステップ実行するため、","遅い","G Suite 系のサービスと連携すると、サービス側の調整(データ準備とか)が","面倒","デバッグ機能が","貧弱","とても","ストレスフル","です。単純な GAS なら別に良いんですが、少し複雑な GAS を作ろうと思うと、問題に感じます。","ローカルで動かそう","GAS をローカル環境で動かすことができる Clasp というコマンドラインツールが Google より公開されています。 https://github.com/google/clasp また、Clasp は Typescript をサポートしているため、型を中心としたコーディングが可能となりました。 https://www.npmjs.com/package/@types/google-apps-script Typescript を選択すると、Interface 設計が容易になります。もちろん、"," ファイルでも同様の事は実現できると思います。 次に、Jest と呼ばれるテストツールを組み合わせることで、ローカル環境でテストが可能になります。 https://jestjs.io/docs/getting-started ただ、単純にテストコードが書けません。 例えば、カレンダーイベントを取得するテストをコーディングするとき、次のようなスクリプトを書いたとします。","こう書いてしまうと、本当のカレンダーイベントを取りに行ってしまいます。テストであれば、そういった処理は避けたいところです。 そこで、"," を偽物のオブジェクト、つまり Mock オブジェクトに差し替えるため、依存性逆転の原則(dependency inversion principle)を適用します。","このようなインターフェース・クラスを準備し、先程のコードを次のようにします。","結果、"," の代わりに Mock オブジェクトを差し込めるようになりました。ローカルテストが可能となります。 もちろん、プロダクトコードでは、"," ではなく、 "," を使用すれば良いです。 Mock で差し替えるオブジェクトが増えると、InversifyJS のような DI コンテナを検討してみると良いかもしれません。 https://github.com/inversify/InversifyJS こうすることで、Jest によるテストが動作するようになります。 実際に、開発・公開したライブラリでも十分にテストをすることができました。 https://www.npmjs.com/package/@silverbirder/caat","ライブラリとして提供する機能のテストが、たったの","約 3 秒","で終わります。 ","ストレスフリー","にローカル開発が可能となりました。 詳しくは、実際に作ったライブラリのソースコード(",")を御覧ください。","終わりに","GAS は、とても便利です。生産性が向上します。 サクッと API を構築できますし、G Suite との連携も(当たり前ですが)簡単です。 ただ、メンテナンス性が低いコードになると、","陳腐化され誰も面倒が見れなくなります","。 常にクリーンであり続けるためには、テストコードは","必須","です。 GAS を運用する方々には、是非ともテストコードを検討下さい。","え、あ、ちょっとまって。ライブラリの紹介！","アジャイル開発で、かつ、Google Calendar で予定管理しているチームには是非とも使って頂きたいライブラリです。 https://github.com/silverbirder/caat","CaAT is the Google Apps Script Library that Calculate the Assigned Time in Google Calendar.","このツールでできることは、次のとおりです。","指定期間における特定ユーザーの Google Calendar で予定されている時間(分)を取得 重複している予定は、連続した予定とみなす 指定の時間・単語は、計算対象外とみなす (ランチなど） 誰がいつ休みなのか、終日イベントから取得","実際にサンプルコードがあるので、ご参考下さい。 https://github.com/silverbirder/SampleCaat"],"t":"Google Apps Script でも テスト がしたい！ (Clasp + Typescript + Jest)"},{"f":"src/routes/blog/contents/things_to_keep_in_writing_unit_tests/index.mdx","c":["ユニットテストを書く上で守りたいこと","背景","普段、業務でユニットテストを書いたり、レビューをしたりしています。 ユニットテストがあることは良いことなのですが、困ったことがあります。 それは、ユニットテストのルールが明確じゃない点です。 そのため、人によって、ユニットテストの書き方がマチマチで、なんとかしたいなと困っています。","１つのテストケースに、Assert が複数あり、散在している。 似たようなテストデータの構築が、いろいろなテストケースにある。 テストする関数名が、パット見よくわからない。","本記事では、ユニットテストを書く上で守ってほしいことをピックアップしました。","守って欲しいこと","1. 可読性のあるテストコード","1.1. テストメソッド名の統一","テストメソッドの命名規則は、あったほうがよいです。 テストランナーツールの実行結果では、関数名を表示されることがあります。 そこから『どういうテストをしていたのか』が読んで分かると、わざわざテストの中身を見る必要はありません。 下記のような命名規則が良いそうです。","テスト対象のメソッドの名前。 それがテストされるシナリオ。 シナリオが呼び出されたときに想定される動作。","※","より引用 ※『正しく表示されていること』という期待値は、極力避けましょう。具体的な値を期待値に設定しましょう。","1.2. 論理制御を避ける","if や for 等の制御は、テストコードの見通しが悪くなるため、極力避けましょう。 共通化するよりも、愚直に書いたほうが読みやすいです。パフォーマンスは、そこまで気にしなくて良いですから。","1.3. マジックナンバーを避ける","プロダクトコードでは、ちゃんとマジックナンバーを避けていても、 テストコードでは、マジックナンバーを使っていることがあります。 横着せずに、ちゃんと適切な変数名で表現しましょう。","1.4. テストデータを外部ファイルに切り出す","テストコードを書いていると、テストに必要なテストデータの構築をする必要があります。 テストケース毎に書いても良いですが、見通しを良くするためにテストデータを別ファイルに分けましょう。 ただ、２つ以上のテストケースで参照される場合のみにしておきましょう。 テストデータは、プログラミング言語の相性に良い構造化ファイルを使うと良いです。","JSON CSV XML YAML","1.5. テストケースの構成を統一する","テストケースは、3 つの構成で記述すると読みやすいです。","Arrange (準備) Act (実行) Assert (検証)","これらを上から順番に実行されるようにテストコードを書いていきましょう。","のように順番を Mix するよりも、","のように順番を揃えるほうが読みやすいです。","1.6. １つのテストケースで複数の検証をしない","１つのテストケースに、複数の検証を行ってしまうと、何のテストケースなのかわかりにくくなってしまいます。そのため、検証するのは 1 つに絞るようにしましょう。 例外としては、コンストラクタの検証です。インスタンス化したオブジェクトの属性（プロパティ）値を検証することがあるので、その際は、複数の検証を許可します。","1.7. テストコードは最小限に","テスト対象のインスタンスに、","検証しない・不必要な","データの設定することは、余計な混乱を招くため、やめましょう。 また、設定するデータは、hoge などの意味のない値を設定するのではなく、本物に近いデータを設定しましょう。","2. Matcher の活用","2.1. 自然言語に近い形式で Assert する","検証すべき変数を Boolean の形に変換し、AssertTrue することが多々あります。 しかし、読みやすさの観点でいうと、Matcher に標準で備わっている関数を使う方が読みやすいです。","2.2. 変数名をわかりやすくする","テストコードを書くときは、期待する値を Expect, 実際の値を Actual という 変数名にしましょう。そうすると、何を検証しているのかわかりやすくなります。 また、モックやスタブ、スパイと言ったテストコードでよく出てくる用語を変数名で使用する際、きちんと使い分けをしましょう。モックという変数名なのに、実際はスタブのような使い方をしていると、誤解を招いて困ります。","3. テストパターン","3.1. ホワイトボックステストとブラックボックステスト","テストコードを書いている人は、基本的には内部仕様を把握しているはずなので、ホワイトボックスでテストを書くことが多いと思います。 また、網羅性を高めるために、パラメータテスト（組み合わせテスト）や、同値、境界値のようなブラックボックステストも書くのも良いでしょう。","3.2. 正常・準正常・異常系","| 種類 | 内容 | | -------------- | -------------------------------------------- | | 正常系テスト | 標準的な振る舞いをテスト | | 準正常系テスト | バリデーションエラー等のエラーに対するテスト | | 異常系テスト | API 接続エラー等のエラーに対するテスト |","3.3. Fail のテスト","テストコードに、予期しない処理が走った場合、強制的に Fail にさせることも大切です。 例えば、『非推奨な機能が使用された場合、Fail にさせる』は便利です。","3.4. private メソッドの検証","private メソッドの検証は、public メソッド経由で検証しましょう。","4. 大量にあるテストコード","4.1. プロダクトコードと同じ階層構造","プロダクトコードと同様に、テストコードもフォルダ毎にグループピングすると良いでしょう。 横並びにテストコードがあると、読みにくくなるからです。 例えば、プロダクトコードと全く同じ階層構造のフォルダ構成にすると良いかも知れません。","4.2. 単位を分けたテストコード","次のような単位でグルーピングすると良いです。","初期化処理を含めた単位でグループ化","ex. 在庫している商品が\"空\"の場合、 \"1 つ\"だけの場合、 \"複数\"の場合","よくあるテストクラスを継承してテストクラスを作るケースがありますが、避けたほうがよいでしょう。いつの間にか、必要以上の巨大な機能が備わってしまっています。Setup や Teardown に、必要な処理をセットしましょう。","4.3. テスト実行時間が長くなってしまったら","下記のいずれかを試すと良いでしょう。","テストの実行環境を強化する（マシンスペック） 並列にする 実行するテストを絞り込む","そもそものプロダクトコードのパフォーマンスが悪いのであれば、そちらを改善するのが一番良いでしょう。","終わりに","書いていて、感じたことは『テストコードは、読みやすさが大事』という点です。 プロダクトコードの書き方と、テストコードの書き方は、似ているようで違います。 プロダクトコードでは、保守性の高い洗練されたクラス設計や、ハイパフォーマンスが発揮される処理が求められることがあります。しかし、テストコードはそれらを求められていないと思います。 テストコードには、プロダクトコードの仕様を","素直に表現されたドキュメント","として扱われるべきです。 意図が伝わりづらいテストコードがあった場合、『このテストケースが落ちているけど、なんで？』と疑問を持ち、『落ちているのが正しいの？』と疑心暗鬼に陥ってしまいます。それはとても残念です。そんなことにならないためにも、テストコードは、『読みやすさ』を大切にして設計すべきと思います。","参考文献"],"t":"ユニットテストを書く上で守りたいこと"},{"f":"src/routes/blog/contents/think_micro_frontends/index.mdx","c":["Micro Frontends を調べたすべて","import { Image } from \"~/components/image/image\"; Micro Frontends に関わる記事を 100 件以上読みました(参考記事に記載しています)。そこから得た Micro Frontends についてこの投稿に記録します。 また、調査メモについて、次のリポジトリに残しています。 https://github.com/silverbirder/think-micro-frontends","発端","https://www.thoughtworks.com/radar/techniques/micro-frontends","実績企業","Airbnb Allegro Amazon Beamery Bit.dev BuzzFeed CircleCI DAZN Elsevier Entando Facebook Fiverr Hello Fresh IKEA Klarna Microsoft Open Table OpenMRS Otto Paypal SAP Sixt Skyscanner Smapiot Spotify Starbucks Thalia Upwork Zalando ZEISS","ProsCons","Pros","| 観点 | 内容 | | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | 独立性 | ・任意のテクノロジーと任意のチームで開発可能  | | 展開 | ・特定の機能をエンドツーエンド(バック、フロント、デプロイ）で確実に実行可能 | | 俊敏性 | ・特定のドメインについて最高の知識を持つチーム間で作業を分散すると、リリースプロセスが確実にスピードアップして簡素化される。 ・フロントエンドとリリースが小さいということは、リグレッションテストの表面がはるかに小さいことを意味する。リリースごとの変更は少なく、理論的にはテストに費やす時間を短縮できる。 ・フロントエンドのアップグレード/変更にはコストが小さくなる |","Cons","| 観点 | 内容 | | -------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | 独立性 | ・独立できず、相互接続しているチームが存在しがち ・多くの機能で複数のマイクロフロントエンドにまたがる変更が必要になり、独立性や自律性が低下 ・ライブラリを共有すること自体は問題ないが、不適切な分割によって作成された任意の境界を回避するための包括的な場所として使用すると、問題が発生する。 ・コンポーネント間の通信の構築は、実装と維持が困難であるだけでなく、コンポーネントの独立性が取り除かれる ・横断的関心事への変更ですべてのマイクロフロントエンドを変更することは、独立性が低下する | | 展開 | ・より大きな機能の部分的な実装が含まれているため、個別にリリースできない ・サイト全体の CI / CD プロセス | | 俊敏性 | ・重複作業が発生する ・検出可能性が低下した結果、一部の標準コンポーネントを共有できず、個別のフロントエンド間で実装が重複してしまう。 ・共有キャッシュがないと、各コンポーネントは独自のデータセットをプルダウンする必要があり、大量の重複呼び出しが発生する。 | | パフォーマンス | ・マイクロフロントエンドの実装が不適切な場合、パフォーマンスが低下する可能性がある。 |","統合パターン","https://bluesoft.com/micro-frontends-the-missing-piece-of-the-puzzle-in-feature-teams/ | 統合 | 選択基準 | 技術 | | ------------------ | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- | | サーバーサイド統合 | 良好な読み込みパフォーマンスと検索エンジンのランキングがプロジェクトの優先事項であること | ・Podium ・Ara-Framework ・Tailor ・Micromono ・PuzzleJS ・namecheap/ilc | | エッジサイド統合 | サーバーサイド統合と同じ | ・Varnish EDI  ・Edge Worker CDN ・ Akamai ・ Cloudfront ・ Fastly ・CloudFlare ・ Fly.io | | クライアント統合 | さまざまなチームのユーザーインターフェイスを 1 つの画面に統合する必要があるインタラクティブなアプリケーションを構築すること | ・Ajax ・Iframe ・Web Components ・Luigi ・Single-Spa ・FrintJS ・Hinclude ・Mashroom | | ビルド時統合 | 他の統合が非常に複雑に思われる場合に、 小さなプロジェクト（3 チーム以下）にのみ使用すること | ・ Bit.dev ・ Open Components ・ Piral |","機能","コミュニケーション","https://developer.mozilla.org/ja/docs/Web/API/CustomEvent https://github.com/postaljs/postal.js","データ共有","ストレージ","URL Cookie Local Storage/Session Storage","モジュール共有","webpack","https://webpack.js.org/concepts/module-federation/ https://webpack.js.org/configuration/externals/ https://webpack.js.org/plugins/dll-plugin/","ルーティング","Vaddin router https://vaadin.com/router","キャッシュ","https://developer.mozilla.org/ja/docs/Web/API/Service_Worker_API https://developer.mozilla.org/ja/docs/Web/API/IndexedDB_API","認証","JWT","https://jwt.io/","計測","Google Analytics Navigation Timing API Resource Timing API High Resolution Time API User Timing API Frame Timing API Server Timing API Performance Observer","Real User Monitoring","SpeedCurve Catchpoint New Relic Boomerang.js Parfume.js sitespeed.io","Synthetics Monitoring","Lighthouse WebpageTest","Proxy","コンポジションプロキシ。テンプレートを組み合わせる。 https://github.com/tes/compoxure","アクセス履歴","https://developer.mozilla.org/ja/docs/Web/API/History_API","分割ポリシー","フロントエンドを分割する方針について","水平分割","画面内にある要素で分割 ビジネス上の機能","垂直分割","画面毎に分割","Web サイト ⇔Web アプリ","マイクロフロントエンドは、かなりのオーバーラップがあるバンドの中央部分の大部分に最も適しています。バンドの両極端に該当するプロジェクトにマイクロフロントエンドアーキテクチャを実装しようとすると、生産性に反するそうです。","リポジトリ","| パターン | Pros | Cons | 技術 | | ---------- | ------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------- | --------------------- | | モノリポ | コードベース全体に簡単にアクセスできる。  (検出可能性が高い) | モノリポジトリは、特に大規模なチームで作業しているときに、 動作が遅くなる傾向があり、バージョン管理下のコミットとファイルの数が増加する。 | ・nx.dev ・lerna | | マルチリポ | ・マルチリポジトリは、非常に大規模なプロジェクトと それに取り組む非常に大規模なチームがある場合に最適。 | マルチリポジトリ環境では、各マイクロアプリを 個別にビルドする必要がある。 | |","他アーキテクチャ","| アーキテクチャ名 | 関係リンク | | -------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | Modular Monolith | ・","・"," | | Enterprise Architecture (Clean Architecture) | ・"," | | Jam Stack | "," | | App Shell | "," |","書籍","https://www.manning.com/books/micro-frontends-in-action","参考記事","rchaves"],"t":"Micro Frontends を調べたすべて"},{"f":"src/routes/blog/contents/this_body_is_made_of_ponderings/index.mdx","c":["この体はポンデリングで出来ている","import { Image } from \"~/components/image/image\"; ミスタードーナツ、それは至高のドーナツを生み出すお店。 私とミスタードーナツの出会い、別れ、そして再開について、話します。","ミスタードーナツとの出会い","出会いは、私が高校生の頃、実家で暮らしていたときの朝食です。 当時、私には大学生の姉がいて、姉がミスタードーナツでアルバイトとして働いていました。 ミスタードーナツを夜勤(ナイト)で、姉は働いていました。 帰りに破棄前のドーナツをダース箱で貰っていました。 それを、実家に持ち帰り、私達の朝食になっていたのです。 生クリームが詰まったエンゼルフレンチが大好きになりました。 パンというより、デザート感覚で食べていました。 そこから、沼にハマりました。","ミスタードーナツでアルバイト","私が大学生になった頃、ミスタードーナツでアルバイトをしました。 当時、アルバイトの周りの人は、女性ばかりで肩身が狭かった印象です。 アルバイトのシフトは、もちろん、ナイト希望です。 実際、ナイトで入らせて貰いました。 働いていると、色々と大変だと思うことが多かったです。","ドーナツは、バーコードがついていないので、商品を覚えてレジ打ちをする","季節によって、どんどん商品が変わってくる","ラーメンやチャーハンなどの作る手順を、覚える必要がある たまに、季節のコスプレをさせられる","ハロウィンとか","おかわりコーヒーのために、店内を回らないといけない","ただ、その代わりに、廃棄前のドーナツをもらえることが最高でした。 ※ 昔も今も、\"廃棄前のドーナツをもらうこと\"は、タブーな話なのかもしれないです。","社会人になりミスタードーナツと別れ","社会人になると、ミスタードーナツを食べる機会が減りました。 オフィスの近くに、ミスタードーナツがそもそもなかったり、 あったとしても、業務多忙で、なかなかいけなかった記憶です。","リモートワーク普及によりミスタードーナツを再び食べる","コロナの環境で、リモートワークが普及し始めた頃、引っ越しをしていました。 引越し先の近くにミスタードーナツを発見しました。やったね。最高。","そして、いま","購入履歴を見ると、週に 1 回は、ミスタードーナツに 500 円程度消費しているようです。","その他(食べ放題)","食べ放題あるんですよ、ミスタードーナツ。","https://aumo.jp/articles/332895","実際に行ってみたんですが、そんなに食べれなかったです。 たまに、少し食べる程度がちょうどよいんだと、思いました。"],"t":"この体はポンデリングで出来ている"},{"f":"src/routes/blog/contents/tiqav2_release/index.mdx","c":["1コマ漫画検索サービスTiqav2 (Algolia + Cloudinary + Google Cloud Vision API) 作ってみた","import { Image } from \"~/components/image/image\";","画像で会話って楽しい","皆さん、チャットツールでコミュニケーションするとき、絵文字や画像って使ってますか？ 僕はよく使ってます。人とコミュニケーションするのに、文字だけだと","堅苦しい","イメージですよね。 例えば、 『OK です、それで先に進めて下さい。』 というフレーズだけだと、相手がどのような感情なのか読み取りにくいです。 そこで、次のような画像でコミュニケーションを取ると、柔らかい印象を与えることができます。","Tiqav2","Tiqav とは？","画像を使って会話をするためのサービスとして、Tiqav があります。","現在は、サービス終了しています。 Tiqav2 は、その Tiqav を参考にして作りました。","Tiqav2 とは？","Tiqav2 は、大きく分けて 2 つの機能があります。","画像とテキストを保存 画像を検索&表示","2 つの機能","画像とテキストを保存","検索する為には、全文検索サービスの Algolia を使います。 https://www.algolia.com/ Algolia に保存する情報は、主に 3 つです。画像 URL と拡張子、そしてテキストです。 画像は画像変換&管理サービスの Cloudinary に保存します。保存後、Cloudinary より、画像 URL と拡張子が手に入ります。 https://cloudinary.com/ テキストは、Google Cloud Vision API へ画像を渡すことでテキストを抽出します。 もちろん、手動でテキストを設定することもできます。 https://cloud.google.com/vision/","画像を検索&表示","テキストで全文検索を行います。その結果の ID と Extension を組み合わせることで、 画像を表示することができます。Extension の種類は、Cloudinary のサポートするもの全てになります。","https://cloudinary.com/documentation/image_transformations#supported_image_formats この画像を表示する機能を使うと、次のように Slack 上で画像を送信することができます。 詳しい機能は、次のリポジトリをご確認下さい。 https://github.com/silverbirder/tiqav2","SaaS は個人開発には最適","今回、全文検索であったり画像管理は、SaaS に全て任せました。テキスト抽出はなくてもよかったのですが、Google Cloud Vision API が、そこそこ使えたため、そちらも使いました。 個人で開発する際、リソース（時間、お金、人）は組織に比べて","とても小さい","です。 SaaS は、１つのことを上手くやってくれるし、個人の利用範囲であれば無料なものが多いです。 ニッチなカスタマイズしたい要求がない限り、SaaS は大体の要望を叶えてくれます。 どんな種類の SaaS があるか知りたい方は、↓ のサイトを見てみて下さい。参考になるはずです。 https://saasblocks.io/","SaaS に面倒なことは任せて、プロダクトコードに集中する","ことは、私にとって、とても大切にしています。 ちなみに今回のプロダクトコードは、CleanArchitecture + InversifyJS で作りました。","終わりに","Tiqav2 は、OSS として公開していますので、誰でも無料で構築できます。 ぜひ、使ってみて下さい。快適なコミュニケーションを目指しましょう！"],"t":"1コマ漫画検索サービスTiqav2 (Algolia + Cloudinary + Google Cloud Vision API) 作ってみた"},{"f":"src/routes/blog/contents/try_connect_web/index.mdx","c":["connect-webやってみた","が、はてブでトレンドになっていました。気になったので、試してみました。 サンプルコードは、次のリポジトリに置いています。","https://github.com/silverbirder/playground/tree/main/node/connect-web-example/frontend","前置き: gRPC と connect-web の雑な理解","RPC (Remote Procedure Call) を実現するためのプロトコルとして、gRPC があります。 このプロトコルは、ブラウザ側からは使えない(?)ため、gRPC-Web というブラウザ向けの gRPC というものを使うことになります。 その場合、ブラウザとサーバーとの間に、プロキシを建てる必要があるようです。(たぶん) そこで、Connect という gRPC 互換の HTTP API を構築するためのライブラリ群が開発されました。 これのおかげで、プロキシを建てる必要がなく、ブラウザ側から gRPC を使うことが可能になります。","https://connect.build/docs/introduction","上記ページに、バックエンドは connect-go、フロントエンドは connect-web という項目があります。 connect-web は、ブラウザから RPC を動かすための小さなライブラリです。タイプセーフなライブラリなため、 型補完が効きます。 connect-go は、go で Connect のサービスを作ることができます。 そのため、フロントエンドの開発は、connect-web を使うことになります。 以降は、フロントエンドの作業を、紹介します。ちなみに、React を使います。","やってみた","フロントエンド側は、主に、次の 2 つの作業になります。","Protocol Buffer スキーマから TypeScript ファイルを生成 生成された TypeScript ファイルから gRPC クライアントを実装","1. Protocol Buffer スキーマから TypeScript ファイルを生成","gRPC で通信するためのスキーマ、ProtocolBuffer スキーマが必要です。 これは、すでにあるものを使います。","https://buf.build/bufbuild/eliza","具体的には、次のようなスキーマです。","TypeScript コードを生成するために、"," という CLI を使います。 buf で利用する、次の定義ファイルを書きます。","これは、後述する "," するときにどういう出力をするかの設定情報です。 codegen の yaml ファイルみたいなものかなと思います。 これを動かすために、次の module をインストールしましょう。","plugin","protoc-gen-es","リクエストやレスポンスメッセージのような基本型を生成","protoc-gen-connect-web","Protocol Buffer スキーマからサービスを生成","runtime","bufbuild/connect-web","Connect および gRPC-web プロトコルのクライアントを提供","bufbuild/protobuf","基本型に対するシリアライズなどを提供","次に、","をインストールしましょう。 私は、brew でインストールしました。","では、ProtocolBuffer スキーマから TypeScript ファイルを生成しましょう。","成功すると、次の 2 つの TypeScript ファイルが生成されます。","gen/buf/connect/demo/eliza/v1/eliza_connectweb.ts gen/buf/connect/demo/eliza/v1/eliza_pb.ts","は、次のコードが含まれています。","これで、準備はできました。","2. 生成された TypeScript ファイルから gRPC クライアントを実装","では、gRPC のクライアントを実装しましょう。 gRPC のクライント生成は、"," でできます。 生成時の引数に、サービスとトランスポート(?)というものを渡す必要があります。 コードを見たほうがわかりやすいと思うので、次のコードを見てください。","このクライアントを、使ってみましょう。","このように、ProtocolBuffers の ElizaService が、型補完として使えるようになります。 良い感じです！","終わりに","意外とあっさり動いて、びっくりしました。"],"t":"connect-webやってみた"},{"f":"src/routes/blog/contents/try_dagger/index.mdx","c":["CI/CDのDaggerで、GithubActionsとCircleCIにシュッと連携してみた","前々から気になっていた、CI/CD の非ベンダーロックインな Dagger というツールを試してみました。 本記事では、試した内容について共有しようと思います。","CI/CD のパイプラインを書く","Dagger では、"," という言語を使って CI/CD のパイプラインを書きます。 ","から、そのまま使ってみます。 コードは、次のようなものになります。","見慣れない構文かもしれませんが、何をやっているかはなんとなく分かるんじゃないかなと思います。 actions は、実行するものを定義していて、"," のようにして使います。 上の定義にある"," は、 "," がアクション名で、"," が実行するパッケージになります。 パッケージは、次の 2 つに分類されます。","標準機能","core","非標準機能","yarn, netlify, aws, bash, etc","ローカル環境で Dagger を動かす","実際にローカルで動かしてみます。","特に問題なく、PASS しています。"," をつけると、実行の詳細な情報が出力されます。","ちなみに、"," が実行されているのは、","が","に依存しているためと思います。","ローカル環境で、CI/CD のパイプラインコードを動かくことができました。 次は、CI と連携したいと思います。","CircleCI で Dagger を動かす","まずは、CircleCI で Dagger を動かしてみます。 CircleCI の yml ファイルは、次の定義になります。","CircleCI の環境変数に、","と","を設定しておきます。 この定義ファイルは、Dagger をインストールして、先程ローカル環境で動かしていた "," や "," を実行しているだけです。 この定義は、CircleCI 上で PASS します。めちゃくちゃ簡単ですね。","GithubActions で Dagger を動かす","次は、GithubActions で Dagger を動かしてみます。 GithubActions の yml ファイルは、次の定義になります。","ここの定義も、CircleCI の定義とほとんど一緒だと思います。 ただ、少し違うのは、GithubActions では、"," が使えるため、 ","が、"," のように、","を書かなくて済むようになります。 GithubActions の環境変数に、","を設定しておきます。 そうすれば、このパイプラインも成功します。","終わりに","ローカル環境で、CI/CD のパイプラインをテストできて、それをシュッと CI サービスに連携できました。 今回は、チュートリアルのものをそのまま使っているので、テストやデプロイがシンプルな構成になっていましたが、 実務になると、より複雑な構成になると思うので、手元で確認できるのは良いものと思いました。 ただし、","への学習コストがかかるため、導入する際は、そのあたりも含めて検討しましょう。"],"t":"CI/CDのDaggerで、GithubActionsとCircleCIにシュッと連携してみた"},{"f":"src/routes/blog/contents/twitter_cotlin/index.mdx","c":["TwitterにあるLinkを収集するツール Cotlin で、世界中のプレゼンテーション資料を知ろう","Twitter に投稿されている Link を収集するツール Cotlin を作りました。 から、Cotlin という名前にしました。Android のアレに似ています。 https://github.com/silverbirder/Cotlin https://www.npmjs.com/package/@silverbirder/cotlin","動機","私は、","等を使って、技術系のカンファレンスに参加することがありました。 カンファレンスで発表された資料は、Twitter で公開されることが多々あるので、それを自動収集できるようにしたいと考えたのが、Cotlin を作った動機です。","技術スタック","Google Apps Script (",") + Twitter API (",") です。 ","していて、それを元に","と","を簡単に作れるようにサンプルコードも用意しています。使い方は、全て","にあります。","困ったこと","Tweet に記述したリンクは、全て","に短縮されてしまいます。 この短縮 URL からオリジナル URL を手に入れるために、リダイレクトする必要があります。 実際に作ってみると、次のような記事に書いたとおり、GAS で書くと、少し困ったことがありました。 https://silverbirder.github.io/blog/contents/gas_fetchall_redirect そこで、複数のリダイレクト URL へリクエストする処理を並列化するために、Golang で開発していました。 しかし、そもそも Twitter API (",")のレスポンスには、オリジナル URL が含まれている(",")ことに気づき、結局、Google Apps Script (",")で開発することになりました。","良かったこと","毎日、プレゼンテーション資料を収集し、スプレットシートに記録するよう、自動化しました。 ※ 都合により、URL のリンクと Tweet のリンクのみ記載しています。 https://docs.google.com/spreadsheets/d/1IaJOw9-GdoHhz3D0CzvJfFitrmEN8KpgIleer9rmxiw/edit?usp=sharing 次のような資料を発見できるようになりました。","世界中","のプレゼンテーション資料","知らない技術カンファレンス","の資料","個人や学生","が公開した資料","そこから、次のような良かったことがありました。","『テストについて、同じように困っている人がいた。○○ という技術で解決してるんだ！知らなかった！』 『最近気になっている ○○ のアーキテクチャの資料だ。メリット・デメリットがよくまとまってて良い！』 『リモートワークを取り組んでいる企業の話だ。これから私もリモートワークになるから、先に知見を知っておこう！』","毎日、資料を読んでいると、1 日 2,3 件ほど、自分の琴線に触れるものが現れます。とても刺激を受けて、作ってよかったなと思いました。","最後に","動機であった技術カンファレンスの参加は、実は最近減少しています。 理由は色々ありますが、まあ伏せておきます。 こういったツールによって、アンテナを広く張り巡らせておくことができます。 これのおかげで、様々な関心事をキャッチアップできるようになりました。 上のスプレットシートは、ずっと更新し続ける予定ですので、活用下さい。"],"t":"TwitterにあるLinkを収集するツール Cotlin で、世界中のプレゼンテーション資料を知ろう"},{"f":"src/routes/blog/contents/unleash_feature_flag/index.mdx","c":["Unleashで始めるフィーチャーフラグ","Unleash","について解説します。 tags: [\"unleash\"]","import { Image } from \"~/components/image/image\"; フィーチャーフラグ（Feature Flag）をご存知でしょうか？これは新機能のリリース制御や AB テストを容易にする強力なツールです。しかし、適切な管理ツールなければ、フィーチャーフラグの管理は容易なことではありません。今回は、そんなフィーチャーフラグの管理を効率化するツール、","について解説します。 Unleash のリポジトリはこちら 👉","フィーチャーフラグの課題と解決策","フィーチャーフラグとは、新機能の活性化・非活性化を制御するためのブール値のスイッチをコードに組み込む手法です。カナリアリリースや AB テストのように、特定のユーザーにだけ新機能を表示したり、新機能のパフォーマンスをテストしたりするために用いられます。 フィーチャーフラグの管理方法はいくつかありますが、それらは環境変数、定義ファイル（CSV、YML、JSON）、データベースなどが一般的です。しかし、これらの管理方法にはフラグの切り替えに時間がかかる、不要なフラグの削除を忘れがちなどの問題点があります。ここで紹介したいのが、フィーチャーフラグ管理ツール","です。","なぜ Unleash なのか？","Unleash は、フィーチャーフラグの管理を容易にするオープンソースツールです。様々な言語やフレームワークで利用でき、新機能の制御、テストの効率化、機能の並行作業などを可能にします。 Unleash の主な特長は以下の通りです。","GUI ベースの操作","：フラグの ON/OFF 切り替えが簡単で、フラグの管理が非エンジニアでも可能です。(データアナリストとか)","柔軟なテスト機能","：AB テストだけでなく、多変量テストも可能です。","豊富な連携","：Slack や Datadog などと連携してフラグの状態を共有できます。","API first","：他のアプリケーションやシステムとの連携が容易です。","セルフホスティングとクラウドサービス","：自分でホスティングすることも、クラウドサービスを利用することも可能です。","これらの特性により、Unleash はフィーチャーフラグの管理を効率化し、開発フローを加速します。","Unleash のセットアップとフィーチャーフラグの使用方法","それでは、Unleash を実際に使用する流れを紹介していきます。今回は Unleash でフィーチャーフラグを一つ作成し、それを React から参照するというシナリオとします。 Unleash のセットアップについては、公式ドキュメンテーションの","を参考に、まずはローカル環境で試してみましょう。クラウド環境での構築を希望する場合は、PostgreSQL が無料で利用可能な","を推奨します。 構築が完了すると、Unleash の画面が表示されます。 フィーチャーフラグを作成してみましょう。","をクリックし、名前には","を入力します。 続いて、React から Unleash への接続に必要な API token を生成します。Unleash の画面上部から "," を選択します。"," をクリックし、API token を作成します。 今回、クライアントサイド(React)で Unleash を利用するため、"," を選択して API token を生成します。もちろん、サーバーサイドから Unleash を利用することも可能です。 ここで、environment を","に設定します。これにより環境ごとに API token を設定できます。production 向けの API token が必要な場合は、別途生成します。 次に生成した API token を "," に設定し、以下の React のコードで使用します。","ここでは、フィーチャーフラグ","を使用する例を示します。","この設定により、Unleash の GUI からフィーチャーフラグをリアルタイムで ON/OFF することが可能になります。変更結果は Unleash 経由でフロントエンドにリアルタイムで反映されます。 以下にフィーチャーフラグのデモを示します。都合上、","の environment を使用しています。以下の画像をクリックすると動画を閲覧できます。 以上が Unleash の基本的な使用方法です。","Unleash 機能: Activation Strategies","Unleash では、フィーチャーフラグの活性化を制御するための Activation Strategies を設定することができます。","Activation Strategies は以下の 4 つがあります。","Gradual rollout","ユーザーの一部に対して段階的に展開します。また、各ユーザーが訪問するたびに同じ経験が提供されることを保証します。","Standard","このストラテジーは全てのユーザーに対してフィーチャーをオン/オフします。 ただし、Gradual rollout ストラテジーを使用することを推奨します。","IPs","特定の IP アドレスのセットに対してフィーチャーを有効にします。","Hosts","特定のホスト名のセットに対してフィーチャーを有効にします。","以下の画像は、Gradual rollout の設定画面です。","特に、Gradual Rollout ストラテジーは AB テストとしても利用可能です。","Unleash 機能: Variant","Unleash では、全てのフィーチャーフラグに Variant という機能が用意されています。これにより、ユーザーをより詳細なセグメントに分けることが可能となります。","例として、先ほど作成した new_feature のフィーチャーフラグに対して、以下の画像の通り、4 つの Variant を設定しました。それぞれの Variant は設定した "," に基づいて選択されます。 以下に、new_feature の Variant を使用する例を示します。"," には、先ほど設定した Variant の name が入力されます。そのため、この","を基に処理を分岐させることが可能です。","このようにして、AB テストだけではなく、多変量テストを行うことが可能となります。","Unleash 機能: Toggle type","Unleash では、フィーチャーフラグに様々なタイプが設定できます。各タイプは寿命(ライフタイム)が異なり、寿命が尽きたフィーチャーフラグはマークされます。マークされたフィーチャーフラグは、アーカイブすることを推奨します。","フィーチャートグルのタイプは以下のように分類されます。","Release"," : 継続的デリバリーを実践するチームのトランクベース開発を有効にします。予想される寿命は 40 日です。","Experiment"," : 多変量テストまたは AB テストを実施します。予想される寿命は 40 日です。","Operational"," : システムの動作に関する運用面を制御します。予想される寿命は 7 日です。","Kill switch"," : システム機能のグレースフルな劣化を可能にします。(永続的)","Permission"," : 特定のユーザーが受け取るフィーチャーや製品体験を変更します。(永続的)","寿命が尽きたフィーチャーフラグは、技術的負債となります。フィーチャーフラグに関する技術的負債についての詳細な記事が以下のリンクで紹介されています。是非ご覧ください。","Unleash では、Health と呼ばれる指標が表示されます。この値が低いと、技術的負債が蓄積されているということを示します。フィーチャーフラグを適切にアーカイブする運用を心掛けましょう。","Unleash 機能: その他","Unleash には、他にも以下に述べるような様々な機能が備わっています。","環境の追加","や","以外にも、テストやステージング環境など、自由に環境を作成することができます。 詳細：https://docs.getunleash.io/how-to/how-to-clone-environments","Playground の利用","フィーチャーフラグの挙動を確認するための Playground が利用可能です。 アクセス URL：","CORS origins の設定","フロントエンドから Unleash サーバーへのアクセスを許可するための CORS 設定ができます。","フィーチャーフラグの承認フローの設定","フィーチャーフラグを有効化するための承認フローを設定することができます。 詳細：https://docs.getunleash.io/reference/change-requests","まとめ","フィーチャーフラグの管理に最適なツールとして Unleash が一押しです。Unleash を使用することで、フラグの切り替えが容易になり、デプロイフローを通さずに高速にフィーチャーフラグを管理することが可能となります。多様なテストを行い、その結果を元に製品の改善が行えます。製品開発のスピードと品質が大幅に向上しますので、ぜひ一度試してみてください。 最後までお読み頂き、ありがとうございました。バッジを贈ってね！","supported by ChatGPT"],"t":"Unleashで始めるフィーチャーフラグ"},{"f":"src/routes/blog/contents/using_stability_ai_API/index.mdx","c":["Stable Diffusion API 開発","import { Image } from \"~/components/image/image\"; Stable Diffusion は、文章を渡すと画像を生成してくれる AI で OSS です。 これを自分の PC で動かそうとすると、GPU が必要になります。 (CPU で動かせる"," というのもあります) できれば、どの PC でも使えるように、かつ、Slack などサービスと連携できるよう API がほしいなと思いました。 そこで、Stable Diffusion の API を開発しました。","結論","DreamStudio.ai の SDK、 ","を使いました。 成果物は、次のリポジトリに置いています。","https://github.com/silverbirder/stable-diffusion-API","ローカル環境でも、Docker コンテナでも、動きます。 動かすには、DreamStudio.ai の API Key が必要になります。 Docker で動くので、Docker をデプロイできるサービスなら、どこでも動きます。(GPU は不要です) 私は、GCP が好きなので、CloudRun というサービスにデプロイしました。 API は、とりあえず、"," というパラメータを受け取り、画像を返却します。 Slack で使ってみると、こんな感じになりました。 ひとまず、API で Stable Diffusion を動かせました。","GPU と設計","を使う前までは、自前で Stable Diffusion を動かす環境を用意しようと設計を考えました。設計の調査メモは、次のリンクにメモを残しています。","https://zenn.dev/silverbirder/scraps/3842c715662551","具体的に、次のようなパターンを考えました。","Google Colaboratory の GPU を使って Stable Diffusion を動かし、簡易な API で公開する サーバー(GCE や CloudRun など) で GPU を使って Stable Diffusion を動かし、簡易な API で公開する バッチ(Cloud Batch)で GPU を使って Stable Diffusion を動かし、必要なときに動かす。(API からバッチ処理をキックする)","1 番目は、Google Colaboratory の利用は 12 時間制限というのがあり、そこを回避する何かが必要なります。ただし、本来の用途と外れていると思うので、却下しました。 2 番目は、金銭的に数万~数十万円以上のランニングコストが発生するので却下です。 3 番目は、一番最初の構想したものです。2 番目のような GPU のサーバを常時起動しているとめちゃくちゃもったいないので、 バッチ処理として 3 番目の案を考えていました。3 番目で実際に構築してみると、(何が原因か深く調べていないですが) 起動に 30 分以上かかってしまい、使い物にならなさそうでした。 で、悩んだ結果、"," がメンテナンスやランニングコストも不要で、シュッとできそうだったことに気づきました。 もちろん、デメリットはあります。","SDK に依存するので、自身がコントロールできない(img2img できない) 課金制","しかし、個人レベルで利用するという前提でしたので、デメリットよりもメリットの方が大きいと判断しました。","stability-sdk","DreamStudio.ai は、Stable Diffusion を使っています。 API として、"," を公開しています。 使うには、Python で書く必要があります。 ソースコードを読むと、gRPC を使っているため、別言語で SDK を書くのは比較的簡単だと思います。 私は、Python でシュッと書けるので、flask と stability-sdk を使いました。","ひとまず、Prompt だけを受け付ける超絶シンプルな API を書きました。 ","は、様々パラメータがあるので、それも受け付けられるようにしようかなと思ったり、Midjourney の discord のボットのようなモノを書いても面白そうだなと思いました。","終わりに","マークダウンで、画像を読み込むときに、今回開発した API を指定すると、マークダウンを開いたタイミングで画像が毎回変わります。 prompt と seed を指定すれば固定できるんですけど、こういうのも面白いなと思っています。"],"t":"Stable Diffusion API 開発"},{"f":"src/routes/blog/contents/urql_transform_exchange/index.mdx","c":["urqlでデータ変換(transform)してみた","GraphQL クライアントを使っていると、データ取得後にデータ変換がしたくなりませんか。私はしたくなります。 GraphQL クライアントの urql で、データ変換するのに、exchanges が使えそうだったので、それを共有します。 サンプルコードは、次のリポジトリに置いています。 https://github.com/silverbirder/urql-exchange-transform","Exchanges","Exchanges とは、","より引用します。","The Client itself doesn't actually know what to do with operations. Instead, it sends them through \"exchanges\". Exchanges are akin to middleware in Redux and have access to all operations and all results. Multiple exchanges are chained to process our operations and to execute logic on them, one of them being the fetchExchange, which as the name implies sends our requests to our API.","ざっくりいうと、GraphQL の通信フロー(リクエスト/レスポンス)にアクセスできる機構です。レスポンスにアクセスできるため、 データ変換もできます。exchanges にデータ変換を一手に引き受けるため、useQuery などクエリ発行する側で、何度も変換コードを書く必要がなくなります。","Transform exchange","サンプルコードを紹介する前に、GraphQL のデータソースとして Pokemon を使います。 URL とクエリは、次のものを使います。","url","データ変換は、次のようなコードを書きます。 map の部分が、実際のデータ変換になります。今回は、name を","しています。","transformExchange 関数を urql のクライアントに渡します。","exchanges は、何も指定しない場合、","が使われます。今回、必要最低限の説明のために、","の内の fetchExchange だけ使いました。 あとは、次のコードのように useQuery でデータ取得すれば良いです。データ取得後のデータは、データ変換された結果になっています。","pokemon.name が "," されています。","終わりに","urql の exchanges って、","という","言語で書かれたライブラリに依存しているので、調査するのが少し苦労しました。"],"t":"urqlでデータ変換(transform)してみた"},{"f":"src/routes/blog/contents/web_frontend_test_pattern_guide/index.mdx","c":["Webフロントエンドにおける網羅的テストパターンガイド","import { Image } from \"~/components/image/image\"; こんにちは、テストが好きな silverbirder と申します。Web フロントエンドのテストは実施していますか？ユニットテストやビジュアルリグレッションテストは広く知られていると思います。しかし、パフォーマンステストのためのテストコードはありますか？また、カオスエンジニアリングテストやアクセシビリティテストはありますか？ 今回、私は Web フロントエンドにおける網羅的なテストパターンを調査し、その結果をここで紹介したいと思います。これらを理解することで、読者の皆さんが適切なテスト戦略を策定する際の参考になれば幸いです。","前提","今回、テスト対象として取り上げる題材は、","という TODO アプリです。フレームワークとして React を使用しますが、紹介するテストパターンはフレームワークに依存しないものです。ただし、使用するライブラリは React に関連しているため、その点についてはご了承ください。また、テストライブラリとして、Jest を使用します。 参考となるコードは次のリポジトリに用意していますので、ぜひご参照ください。 https://github.com/silverbirder/react-todoMVC/ また、動作するアプリケーションを Vercel で公開しています。こちらも参考にしてください。 https://silverbirder-react-todo-mvc.vercel.app コンポーネントの構造は以下の通りです。","App.tsx","TodoInput.tsx TodoList.tsx","TodoItem.tsx","TodoContext.ts","概要","まず、これから紹介するテストパターンの全体図を紹介します。 テストパターンは、大きく以下の 3 つに分類されます。","機能テスト 非機能テスト UI/UX テスト","それぞれの内容について、これから説明していきます。","テストパターン","機能テスト(Functional)","機能とは、システムやソフトウェア、製品、またはサービスが提供する特定の目的を達成するために実行されるタスクや操作を指します。Web フロントエンドにおける機能は、Web アプリケーションのユーザーインターフェース（UI）部分で実現されるタスクや操作を指します。これには画面表示や操作、ページ遷移や更新などが含まれます。 機能テストには、テストトロフィーやテストピラミッドが有名です。一般的に次の 3 つのカテゴリがあります。","ユニットテスト","関数やコンポーネントなど、単体レベルのテスト","インテグレーションテスト","複数の関数やコンポーネントが正しく連携するかのテスト","E2E(システム)テスト","フロントエンドからバックエンドまで一気通貫したテスト","ユニットテストとインテグレーションテストは、フロントエンドだけで完結し速度重視のテストです。一方、E2E テストはバックエンドと連携し、速度は遅く重たいものの、より本番環境に近い状態でテストを行います。 ユニットテストは一つの機能やコンポーネントに対して行うのに対し、インテグレーションテストではユーザーにとって意味のある単位で結合したコンポーネントに対してテストを行うことがお勧めです。画面を構成するページレベルのコンポーネントが適切でしょう。","まず、よく知られているユニットテストから始めましょう。TodoItem コンポーネントのテストを行ってみましょう。 TodoItem コンポーネントは以下の通りです。","このコンポーネントに対するテストコードは、以下のようになります。","特に目新しいテストコードはないでしょう。今回はコンポーネントに対するテストコードでしたが、コンポーネントから切り離されたロジックファイル（例えば、React の hooks）ももちろん対象です。 ユニットテストを作成する際には、以下の 3 つのポイントに注意しています。","Arange-Act-Assert（AAA）パターンを使用する","上から下に読みやすい構造にする","下から上を読み返さない","自然言語を意識して記述する","テスト実行後の出力メッセージが読みやすくなる","DRY 原則や、制御文（if/while など）を避け、愚直に書く","シンプルで直感的なテストコードを好む","ハードコードした変数も良い","1 つのテストファイルで内容を確認できるようにする","例えば、データファイルを別ファイルに分割しない","1 つのテストケースには 1 つの検証を行う","異なる目的の検証は、テストケースを分ける","脱線) なんでテストコードを書くのか","テストコードは、プロダクトを利用するユーザーではなく、開発するエンジニアのために存在すると、私は考えています。そのため、私が考えるテストコードの目的は以下の 3 つです。","開発効率の向上（キーストロークの削減） 機能品質の維持 仕様の明確化","開発効率の向上では、例えば多くの組み合わせパターンがあるスキーマバリデーションの場合、手動での確認は大変です。そこで、パラメタライズドテストやプロパティベースドテストを実施することで効率的に開発できます。 機能品質の維持では、CI/CD プロセスでテストを実行し、テストが成功した場合にのみデプロイするリリースフローを構築します。そうすると、デプロイされたプロダクトは、テストで記述された内容の品質を担保できます。 仕様の明確化では、テストコードを自然言語として分かりやすく書くことで、どのような機能があるのかが理解できます。仕様書をプロダクトコードから読み取るのは不親切ですが、テストコードが分かりやすい場合、機能の現状の動作がある程度把握できるようになります。","ユニットテストでは、個々のコンポーネントや関数が対象となります。一方、インテグレーションテストでは、1 画面に構成されるコンポーネントなど、ユーザーに価値を提供できる状態のものが対象です。今回の場合、TodoList.tsx や TodoInput.tsx を使用している App.tsx が該当します。 インテグレーションテストにおいては、バックエンドとの通信は模擬されますが、それ以外の要素はすべて実際のものです。ユニットテストではコンポーネント単体のみのテストでしたが、インテグレーションテストでは画面に表示される必要なコンポーネントが全て揃っています。そのため、コンポーネント間の連携を確認する目的で、ユーザー操作、すなわちインタラクションテストを行うことが効果的です。 インタラクションテストには、Storybook を利用すると便利です。 https://storybook.js.org/docs/react/writing-tests/interaction-testing 従来はターミナルでインタラクション（クリックなど）のテストを行っていたと思いますが、Storybook を用いることで視覚的に確認しながらテストが作成できるため、非常に開発しやすくなります。 それでは、1 つの機能として提供されている App.tsx を対象に、インタラクションテストを作成してみましょう。 まず、Storybook を作成すると、次のようなものができます。 :::message 今回紹介する Storybook のバージョンは 6.5 です。 :::","このように、Storybook では、Story に play を記述することでインタラクションを表現できます。Storybook 上でインタラクションを確認することができます。 それでは、これをテストに活用しましょう。","このように、Storybook の Story オブジェクトをテストコードに読み込み、play 関数を実行できます。したがって、Storybook で確認したインタラクションがそのままテストコードで動きます。後は、expect を書くだけです。 インテグレーションテストは、ユニットテストよりも非常に価値があるテストを多く実施できます。その理由は、ユーザー目線のテストが可能となるからです。ユニットテストでは、関数やコンポーネントの詳細なテストは可能ですが、それは基本的にエンジニア目線のテストになります。 ユニットテストよりもインテグレーションテストが多くなっても問題ありません。ユニットテストとインテグレーションテストのテストが重なるコードがあるかもしれませんが、完璧な必要最小限な網羅的なテストメンテンナンスは現実的ではないため、また、重なっていても困ることもそう多くない場合、許容して良いと思っています。","API 通信のモック","フロントエンドは、バックエンドとの通信を発生させることは多々あります。インテグレーションテストやユニットテストでは、API 通信をモックすることがよくあります。通信をインターセプトして、固定データを返却する mswjs/msw が有名です。また、固定データではなく、動的にデータを返却できる mswjs/data というものも便利なので紹介です。 https://github.com/mswjs/msw https://github.com/mswjs/data","E2E テスト(システム)","ここでは、フロントエンドから API までを含んだ一気通貫のテストを作成します。E2E テストでは、一般的にヘッドレスブラウザを使用したテストが行われます。私は、Playwright を好んで使います。 E2E テストの作成方法としては、エンジニア目線で細かいテストを行うよりも、ビジネス目線で要件を満たすかどうかを確認する受け入れテストを書くことが望ましいでしょう。そこで、ATDD として有名な","を用いたサンプルを紹介します。 cucumber では、受け入れのためのシナリオを BDD 形式の gherkin で記述します。また、","ができます。 それでは、簡単な受け入れテストのシナリオを作成してみましょう。以下のようなシナリオになります。","このシナリオを参考にして、テストコードを作成してみましょう。次のようなテストコードを記述します。"," は、Playwright を使用してヘッドレスブラウザを起動しているだけです。"," を実行すると、シナリオをもとにテストが実行されます。","このテストをリリース前に実施することで、シナリオで記述された内容が保証されます。ぜひ、リリース前の受け入れテストの実施をご検討ください。 ※ Screenplay パターンと呼ばれる、Actor、Task、World についてもお話ししたいのですが、情報量が多くなるため、今回は割愛させていただきます。","非機能テスト(Non-Functional)","バックエンドではマシンリソースに対するパフォーマンステストが行われるように、フロントエンドでもブラウザに対するパフォーマンステストが実施されます。パフォーマンスは非機能要件の 1 つです。これから紹介する非機能要件は以下の通りです。","パフォーマンス レジリエンス ミューテーション 互換性 ~~Security~~","パフォーマンス","フロントエンド開発では、パフォーマンス問題に直面することは避けられません。パフォーマンス問題が発生した際には、パフォーマンスチューニングが一般的に行われます。DevTool やプロファイラーを活用して問題を特定し、解決を図ります。 しかし、問題解決だけで終わらせてしまうのは非常に勿体ないことです。同じような問題が再発しないように、パフォーマンスに関するテストコードが存在するとさらに良いでしょう。","Profiler","React では、Profiler というツールが提供されており、これを Jest と組み合わせることで効果的にテストが可能です。 https://ja.reactjs.org/docs/profiler.html 例えば、TodoList の Todo が 100 個レンダリングされる場合の描画時間を、200ms 未満であることを確認するテストが作成できます。","これは単純な例ですが、実際にはもっと複雑な操作が求められることがあります。パフォーマンスチューニング後に、描画時間などの要件が保証されるようにしましょう。","PerformanceObserver","レイアウトの再計算（reflow）が強制的に実行される場合、ブラウザのメインスレッドでの JS 実行時間が長く続く可能性があります。これにより、描画速度が遅れ、フレームレート（fps）が低下することがあります。 この問題に対処するために、Performance API の中にある、まだ実験段階の機能である PerformanceLongTaskTiming という指標を用いたテストコードを作成することができます。 以下のリンクで Performance API および PerformanceLongTaskTiming に関する詳細情報を参照できます。","https://developer.mozilla.org/ja/docs/Web/API/Performance_API https://developer.mozilla.org/ja/docs/Web/API/PerformanceLongTaskTiming","具体的にテストコードを作成してみましょう。@playwright/test を利用してみます。","以下のコマンドを使用してテストを実行することができます。","このテストコードによって、longTasks が発生しないことが保証されます。 longTasks は、50ms を超えるタスクが対象となります（","）。その他の指標には、","や","などが含まれます。指標の一覧は、","で確認できます。","メモリリーク","SPA のようなフロントエンドの場合、メモリリークが懸念されることがあります。この問題に対処するために、Meta 社が開発した Memlab というツールが便利です。Memlab は、Puppeteer を起動し、画面操作を行い、","。 ここでは、簡単な Memlab を利用したテストコードを紹介します。"," は、ベースとなる URL へのアクセスであり、メモリの使用量を監視します。"," でメモリリークが発生しそうな操作を記述し、"," で元の状態に戻す操作を行います。 Memlab の実行は、以下のコマンドです。","レジリエンス","フロントエンドでもカオスエンジニアリングテストが実施可能です。 https://www.npmjs.com/package/chaos-frontend-toolkit ブラウザの操作はユーザーによって多様です。例えば、以下のようなブラウザ操作が存在します。","自由なブラウザバックやマウス、キーボード操作 シングルクリックではなくダブルクリック","さらに、ブラウザが外部と通信することはよくあります。ネットワークに関しても、次のような状況が起こり得ます。","リクエストの失敗や遅延 プロキシ環境下でよくあるリクエストの遮断","このようなカオスを注入してもアプリケーションがクラッシュしないようにテストを書くことは有益であるかもしれません。 例えば、Storybook 上でランダムなクリックを発生させるためのコードは、次のようになります。","その後、console.error を監視し、発生しないことを確認するテストコードを書くと良いでしょう。ただし、もしエラーが検出されたとしても、再現できなければ問題解決が難しいため、何らかの方法で追跡しやすいログを残すか、セッションリプレイのように記録するなどの工夫が必要です（シード値があればさらに良いですが）。","ミューテーション","突然変異テストという手法が存在します。 https://stryker-mutator.io/ これは、フロントエンドという文脈ではありませんが、面白かったので紹介します。(笑) Stryker は、プロダクションコードを書き換え（突然変異）する際に、テストコードも失敗することを期待するものです。つまり、テストコードのテストを行うイメージです。これにより、偽のテストカバレッジを見抜くことができます。Stryker の設定は、公式ページに従ってセットアップし、"," を実行するだけです。実際に動かしてみます。","TodoItem の中で、プロダクトコードを破壊したミュータント（👽）を検出し、対応するテストが失敗した（Killed）ものは、次の画像の通りです。 一方、ミュータントが生き残って（Survived）しまった例として、例えば、className の箇所が挙げられます。 className は見た目に関わるため、後述するビジュアルリグレッションテストで検出したいところですね。 このように、ミュータントを見つけてテストの品質を向上させることは、効果的な手段の 1 つだと考えられます。","互換性","フロントエンド開発では、サポート対象のブラウザで動作確認を行う必要があります。ブラウザごとに異なる JavaScript エンジンやレンダリングエンジンがあるため、各ブラウザにおける JavaScript の動作や見た目を確認することが必要です。 クロスブラウザテストには、実機の購入、仮想サーバーの利用、BrowserStack のような SaaS を活用する、Playwright でのマルチブラウザ利用など、さまざまな方法が存在します。費用対効果に見合った選択を行うことが重要です。","UI/UX","UI/UX は、フロントエンド開発において、切っても切り離せない重要なテーマです。今回は、テスト自動化が可能な非常に小さな部分だけを紹介します。UI/UX の範囲は、人の判断が多く求められる領域であるため、書く内容はあまり多くありません。","ビジュアル","見た目は、フロントエンド特有の非常に重要な要素です。レスポンシブデザイン、デスクトップ・モバイルのデバイス、Windows/Mac などの OS、ブラウザの外観に関する機能（ダークモード）など、見た目の変化をテストすることも重要です。 ビジュアルリグレッションテストという手法を開発サイクルに取り入れましょう。Lost Pixel や Chromatic など、様々な手段があるので試してみてください。 https://storybook.js.org/docs/react/writing-tests/visual-testing","アクセシビリティ","アクセシビリティについては、人の判断が必要なケースもありますが、機械的にチェックできる要素も存在します。キーボード操作のみで機能が正常に動作するか確認するためには、インタラクションテストが必要です。VoiceOver の対応状況はいかがでしょうか？さらに、画像の alt 属性は適切に設定されていますか？文章の表現については人の目で判断が必要ですが、少なくとも入力されているかどうかはチェックできます。 https://storybook.js.org/docs/react/writing-tests/accessibility-testing","その他","テストの観点として、網羅的に知りたいと思いませんか？私は、自前で","というものを作成しました。これは、様々な観点の一覧表です。この一覧表から、何かインスパイアされるものを見つけるのが、楽しいですね。 例えば、以下のようなことが考えられます。","精度(precision)","バックエンド側の数値は、フロントエンド側の数値の範囲に収まりますか？ JavaScript の Integer の最大値は、9,007,199,254,740,991 です。","耐障害性(fault tolerance)","フロントエンドから、コアなデータ参照と、補助のデータ参照は分けていますか？ 補助のデータ参照が失敗したとしても、アプリケーションが動作できる状態になっているのが良いでしょう。","終わりに","いかがでしたでしょうか。フロントエンドにおけるテストパターンは、まだまだ存在すると思います。もし他にもご存知のものがあれば、ぜひ教えてください。"],"t":"Webフロントエンドにおける網羅的テストパターンガイド"},{"f":"src/routes/blog/contents/what_I_learned_when_I_started_using_React_in_my_work/index.mdx","c":["［覚書］Reactを業務で使い始めて知ったこと","私は、これまでプライベートでしか React を使っていませんでした。 最近、業務で React を使う機会が増えたので、学んだことを残そうと思います。","React の歴史","なんで React って生まれたんだろうって気になりました。 簡単ですが、ちょこっとだけ調べて、次の記事にまとめました。","React は、次の問題を解決したかったんだと思います。","DOM ツリーが大きくなるにつれて、下位の変更によるカスケード更新の負荷が大きくなる","そこで、React は、この問題を解決するために、仮想 DOM という仕組みを作ったんだと思います。","仮想 DOM、差分検出処理、そして Fiber","React は、直接 DOM を操作するのではなく、仮想 DOM に対して操作します。仮想 DOM は、名前の通り仮想的な DOM です。 仮想 DOM を DOM へ反映するために、差分検出処理(reconciliation)というアルゴリズムがあったり、Fiber と呼ばれる、レンダリングの最適化(優先順位)を目的としたアルゴリズムもあるようです。これらのおかげで、レンダリング負荷が軽減されるんだと思います。(しらんけど) まだまだ理解が浅いので、これからもっと学んでいきたいと思います。","レンダリングのタイミングは、いつなんでしょうか。","レンダリングタイミング","基本的に、React は、親コンポーネントをレンダリングすると、子コンポーネントもレンダリングされます。 再レンダリングをキューイングする関数、setState や forUpdate などを呼ぶと、コンポーネントはレンダリングされることになります。","コードベースが大きくなるにつれて、レンダリングのパフォーマンスが悪化していきます。 そこで、パフォーマンスの最適化が求められます。","パフォーマンス最適化","最初からパフォーマンス最適化をする必要はありませんが、要件によっては必要になることもあります。 最適化の手段として、React にある、次の 3 つの関数が使えます。","コンポーネントのレンダーをスキップできる","以前の props と現在の props で変更がなければ","値を","できる","関数を","memo と併用して使う","も参考になります。","比較アルゴリズム","React では、コンポーネントや状態が変更されたかどうかの判定に、"," を使っているようです。 Object.is のサンプルコードは、次のとおりです。","string や integer のようなプリミティブな値は良いのですが、非プリミティブな値(Object)の場合の考慮が必要です。 例えば、","の場合は、第二引数に比較関数を渡すことができます。 例えば、次のような感じです。","が、パフォーマンス最適化のみに使いましょう。","Tips"," を使われている影響で、非プリミティブな値の状態更新に、工夫が必要です。","NG の方は、同じオブジェクトを使いまわしているのに対し、OK の方は、新しくオブジェクトを生成しています。","パフォーマンス調査","トップダウンでパフォーマンス調査をするのが、ベターと思います。","Chrome Developer Tools > Lighthouse を使い、performance score を確認 Chrome Developer Tools > Performance を使い、処理に時間がかかっている箇所を見つける React Developer Tools > Profiler を使い、React コンポーネントのレンダリングで時間がかかっている箇所を調査","React コンポーネント デザインパターン","React でコンポーネントを実装していると、次の 3 つのパターンがあるようです。","Container and presentation","ロジックと UI を分離 XxxContainer, Xxx という命名が多い","Higher order component","高階コンポーネント withXxx という命名が多い","Function as child","コンポーネントではなく関数を child として渡す","ロジックを独自フックとして切り出す","テスタビリティや再利用性の観点より、ロジックを hooks として切り出すのが良さそうです。","命名は、use から始まることが多いです。","その他","コンポーネントコードと同じフォルダ内に、次のファイルを置きたい","テストコード (test)","仕様を知る","カタログコード(storybook)","UI を見る","スタイルコード (scss)","input 要素などの onChange には、debounce を使う","onChange の処理が重たいときに"],"t":"［覚書］Reactを業務で使い始めて知ったこと"},{"f":"src/routes/blog/contents/what_i_learned_from_developing_o_embed_web_components/index.mdx","c":["WebComponentsでoEmbedのコンポーネントを開発して、学んだこと","WebComponents で、oEmbed コンポーネントを開発し、公開しました。","https://www.webcomponents.org/element/Silver-birder/o-embed https://www.npmjs.com/package/@silverbirder/o-embed","開発していく上で、学んだことを列挙しようと思います。","スターターキット","Web Components を開発する場合、次のどちらかのスターターキットを使うのが良さそうです。","これらを使わずとも、Web Components を開発できるのですが、Typescript で書いたり、テストをしたりするには、 それなりに準備が必要です。そのため、開発の初速を高めたいなら、スターターキットを使いましょう。 もしくは、先にスターターキットを使わず素の Javascript だけで Web Components を作ってみて、その後にスターターキットを使うと良さを実感できるかもしれません。 個人的に、open-wc をお勧めします。なぜなら、以下のツールが揃っているからです。","Testing Demoing Building Linting","もちろん、Typescript もサポートしています。","キャッチアップ","Web Components ってどういうものなのか、キャッチアップするには MDN のサイトが参考になります。","また、日本語で WebComponents(Custom Elements)の仕様書もあります。","Chrome の中にある Chromium におけるレンダリングエンジン blink の実装コードも、公開されています。","ベストプラクティス","Google より、Custom Elements のベストプラクティスが公開されています。","例えば","Always accept primitive data (strings, numbers, booleans) as either attributes or properties.","にあるように、プリミティブなデータのみ HTML の属性に渡すようにしましょう。 オブジェクトや配列のようなリッチなデータは、シリアル化する必要がありオブジェクト参照がなくなってしまう欠点があります。","テスト","Web Components のテストを書くには、Shadow DOM に対応する必要があります。 JSDOM のように、ブラウザ API をラップするライブラリを使っても良いのですが、ヘッドレスブラウザを使ったほうが妥当です。 そこで、","が便利です。 このテストライブラリは、open-wc と同じ Modern Web というモノの 1 つです。 @web/test-runner には、Puppeteer、Playwright、Selenium の 3 つをサポートしています。","Publish","作成した Web Components を Publish したい場合、次の記事を読むと良いです。","特に、してはいけないことを読むと、なるほどな〜ってなります。","❌ Do not optimize ❌ Do not bundle ❌ Do not minify ❌ Do not use .mjs file extensions ❌ Do not import polyfills","詳しくは、上記の記事を読んでください。","終わりに","Web Components をプロダクションレベルで使えるようになりたいなと思います。"],"t":"WebComponentsでoEmbedのコンポーネントを開発して、学んだこと"},{"f":"src/routes/blog/contents/writing_efficiency_tool_introducing_ai_ghostwriter/index.mdx","c":["ライティングの効率化ツール：AI Ghostwriterの紹介","import { Image } from \"~/components/image/image\"; 最近、ビアードパパの焼きチーズケーキシューにハマっている silverbirder です。 文章作成が苦手な私は、AI が文章を代筆する「AI Ghostwriter」という Chrome の拡張機能を開発しました。今回は、この便利なツールの紹介をします。 Chrome ウェブストアで公開しています。気になる方は、以下のリンクよりダウンロードしてください! 無料です! https://chrome.google.com/webstore/detail/ai-ghostwriter/hpcokeldeijnfmbbbjkedhnedjjbjmoa","AI Ghostwriter って？","AI Ghostwriter は、ChatGPT を活用して執筆者のライティング作業を助け、その品質を向上させる Chrome 拡張機能です。これはブラウザ上で選択したテキストに対して様々なアクションを実行できるツールで、ライティングにおけるあらゆる問題を解決します。 アクションは、デフォルトで以下の 3 つを用意しています。(後述しますがカスタマイズ可能です!)","校正(Proofreading)","タイトル生成(Generate title)","続きの文章生成(Generate following text)","仕組みは、とてもシンプルです。選択されたテキストを","のパラメータとして指定した状態でリクエストし、レスポンスをサイドパネルに表示するだけです。 百聞は一見にしかず、デモ動画を紹介します。以下の画像をクリックするとデモ動画が再生されます。 以下は、デモ動画で紹介している AI Ghostwriter の操作手順です。","右上の AI Ghostwriter アイコンをクリックし、サイドパネルを開く ブラウザ上のテキストを選択する 右クリックし、コンテキストメニューを開き、校正などのアクションをクリック サイドパネルに、AI Ghostwriter からコメントが表示される","コンテキストメニューのアクションをカスタマイズ","コンテキストメニューのアクションは、ユーザーのニーズに合わせて自由にカスタマイズ可能です。具体的には、アクションの追加・削除・編集が可能です。以下の画像の通りです。アクションは、オプションページで登録できます。 ※ オプションページは、右上の AI Ghostwriter アイコンを右クリックするとオプションという選択肢があり、それをクリックするとオプションページに遷移できます。 カスタマイズにより、ご自身の好みに合わせたライティング補助が可能となります。つまり、","自分だけのアクションを作成し、より効率的なライティングを実現できます","。AI からのコメントがイマイチの場合、コンテンツパラメータのテキストに少し手を加えてみると良いでしょう。また、","翻訳や要約","といったアクションも便利だと感じるかもしれません。これらの機能を活用して、より効率的なライティング体験を実感してみてください。","その他の機能","その他に、ユーザーの利便性を考慮して、以下の機能も提供しています。","多言語対応","英語と日本語に対応しています。","フィードバックリンク","ユーザーからのフィードバックを受け付けるためのリンクを設置しています。","誤操作防止","API トークンが未定義であったり、サイドパネルが未オープンであったりする場合に、Chrome の通知を通じて誤操作を防止します。","生成停止","不要な生成処理を停止するために、アクションを停止するボタンも用意","ショートカットキーの設置","サイドパネルをショートカットキーで開くようにしています"," で確認できます","終わりに","AI Ghostwriter のおかげで、私の執筆効率はとてもよくなりました。既にアクションが 6 つあります(笑)。皆さんの執筆効率もよくなることを願っています！","余談: 開発経緯","ここからは、余談です。なぜ、このアプリを開発に至ったかについて簡単に紹介します。","アイデアの出発点",": 日頃から ChatGPT をブログ執筆に活用していた経験から、何か新しいものを創れないかと考え始めました。","市場調査",": AI ライティング補助のサービスを調査しましたが、既存のサービスはメールテンプレート生成など、私が求めていた特定テキストの校正機能を持っていませんでした。","アプリのイメージ",": 特定テキストに対する特定アクション、具体的には校正機能を持つアプリを作りたいと考えました。","開発方法の検討",": 新しい Web フレームワークや技術を試すことも考えましたが、継続性を考慮し、既存の経験と知識を活用することにしました。","Vercel のテンプレート",": Vercel のテンプレートにある","というものを見つけ、これが理想的なツールだと感じました。このエディタは Notion のような形式で、OpenAI の組み込みが可能で、選択したテキストに対してアクションを実行できます。","エディタ開発の断念",": しかし、エディタを作ることは大変で、開発時間が伸びることから断念しました。私が求めていたのは、任意のテキストを校正した結果を知るだけでした。","Chrome 拡張機能の活用",": その考えを元に、Deepl の Chrome 拡張機能を思い出しました。Chrome 拡張機能にはコンテキストメニューの API がビルドインされており、最近リリースされたサイドパネル機能を使えば、現在開いているタブの隣に別のウィンドウを表示できます。そこに校正後のテキストを表示すれば良いと考えました。","以上、私の簡単な開発経緯についてでした。最後までお読み頂きありがとうございました。"],"t":"ライティングの効率化ツール：AI Ghostwriterの紹介"},{"f":"src/routes/blog/contents/zod_refine_path/index.mdx","c":["zodのrefineにあるpathにハマった","zod の refine を使っていたのですが、path の使い方を全く理解できておらず、小一時間ほどハマってしまったことがあったので、備忘録として残しておきます。","背景","zod を使って、バリデーションロジックを書いていました。 バリデーションで複数フィールドを参照する必要があったため、refine を使っていました。 https://github.com/colinhacks/zod#refine","path","サンプルコードを以下に示します。","a というオブジェクトに refine を定義しました。refine の第 2 引数の path に "," を定義しました。 schema のテストをしてみます。","エラー(issue)の path は、"," になります。めちゃくちゃ当たり前なんですが、以上です。(笑) stackblitz にも残しておきました。 https://stackblitz.com/edit/nodemon-fkzaw5?file=index.js zod の github にある path の説明は、"," と書いています。"],"t":"zodのrefineにあるpathにハマった"},{"f":"src/routes/blog/contents/zoom-meeting-creator/index.mdx","c":["ZoomのMeetingを自動生成するGASライブラリ zoom-meeting-creator を作った","みなさん、Zoom 使っていますか？ Zoom の Meeting を自動生成する GAS ライブラリを公開しましたので、 そのきっかけと使い方について紹介しようと思います。","きっかけ","社の Slack で次の qiita の記事を知りました。 https://qiita.com/kudota/items/b480610cc3f575a8ec6f GAS から Zoom の Meeting を作れるのって、簡単なんだな〜と思いつつ、 \"cron のように使いたい\"という Slack のコメントがあったので、サクッと一日で作ってみました。 定期的に Zoom の Meeting(ID やパスワード)を更新する会社はあるはずです。 そういう会社にとっては、このツールは、便利かもしれません。","作ったもの","https://github.com/silverbirder/zoom-meeting-creator これを GAS 側でライブラリ追加すると使えます。 この GAS では、","Zoom の Meeting を作成 Slack との連携","ができます。この機能を、GAS の定期実行を組み合わせれば、\"Zoom の Meeting 作成を cron のように\"使えるようになります。"],"t":"ZoomのMeetingを自動生成するGASライブラリ zoom-meeting-creator を作った"}]